{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20180d59-bcc7-40a1-930b-f232caee62af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb29cf-dde6-4181-8791-89aaa088b03d",
   "metadata": {},
   "source": [
    "#### Set hyperparameters for the data processing\n",
    "1. Set embedding layer\n",
    "    - TOKENIZER: Type of tokenizer used \n",
    "    - input_folder: input data to create the embedding layer\n",
    "    - embedding_model_type: Wordembedding algorithm\n",
    "    - embedding_model_fname: filname of output embedding layer model\n",
    "    - vector_size: length of the word embeddings\n",
    "    - window: Size of wondow to creat \n",
    "    - min_count: minimum frequency of word occuring in training data to exist in word embedding\n",
    "    - sg\n",
    "    - TOKENS_TPYE: Tokens to use for building the embedding layer (with or without stopwords)\n",
    "2. PDF PARSER\n",
    "    - TOKENIZER: Tokenizer used to split the text\n",
    "    - input_folder: input data to parse\n",
    "    - embedding_model_fname: filname of input embedding layer model\n",
    "    - chunk_size: Size of candidate documents\n",
    "    - chunk_overlap: by how much the chunks overlap\n",
    "3. Build Query\n",
    "    - TOKENIZER: Tokenizer used to split the query\n",
    "    - embedding_model_fname: filename of input embedding layer to get embeddings of tokenized query\n",
    "    - max_query_length: Size to pad of truncate query\n",
    "4. Get Candidate Documents\n",
    "    - top_N: Number of candidate documents to save\n",
    "    - TOKENS_EMBEDDINGS: Which tokens/embeddings of the query to use\n",
    "    - DOCUMENT_EMBEDDING: Which embeddings of the chunks to use\n",
    "    - DOCUMENT_TOKENS: Which tokens of the chunks to use (same as DOCUMENT_EMBEDDING)\n",
    "    - METHOD: method of cosine similarity\n",
    "5. Question and Answers\n",
    "    - transformer_model_name: What model used to fine the answer\n",
    "    - TOKENIZER: Tokenizer used to create the model inputs and the decode the message\n",
    "    - context_size: Size of the context vector to fit the candidate documents into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86150bf0-739d-4f1c-8cdd-6e769b4316cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the global vars\n",
    "TOKENIZER = 'roberta' # 'bert' or 'roberta'\n",
    "input_folder = \"space_based_pdfs\" # 'space_based_pdfs' or 'math_based_pdfs'\n",
    "embedding_model_type = 'Word2Vec' # Word2Vec, Fastrack, GLove\n",
    "embedding_model_fname = f\"{TOKENIZER}_{input_folder}_{embedding_model_type}_model.bin\"\n",
    "vector_size = 50\n",
    "window = 3\n",
    "min_count = 3\n",
    "sg = 0\n",
    "TOKENS_TPYE = \"tokens_less_sw\" # \"tokens_less_sw\", \"tokens\"\n",
    "chunk_size = 350\n",
    "chunk_overlap = 0\n",
    "max_query_length = 20\n",
    "top_N = 20\n",
    "TOKENS_EMBEDDINGS = \"query_search_less_sw\" # \"query_search_less_sw\", \"query_search\", \"query\"\n",
    "DOCUMENT_EMBEDDING = \"token_embeddings_less_sw\" # \"token_embeddings_less_sw\", \"token_embeddings\"\n",
    "DOCUMENT_TOKENS = \"tokens_less_sw\" # \"tokens_less_sw\", \"tokens\"\n",
    "METHOD = \"MEAN_MAX\" # 'MEAN_MAX', 'MEAN_MEAN', 'COMBINE_MEAN'\n",
    "transformer_model_name = \"deepset/roberta-base-squad2\" # \"deepset/roberta-base-squad2\", \"bert-base-uncased\"\n",
    "context_size = 350 \n",
    "\n",
    "hyperparameters = {\n",
    "    'TOKENIZER': TOKENIZER,\n",
    "    'input_folder': input_folder,\n",
    "    'embedding_model_type': embedding_model_type,\n",
    "    'embedding_model_fname': embedding_model_fname,\n",
    "    'vector_size': vector_size,\n",
    "    'window': window,\n",
    "    'min_count': min_count,\n",
    "    'sg': sg,\n",
    "    'TOKENS_TPYE': TOKENS_TPYE,\n",
    "    'chunk_size': chunk_size,\n",
    "    'chunk_overlap': chunk_overlap,\n",
    "    'max_query_length': max_query_length,\n",
    "    'top_N': top_N,\n",
    "    'TOKENS_EMBEDDINGS': TOKENS_EMBEDDINGS,\n",
    "    'DOCUMENT_EMBEDDING': DOCUMENT_EMBEDDING,\n",
    "    'DOCUMENT_TOKENS': DOCUMENT_TOKENS,\n",
    "    'METHOD': METHOD,\n",
    "    'transformer_model_name': transformer_model_name,\n",
    "    'context_size': context_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5315cf6-be11-4447-b980-9af7c062567e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file '../vars/hyperparameters1.json' updated with new data.\n"
     ]
    }
   ],
   "source": [
    "# File path for the JSON file\n",
    "json_file_path = os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")\n",
    "\n",
    "# Check if the JSON file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    # If the file doesn't exist, create and write to it\n",
    "    with open(json_file_path, \"w\") as json_file:\n",
    "        json.dump(hyperparameters, json_file, indent=4)\n",
    "    print(f\"JSON file '{json_file_path}' created and data written.\")\n",
    "else:\n",
    "    # If the file exists, update its contents\n",
    "    with open(json_file_path, \"w\") as json_file:\n",
    "        json.dump(hyperparameters, json_file, indent=4)\n",
    "    print(f\"JSON file '{json_file_path}' updated with new data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf244d4-3ca2-43d5-bc40-1d4677a18cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e6bef3-e959-4fb1-a98d-c5877728001a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "29e779d3-a20f-4e93-9c8e-91f2d3c8ab9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hyperparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Set the Tokenizer for your specific BERT model variant\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m TOKENIZER \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparams\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOKENIZER\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m bert_base_tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m roberta_tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepset/roberta-base-squad2\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_prefix_space \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hyperparams' is not defined"
     ]
    }
   ],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "TOKENIZER = hyperparams[\"TOKENIZER\"]\n",
    "\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c957b59a-8541-4118-a562-a9e97c7594a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġbeginnings', 'Ġameric', \"'s\", 'Ġfirst', 'Ġinter', 'continental', 'Ġreading', 'las', 'rocket', 'Ġfamily', 'Ġwik', 'ipedia', 'Ġballistic', 'Ġmissile', ',\"', 'Ġtechnology', 'Ġculture', 'ap', 'ril', '),', 'âĢĵ', 'las', 'Ġencyclopedia', 'Ġastronaut', 'ica', 'htm', 'Ġretrieved', 'Ġexternal', 'Ġlinks']\n"
     ]
    }
   ],
   "source": [
    "fname = os.path.join(\"..\", \"data\", \"space_based_pdfs_roberta_parsed\", \"1431.json\")\n",
    "with open(fname) as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "current_chunk_less_sw = data['tokens_less_sw']\n",
    "print(current_chunk_less_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "636790cb-7c2c-4999-a3fd-5df75ef36d41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' beginnings americ\\'s first intercontinental readinglasrocket family wikipedia ballistic missile,\" technology cultureapril),–las encyclopedia astronauticahtm retrieved external links'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.convert_tokens_to_ids(current_chunk_less_sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993a83c-cae0-44de-ae8e-2939e5bdd85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5a74d-94b7-4e87-9ac4-b1ec4227d059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
