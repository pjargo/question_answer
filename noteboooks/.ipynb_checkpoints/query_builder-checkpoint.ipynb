{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b1ff252-c3ab-413b-ae16-139912919800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import nltk\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "from utils import get_sha256, clean_text, remove_non_word_chars, clean_text, tokens_to_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af33402-81aa-41d6-b2e0-27975ac5ac9b",
   "metadata": {},
   "source": [
    "#### Prompt user for query\n",
    "- Specify the tokenizer, consistant with Q&A model and parsed pdfs (TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb1d2a1-2785-42d3-a8bd-55b31acdf370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "# TOKENIZER = 'bert'\n",
    "TOKENIZER = 'roberta'\n",
    "\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b6c7669-1ce2-4884-a11e-b6b89032cf5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  What was generalized in the paper \"On The Definition of Higher Gamma functions\"?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized query:\n",
      " ['What', 'Ġwas', 'Ġgeneralized', 'Ġin', 'Ġthe', 'Ġpaper', 'Ġ\"', 'On', 'ĠThe', 'ĠDefinition', 'Ġof', 'ĠHigher', 'ĠGamma', 'Ġfunctions', '\"?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'] \n",
      "\n",
      "Tokenized query for seach:\n",
      " ['What', 'Ġwas', 'Ġgeneralized', 'Ġin', 'Ġthe', 'Ġpaper', 'ĠOn', 'ĠThe', 'ĠDefinition', 'Ġof', 'ĠHigher', 'ĠGamma', 'Ġfunctions'] \n",
      "\n",
      "Tokenized query for seach less stop words:\n",
      " ['What', 'Ġgeneralized', 'Ġpaper', 'ĠOn', 'ĠThe', 'ĠDefinition', 'ĠHigher', 'ĠGamma', 'Ġfunctions'] \n",
      "\n",
      "Input IDs query:\n",
      " tensor([[ 2264,    21, 44030,    11,     5,  2225,    22,  4148,    20, 38764,\n",
      "             9, 13620, 43566,  8047, 24681,     1,     1,     1,     1,     1]]) \n",
      "\n",
      "Attention mask query:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user for an input query\n",
    "user_query = input(\"Enter your query: \")\n",
    "user_query = user_query.lower()\n",
    "\n",
    "# clean query for BERT input\n",
    "user_query = clean_text(user_query)\n",
    "\n",
    "# clean query for candidate search\n",
    "user_query_for_search = remove_non_word_chars(user_query)\n",
    "\n",
    "# Tokenize the query for BERT input\n",
    "tokenized_query = tokenizer.tokenize(user_query)\n",
    "\n",
    "# Tokenize the query for candidate search\n",
    "tokenized_query_for_search = tokenizer.tokenize(user_query_for_search)\n",
    "\n",
    "# Remove the stop words for the tokenized query for search\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "nltk_stop_words.extend([\"Ġ\" + word for word in nltk_stop_words])  # Add the roberta modified tokens\n",
    "tokenized_query_for_search_less_sw = [token for token in tokenized_query_for_search if token not in nltk_stop_words]\n",
    "\n",
    "# Pad or truncate the query to a fixed length of 20 tokens (BERT input)\n",
    "max_query_length = 20\n",
    "if len(tokenized_query) > max_query_length:\n",
    "    tokenized_query = tokenized_query[:max_query_length]\n",
    "else:\n",
    "    padding_length = max_query_length - len(tokenized_query)\n",
    "    tokenized_query = tokenized_query + [tokenizer.pad_token] * padding_length\n",
    "\n",
    "# Convert the tokenized query to input IDs and attention mask\n",
    "input_ids_query = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
    "attention_mask_query = [1] * len(input_ids_query)\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_query = torch.tensor(input_ids_query).unsqueeze(0)  # Add batch dimension\n",
    "attention_mask_query = torch.tensor(attention_mask_query).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(\"Tokenized query:\\n\", tokenized_query, \"\\n\")\n",
    "print(\"Tokenized query for seach:\\n\", tokenized_query_for_search, \"\\n\")\n",
    "print(\"Tokenized query for seach less stop words:\\n\", tokenized_query_for_search_less_sw, \"\\n\")\n",
    "print(\"Input IDs query:\\n\", input_ids_query, \"\\n\")\n",
    "print(\"Attention mask query:\\n\", attention_mask_query, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce0bdf-7a2d-4cdc-90b0-be9513da32b1",
   "metadata": {},
   "source": [
    "##### Add the embeddings\n",
    "- Specify the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50e01267-a1c9-4320-b422-6679073328b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "# model_fname = os.path.join(\"..\", \"models\", \"word_embeddings\", \"word2vec_model.bin\")\n",
    "model_fname = os.path.join(\"..\", \"models\", \"word_embeddings\", \"roberta_word2vec_model.bin\")\n",
    "model = Word2Vec.load(model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a49f57-9661-4f92-9915-401897e35a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\tTokens Length\t\tEmbeddings Shape\n",
      "\t\t\t   Query embeddings:\t      20\t\t     (20, 100)\n",
      "\t\tQuery embeddings for search:\t      13\t\t\t     (13, 100)\n",
      " Query embeddings for search less stopwords:\t      9\t\t\t     (9, 100)\n"
     ]
    }
   ],
   "source": [
    "# Get the query embeddings for the candidate document search\n",
    "query_embeddings = tokens_to_embeddings(tokenized_query, model, RANDOM=False)\n",
    "query_embeddings_search = tokens_to_embeddings(tokenized_query_for_search, model, RANDOM=False)\n",
    "query_embeddings_less_sw = tokens_to_embeddings(tokenized_query_for_search_less_sw, model, RANDOM=False)\n",
    "\n",
    "print(\"\\t\\t\\t\\t\\t\\tTokens Length\\t\\tEmbeddings Shape\")\n",
    "print(f\"\\t\\t\\t   Query embeddings:\\t      {len(tokenized_query)}\\t\\t     {query_embeddings.shape}\")\n",
    "print(f\"\\t\\tQuery embeddings for search:\\t      {len(tokenized_query_for_search)}\\t\\t\\t     {query_embeddings_search.shape}\")\n",
    "print(f\" Query embeddings for search less stopwords:\\t      {len(tokenized_query_for_search_less_sw)}\\t\\t\\t     {query_embeddings_less_sw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb34f19-37d2-4a76-a013-b1cbd88da2b5",
   "metadata": {},
   "source": [
    "##### Store the output the the query directory, filename is hash of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29dd5893-3316-4690-86c8-23267f788b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7ff4d51a240429a1e936dbe31c4c79924ad112923dcc4e0d1a7de488400dde79\n",
      "../query/7ff4d51a240429a1e936dbe31c4c79924ad112923dcc4e0d1a7de488400dde79.json\n"
     ]
    }
   ],
   "source": [
    "# store the query\n",
    "query_data = {\n",
    "    \"query\": user_query,\n",
    "    \"input_ids_query\":input_ids_query.tolist(),\n",
    "    \"attention_mask_query\": attention_mask_query.tolist(),\n",
    "    \"query_search\":user_query_for_search,\n",
    "    \"tokenized_query\":tokenized_query,\n",
    "    \"tokenized_query_search\":tokenized_query_for_search,\n",
    "    \"tokenized_query_search_less_sw\":tokenized_query_for_search_less_sw,\n",
    "    \"query_embedding\": query_embeddings_search.tolist(), # Just used for the candidate search\n",
    "    \"query_embedding_search\": query_embeddings_search.tolist(), # Just used for the candidate search, cleaned\n",
    "    \"query_embedding_search_less_sw\": query_embeddings_less_sw.tolist() # Just used for the candidate search, cleaned more\n",
    "}\n",
    "\n",
    "json_string = json.dumps(query_data['query'], indent=2)\n",
    "# print(json_string)\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = os.path.join(\"..\", 'query')\n",
    "\n",
    "# Check if the directory exists, if not create the directory\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# Generate a UUID\n",
    "# unique_id = uuid.uuid4()\n",
    "unique_id = get_sha256(json_string)\n",
    "print(unique_id)\n",
    "\n",
    "fname = os.path.join(directory_path, str(unique_id)+'.json')\n",
    "print(fname)\n",
    "\n",
    "with open(fname, 'w') as j_file:\n",
    "    json.dump(query_data, j_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c42e9-3116-4978-9226-83fdfef0566d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
