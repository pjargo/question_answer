{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2596aca2-ff60-4e23-b444-e27cbbd68662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "from utils import pdfs_to_df, tokenize_df_of_texts\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf0e55-acc2-4e5b-a775-92f02f003171",
   "metadata": {},
   "source": [
    "#### Get text from test corpus\n",
    "- Specify tokenizer, keep consistent with downstream Q&A model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb659758-9325-4bdf-8a73-be97bad99559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/test_pdfs/2101.00031.pdf\n",
      "../data/test_pdfs/2101.01089.pdf\n",
      "../data/test_pdfs/2101.00182.pdf\n",
      "../data/test_pdfs/2101.00525.pdf\n",
      "../data/test_pdfs/2101.01017.pdf\n",
      "../data/test_pdfs/2101.00005.pdf\n",
      "../data/test_pdfs/2101.00763.pdf\n",
      "../data/test_pdfs/2101.01291.pdf\n",
      "../data/test_pdfs/2101.00831.pdf\n",
      "../data/test_pdfs/2101.01094.pdf\n",
      "../data/test_pdfs/2101.00572.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "['Abstract', 'Abstract_Original', 'sha_256', 'language', 'language_probability', 'Authors', 'Title', 'url', 'date', 'token_embeddings']\n"
     ]
    }
   ],
   "source": [
    "# From the test pdf dir, extract the text and tokenize it. Store in pandas dataframe\n",
    "\n",
    "directory = os.path.join(\"..\", \"data\", \"test_pdfs\")\n",
    "df = pdfs_to_df(directory)\n",
    "df = tokenize_df_of_texts(df, bert_base_tokenizer)\n",
    "\n",
    "drop_cols = [col for col in df.columns if col not in ['Document', 'Text', 'Original_Text', 'Path', 'tokens']]\n",
    "print(drop_cols)\n",
    "\n",
    "df = df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2248cb19-8b67-497d-851b-be7a2c8af778",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Text</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>Path</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2101.00031.pdf</td>\n",
       "      <td>lagrangian cobordisms between legendrian knots...</td>\n",
       "      <td>. Lagrangian cobordisms between Legendrian kno...</td>\n",
       "      <td>../data/test_pdfs/2101.00031.pdf</td>\n",
       "      <td>[la, ##gra, ##ng, ##ian, co, ##bor, ##dis, ##m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2101.01089.pdf</td>\n",
       "      <td>cotangent sums play signiﬁcant role in the nym...</td>\n",
       "      <td>. Cotangent sums play a signiﬁcant role in the...</td>\n",
       "      <td>../data/test_pdfs/2101.01089.pdf</td>\n",
       "      <td>[cot, ##ange, ##nt, sums, play, sign, ##i, ##ﬁ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2101.00182.pdf</td>\n",
       "      <td>let be an open subset of rn, and let p, [1, ∞]...</td>\n",
       "      <td>. Let Ω be an open subset of RN, and let p, q ...</td>\n",
       "      <td>../data/test_pdfs/2101.00182.pdf</td>\n",
       "      <td>[let, be, an, open, subset, of, rn, ,, and, le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2101.00525.pdf</td>\n",
       "      <td>the multivariable autoregressive ﬁlter problem...</td>\n",
       "      <td>\\nThe multivariable autoregressive ﬁlter probl...</td>\n",
       "      <td>../data/test_pdfs/2101.00525.pdf</td>\n",
       "      <td>[the, multi, ##var, ##iable, auto, ##re, ##gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2101.01017.pdf</td>\n",
       "      <td>we derive an upper bound for the assouad dimen...</td>\n",
       "      <td>. We derive an upper bound for the Assouad dim...</td>\n",
       "      <td>../data/test_pdfs/2101.01017.pdf</td>\n",
       "      <td>[we, derive, an, upper, bound, for, the, ass, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2101.00005.pdf</td>\n",
       "      <td>while teaching course on integral equations, n...</td>\n",
       "      <td>\\nWhile teaching a course on integral equation...</td>\n",
       "      <td>../data/test_pdfs/2101.00005.pdf</td>\n",
       "      <td>[while, teaching, course, on, integral, equati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2101.00763.pdf</td>\n",
       "      <td>let is certain tensor product of simple dyadic...</td>\n",
       "      <td>. Let T is a certain tensor product of simple ...</td>\n",
       "      <td>../data/test_pdfs/2101.00763.pdf</td>\n",
       "      <td>[let, is, certain, tensor, product, of, simple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2101.01291.pdf</td>\n",
       "      <td>we generalize our previous new deﬁnition of eu...</td>\n",
       "      <td>. We generalize our previous new deﬁnition of ...</td>\n",
       "      <td>../data/test_pdfs/2101.01291.pdf</td>\n",
       "      <td>[we, general, ##ize, our, previous, new, de, #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2101.00831.pdf</td>\n",
       "      <td>given constants x, and the space of entire fun...</td>\n",
       "      <td>. Given constants x, ν ∈ C and the space H0 of...</td>\n",
       "      <td>../data/test_pdfs/2101.00831.pdf</td>\n",
       "      <td>[given, constant, ##s, x, ,, and, the, space, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2101.01094.pdf</td>\n",
       "      <td>logarithmic potentials and many other potentia...</td>\n",
       "      <td>. Logarithmic potentials and many other potent...</td>\n",
       "      <td>../data/test_pdfs/2101.01094.pdf</td>\n",
       "      <td>[log, ##ari, ##th, ##mic, potential, ##s, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2101.00572.pdf</td>\n",
       "      <td>in this paper we solve the eigenvalue problem ...</td>\n",
       "      <td>: In this paper we solve the eigenvalue proble...</td>\n",
       "      <td>../data/test_pdfs/2101.00572.pdf</td>\n",
       "      <td>[in, this, paper, we, solve, the, e, ##igen, #...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Document                                               Text  \\\n",
       "0   2101.00031.pdf  lagrangian cobordisms between legendrian knots...   \n",
       "1   2101.01089.pdf  cotangent sums play signiﬁcant role in the nym...   \n",
       "2   2101.00182.pdf  let be an open subset of rn, and let p, [1, ∞]...   \n",
       "3   2101.00525.pdf  the multivariable autoregressive ﬁlter problem...   \n",
       "4   2101.01017.pdf  we derive an upper bound for the assouad dimen...   \n",
       "5   2101.00005.pdf  while teaching course on integral equations, n...   \n",
       "6   2101.00763.pdf  let is certain tensor product of simple dyadic...   \n",
       "7   2101.01291.pdf  we generalize our previous new deﬁnition of eu...   \n",
       "8   2101.00831.pdf  given constants x, and the space of entire fun...   \n",
       "9   2101.01094.pdf  logarithmic potentials and many other potentia...   \n",
       "10  2101.00572.pdf  in this paper we solve the eigenvalue problem ...   \n",
       "\n",
       "                                        Original_Text  \\\n",
       "0   . Lagrangian cobordisms between Legendrian kno...   \n",
       "1   . Cotangent sums play a signiﬁcant role in the...   \n",
       "2   . Let Ω be an open subset of RN, and let p, q ...   \n",
       "3   \\nThe multivariable autoregressive ﬁlter probl...   \n",
       "4   . We derive an upper bound for the Assouad dim...   \n",
       "5   \\nWhile teaching a course on integral equation...   \n",
       "6   . Let T is a certain tensor product of simple ...   \n",
       "7   . We generalize our previous new deﬁnition of ...   \n",
       "8   . Given constants x, ν ∈ C and the space H0 of...   \n",
       "9   . Logarithmic potentials and many other potent...   \n",
       "10  : In this paper we solve the eigenvalue proble...   \n",
       "\n",
       "                                Path  \\\n",
       "0   ../data/test_pdfs/2101.00031.pdf   \n",
       "1   ../data/test_pdfs/2101.01089.pdf   \n",
       "2   ../data/test_pdfs/2101.00182.pdf   \n",
       "3   ../data/test_pdfs/2101.00525.pdf   \n",
       "4   ../data/test_pdfs/2101.01017.pdf   \n",
       "5   ../data/test_pdfs/2101.00005.pdf   \n",
       "6   ../data/test_pdfs/2101.00763.pdf   \n",
       "7   ../data/test_pdfs/2101.01291.pdf   \n",
       "8   ../data/test_pdfs/2101.00831.pdf   \n",
       "9   ../data/test_pdfs/2101.01094.pdf   \n",
       "10  ../data/test_pdfs/2101.00572.pdf   \n",
       "\n",
       "                                               tokens  \n",
       "0   [la, ##gra, ##ng, ##ian, co, ##bor, ##dis, ##m...  \n",
       "1   [cot, ##ange, ##nt, sums, play, sign, ##i, ##ﬁ...  \n",
       "2   [let, be, an, open, subset, of, rn, ,, and, le...  \n",
       "3   [the, multi, ##var, ##iable, auto, ##re, ##gre...  \n",
       "4   [we, derive, an, upper, bound, for, the, ass, ...  \n",
       "5   [while, teaching, course, on, integral, equati...  \n",
       "6   [let, is, certain, tensor, product, of, simple...  \n",
       "7   [we, general, ##ize, our, previous, new, de, #...  \n",
       "8   [given, constant, ##s, x, ,, and, the, space, ...  \n",
       "9   [log, ##ari, ##th, ##mic, potential, ##s, and,...  \n",
       "10  [in, this, paper, we, solve, the, e, ##igen, #...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65c0f4-f289-42c6-a0e1-dcfb76cff4ed",
   "metadata": {},
   "source": [
    "#### Train model on tokenized text\n",
    "- Set:\n",
    "    - Vector Size: length of word embeddings\n",
    "    - Window Size: span of sorrounding words to train model\n",
    "    - Min Count: minimum number of occurances of word to be be viable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15e9850-1623-42ed-a54e-b3666f0da578",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your DataFrame with tokenized texts\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=df['tokens'].to_list(), vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(os.path.join(\"..\", \"models\", \"word_embeddings\", \"word2vec_model.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eff82a-9997-49e8-ada3-e99515c4e964",
   "metadata": {},
   "source": [
    "#### Examine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7a83fa-eff0-4458-8903-ff0a3ff5cc6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count token frequencies\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m token_frequencies \u001b[38;5;241m=\u001b[39m \u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print the frequency of \"number\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_frequencies[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/collections/__init__.py:552\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/collections/__init__.py:637\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(iterable) \u001b[38;5;66;03m# fast path when counter is empty\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count token frequencies\n",
    "token_frequencies = Counter(df['tokens'].to_list())\n",
    "\n",
    "# Print the frequency of \"number\"\n",
    "print(\"Frequency of 'number':\", token_frequencies[\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c141a3dd-47ed-41c5-a6b2-543083323093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.50522113e-02  7.75543526e-02 -8.95991474e-02 -1.86093464e-01\n",
      "  8.59568715e-02 -1.37554884e-01 -4.07235883e-02  1.45292446e-01\n",
      " -7.10864216e-02 -6.32060394e-02  2.07130127e-02 -1.79543078e-01\n",
      " -2.39383861e-01 -3.73561010e-02  4.36756946e-02  5.16275242e-02\n",
      " -1.59504488e-01 -1.03847787e-01  6.49311543e-02 -4.41587061e-01\n",
      " -3.52784097e-02  6.61083311e-02  2.50223696e-01 -5.47317648e-03\n",
      " -1.99339658e-01 -8.27215165e-02  2.91567110e-02 -1.79247096e-01\n",
      "  1.18086323e-01  9.77727864e-03  1.70967519e-01  1.92541592e-02\n",
      "  2.49241024e-01  7.86685124e-02  1.71444342e-02  8.05471931e-03\n",
      " -5.90218492e-02  5.57746142e-02  1.29359856e-01 -1.01037314e-02\n",
      "  1.69563130e-01 -2.50092298e-01  1.74485728e-01  7.36795142e-02\n",
      "  9.38774720e-02  3.18660997e-02 -2.03889050e-02 -1.47375315e-01\n",
      " -2.41411166e-04  1.45220965e-01 -5.30875847e-02  2.34161764e-01\n",
      "  1.24412812e-01  5.06878309e-02 -6.93341121e-02  1.41992196e-01\n",
      "  1.47458076e-01 -3.54603343e-02  1.14661060e-01 -4.90062125e-02\n",
      "  1.29437506e-01  1.65118873e-01 -2.12452978e-01  9.51182917e-02\n",
      "  1.48294736e-02  3.18889022e-02  1.28024936e-01  2.55032063e-01\n",
      " -1.35519475e-01  2.41379142e-01 -2.98457980e-01 -2.56001294e-01\n",
      "  2.74185706e-02  1.48756832e-01  1.07337162e-01 -5.92970885e-02\n",
      " -1.10862195e-01  4.75074314e-02  2.08579991e-02  1.35715097e-01\n",
      " -4.10815934e-03 -9.90763679e-03 -3.07462305e-01  1.58846483e-01\n",
      "  1.18136860e-01  5.06577976e-02  2.44228225e-02  1.78087160e-01\n",
      "  8.06155205e-02 -1.06869675e-01  4.36401973e-03 -6.29370958e-02\n",
      " -1.25171170e-01 -6.25127777e-02  4.00070027e-02  1.51053473e-01\n",
      "  2.13633422e-02 -1.62481815e-01 -1.17547423e-01  4.87780236e-02]\n",
      "[('exact', 0.9938832521438599), ('iso', 0.9916039109230042), ('front', 0.9914003014564514), ('up', 0.9910028576850891), ('related', 0.9909995794296265), ('##tangle', 0.9904268980026245), ('st', 0.9899924993515015), ('parameter', 0.9899809956550598), ('admit', 0.9893457889556885), ('negative', 0.9892069697380066)]\n"
     ]
    }
   ],
   "source": [
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(\"word2vec_model.bin\")\n",
    "\n",
    "# Access the embedding of a word\n",
    "embedding = model.wv[\"manifold\"]\n",
    "print(embedding)\n",
    "# Find similar words based on embedding similarity\n",
    "similar_words = model.wv.most_similar(\"manifold\")\n",
    "print(similar_words)\n",
    "\n",
    "# You can also perform vector arithmetic operations\n",
    "# result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c82ce892-41cf-41e2-9ab8-b84043f24fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 3207\n",
      "Is 'number' in vocabulary? True\n"
     ]
    }
   ],
   "source": [
    "vocabulary = model.wv.index_to_key\n",
    "print(\"Number of words in vocabulary:\", len(vocabulary))\n",
    "print(\"Is 'number' in vocabulary?\", 'number' in vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441fb34d-7f82-42bc-bf05-750c8996dcf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '[PAD]' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[PAD]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '[PAD]' not present\""
     ]
    }
   ],
   "source": [
    "token = '[PAD]'\n",
    "\n",
    "print(model.wv['[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291b722-ea15-4915-a40b-6eb6b1c3c1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
