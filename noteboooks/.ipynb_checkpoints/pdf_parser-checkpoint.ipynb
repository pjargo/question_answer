{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45a15b0-2934-4da3-855e-fd95b3533f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "from utils import parsed_pdf_to_json\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe55cd-e1a2-475e-ae6f-96008da9cacd",
   "metadata": {},
   "source": [
    "#### Parse pdfs in directory for text, tokenize, chunk and save to JSON files\n",
    "- Specify PDF directory (directory)\n",
    "- Specify Storage directory for the JSON files (storage_dir)\n",
    "- Specify word embedding model, consistent with the query for getting candidate documents (model_fname)\n",
    "- Specify chunk overlap: the number of tokens consecutive chunks overlap by (chunk_overlap)\n",
    "- Specify the tokenizer (TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a6feaf-5281-45a2-a22f-52bfdd17c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "# TOKENIZER = 'bert'\n",
    "TOKENIZER = 'roberta'\n",
    "\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144c21b-b0c2-434a-b179-02dddfb6e4e6",
   "metadata": {},
   "source": [
    "##### Test on one_pdf directory containing one pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55d4cb6c-6a8f-4644-bd67-4f8138818276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = os.path.join(\"..\", \"data\", \"one_pdf\")\n",
    "storage_dir = os.path.join(\"..\", \"data\", \"one_pdf_parsed\")\n",
    "\n",
    "# Load your trained Word2Vec model\n",
    "model_fname = os.path.join(\"..\", \"models\", \"word_embeddings\", \"roberta_word2vec_model.bin\")\n",
    "model = Word2Vec.load(model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36943aef-acb4-4015-95ea-29e88d8f6532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/one_pdf/2101.00031.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "Chunking the tokenized text\n",
      "\n",
      "printing the shape of chunked dataframe\n",
      "(124, 17)\n"
     ]
    }
   ],
   "source": [
    "parsed_pdf_to_json(directory, storage_dir, embedding_layer_model=model, chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4516b4-0cc5-4ee5-90fc-b1eae7e83363",
   "metadata": {},
   "source": [
    "##### Parse documents in test_pdfs dirctory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7feb646b-9bca-41a1-a90b-c2bace557135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "directory = os.path.join(\"..\", \"data\", \"test_pdfs\")\n",
    "storage_dir = os.path.join(\"..\", \"data\", \"parsed_cleaned_pdfs\", \"roberta\")\n",
    "\n",
    "# Load your trained Word2Vec model\n",
    "model_fname = os.path.join(\"..\", \"models\", \"word_embeddings\", \"roberta_word2vec_model.bin\")\n",
    "model = Word2Vec.load(model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "241ef5c5-9b41-4ca5-9da3-3fbaf755f3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/test_pdfs/2101.00031.pdf\n",
      "../data/test_pdfs/2101.01089.pdf\n",
      "../data/test_pdfs/2101.00182.pdf\n",
      "../data/test_pdfs/2101.00525.pdf\n",
      "../data/test_pdfs/2101.01017.pdf\n",
      "../data/test_pdfs/2101.00005.pdf\n",
      "../data/test_pdfs/2101.00763.pdf\n",
      "../data/test_pdfs/2101.01291.pdf\n",
      "../data/test_pdfs/2101.00831.pdf\n",
      "../data/test_pdfs/2101.01094.pdf\n",
      "../data/test_pdfs/2101.00572.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "Chunking the tokenized text\n",
      "\n",
      "printing the shape of chunked dataframe\n",
      "(906, 17)\n"
     ]
    }
   ],
   "source": [
    "parsed_pdf_to_json(directory, storage_dir, embedding_layer_model=model, tokenizer= chunk_overlap=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff4091-0593-423a-a9a8-9ca83c4a51ee",
   "metadata": {},
   "source": [
    "#### Testing some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02ae93ef-5082-49e3-9b7f-ce4f95596de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "therea are some known elementary building blocks for lagrangian cobordisms...\n"
     ]
    }
   ],
   "source": [
    "# BELOW CODE: Adding a spell checker to the clean_text function in utils.py\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove unnecessary whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    \n",
    "    # Spell checking and correction using pyspellchecker\n",
    "#     spell = SpellChecker()\n",
    "#     words = text.split()\n",
    "#     for word in words:\n",
    "#         print(spell.correction(word))\n",
    "#     corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words ]\n",
    "#     text = ' '.join(corrected_words)\n",
    "    \n",
    "    return text.strip()  # Remove leading and trailing whitespace\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"therea are some known \\u201celementary\\u201d building blocks for lagrangian cobordisms...\"\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(cleaned_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27726824-1cbe-40da-bdb4-3e6639b45d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
