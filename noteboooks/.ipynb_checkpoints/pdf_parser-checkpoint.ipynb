{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45a15b0-2934-4da3-855e-fd95b3533f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from utils import parsed_pdf_to_json\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d7005-9645-4267-ad9a-489497adab5a",
   "metadata": {},
   "source": [
    "#### Get Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be83a5b7-6211-486b-b852-f59decf3d7cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TOKENIZER\": \"roberta\",\n",
      "    \"input_folder\": \"space_based_pdfs\",\n",
      "    \"embedding_model_type\": \"Word2Vec\",\n",
      "    \"embedding_model_fname\": \"roberta_space_based_pdfs_Word2Vec_model.bin\",\n",
      "    \"vector_size\": 100,\n",
      "    \"window\": 5,\n",
      "    \"min_count\": 1,\n",
      "    \"sg\": 0,\n",
      "    \"TOKENS_TPYE\": \"tokens_less_sw\",\n",
      "    \"chunk_size\": 100,\n",
      "    \"chunk_overlap\": 0,\n",
      "    \"max_query_length\": 20,\n",
      "    \"top_N\": 10,\n",
      "    \"TOKENS_EMBEDDINGS\": \"query_search_less_sw\",\n",
      "    \"DOCUMENT_EMBEDDING\": \"token_embeddings_less_sw\",\n",
      "    \"METHOD\": \"MEAN_MAX\",\n",
      "    \"transformer_model_name\": \"deepset/roberta-base-squad2\",\n",
      "    \"context_size\": 350\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")) as json_file:\n",
    "    hyperparams = json.load(json_file)\n",
    "    print(json.dumps(hyperparams, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe55cd-e1a2-475e-ae6f-96008da9cacd",
   "metadata": {},
   "source": [
    "#### Parse pdfs in directory for text, tokenize, chunk and save to JSON files\n",
    "- Specify PDF directory (directory)\n",
    "- Specify Storage directory for the JSON files (storage_dir)\n",
    "- Specify word embedding model, consistent with the query for getting candidate documents (model_fname)\n",
    "- Specify chunk overlap: the number of tokens consecutive chunks overlap by (chunk_overlap)\n",
    "- Specify the tokenizer (TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4516b4-0cc5-4ee5-90fc-b1eae7e83363",
   "metadata": {},
   "source": [
    "##### Parse documents in test_pdfs dirctory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a6feaf-5281-45a2-a22f-52bfdd17c250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "TOKENIZER = hyperparams[\"TOKENIZER\"]\n",
    "\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "398d03b4-6975-4e9c-9523-5946d7b015e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "embedding_model_fname = hyperparams[\"embedding_model_fname\"]\n",
    "\n",
    "embedding_model_type = hyperparams['embedding_model_type']\n",
    "if embedding_model_type == 'Word2Vec':\n",
    "    model = Word2Vec.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7feb646b-9bca-41a1-a90b-c2bace557135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data location: ../data/space_based_pdfs\n",
      "\n",
      "Contents of directory '../data/space_based_pdfs_roberta_parsed' deleted.\n"
     ]
    }
   ],
   "source": [
    "input_folder = hyperparams[\"input_folder\"]\n",
    "directory = os.path.join(\"..\", \"data\", input_folder)\n",
    "\n",
    "# Specify the directory path you want to check and create\n",
    "output_folder = f\"{input_folder}_{TOKENIZER}_parsed\"\n",
    "storage_dir = os.path.join(\"..\", \"data\", output_folder)\n",
    "\n",
    "print(f\"input data location: {directory}\\n\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(storage_dir):\n",
    "    # If the directory doesn't exist, create it\n",
    "    os.makedirs(storage_dir)\n",
    "    print(f\"Directory '{storage_dir}' created.\")\n",
    "else:\n",
    "    # If the directory exists, delete its contents\n",
    "    for filename in os.listdir(storage_dir):\n",
    "        file_path = os.path.join(storage_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "    print(f\"Contents of directory '{storage_dir}' deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b47f9134-58df-43c0-9804-59989ab48a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '://', 'https', '\"', '\"...', '/)', 'www', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '.[', ',[', '-,', '][', 'com', 'Ġ!', 'Ġ\"', 'Ġ#', 'Ġ$', 'Ġ%', 'Ġ&', \"Ġ'\", 'Ġ(', 'Ġ)', 'Ġ*', 'Ġ+', 'Ġ,', 'Ġ-', 'Ġ.', 'Ġ/', 'Ġ://', 'Ġhttps', 'Ġ\"', 'Ġ\"...', 'Ġ/)', 'Ġwww', 'Ġ:', 'Ġ;', 'Ġ<', 'Ġ=', 'Ġ>', 'Ġ?', 'Ġ@', 'Ġ[', 'Ġ\\\\', 'Ġ]', 'Ġ^', 'Ġ_', 'Ġ`', 'Ġ{', 'Ġ|', 'Ġ}', 'Ġ~', 'Ġ.[', 'Ġ,[', 'Ġ-,', 'Ġ][', 'Ġcom']\n"
     ]
    }
   ],
   "source": [
    "# Specify additional stopwords to remove from the chunk cleaned for the candidate document search\n",
    "special_characters = [\n",
    "    \"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \"://\", \"https\",'\"', '\"...', \"/)\",\"www\",\n",
    "    \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\", \".[\", \",[\", \"-,\", \"][\", \"com\",\n",
    "    \"),\", ',\"'\n",
    "]\n",
    "\n",
    "special_characters += list(map(lambda x: \"Ġ\" + x, special_characters))\n",
    "print(special_characters)\n",
    "\n",
    "# Add numbers to remove\n",
    "special_characters += list(map(lambda x: str(x), range(100000)))\n",
    "special_characters += list(map(lambda x: \"Ġ\" + str(x), range(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "241ef5c5-9b41-4ca5-9da3-3fbaf755f3f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/space_based_pdfs/Galaxy 15 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Swarm Technologies - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Fengyun - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Falcon 9 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Cygnus NG-19 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Atlas V - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Inmarsat - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Kepler-11 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/James Webb Space Telescope - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Space-Based Infrared System - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Yaogan - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Starlink - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Atlas (rocket family) - Wikipedia.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "Chunking the tokenized text...\n",
      "\n",
      "printing the shape of chunked dataframe\n",
      "(1432, 13)\n"
     ]
    }
   ],
   "source": [
    "parsed_pdf_to_json(directory, storage_dir, \n",
    "                   embedding_layer_model=model, \n",
    "                   tokenizer=tokenizer, \n",
    "                   chunk_size=hyperparams['chunk_size'],\n",
    "                   chunk_overlap=hyperparams['chunk_overlap'], \n",
    "                   additional_stopwords=special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144c21b-b0c2-434a-b179-02dddfb6e4e6",
   "metadata": {},
   "source": [
    "##### Test on one_pdf directory containing one pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d4cb6c-6a8f-4644-bd67-4f8138818276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data location: ../data/one_pdf\n",
      "\n",
      "Contents of directory '../data/one_pdf_roberta_parsed' deleted.\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"one_pdf\"\n",
    "directory = os.path.join(\"..\", \"data\", input_folder)\n",
    "\n",
    "# Specify the directory path you want to check and create\n",
    "output_folder = f\"{input_folder}_{TOKENIZER}_parsed\"\n",
    "storage_dir = os.path.join(\"..\", \"data\", output_folder)\n",
    "\n",
    "print(f\"input data location: {directory}\\n\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(storage_dir):\n",
    "    # If the directory doesn't exist, create it\n",
    "    os.makedirs(storage_dir)\n",
    "    print(f\"Directory '{storage_dir}' created.\")\n",
    "else:\n",
    "    # If the directory exists, delete its contents\n",
    "    for filename in os.listdir(storage_dir):\n",
    "        file_path = os.path.join(storage_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "    print(f\"Contents of directory '{storage_dir}' deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36943aef-acb4-4015-95ea-29e88d8f6532",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/one_pdf/Starlink - Wikipedia.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "Chunking the tokenized text...\n",
      "\n",
      "printing the shape of chunked dataframe\n",
      "(333, 13)\n"
     ]
    }
   ],
   "source": [
    "parsed_pdf_to_json(directory, storage_dir, \n",
    "                   embedding_layer_model=model, \n",
    "                   tokenizer=tokenizer, \n",
    "                   chunk_size=hyperparams['chunk_size'],\n",
    "                   chunk_overlap=hyperparams['chunk_overlap'], \n",
    "                   additional_stopwords=special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff4091-0593-423a-a9a8-9ca83c4a51ee",
   "metadata": {},
   "source": [
    "#### Testing some code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02ae93ef-5082-49e3-9b7f-ce4f95596de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "therea are some known elementary building blocks for lagrangian cobordisms...\n"
     ]
    }
   ],
   "source": [
    "# BELOW CODE: Adding a spell checker to the clean_text function in utils.py\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def clean_text(text):\n",
    "    # Normalize Unicode characters\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove unnecessary whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    \n",
    "    # Spell checking and correction using pyspellchecker\n",
    "#     spell = SpellChecker()\n",
    "#     words = text.split()\n",
    "#     for word in words:\n",
    "#         print(spell.correction(word))\n",
    "#     corrected_words = [spell.correction(word) if spell.correction(word) is not None else word for word in words ]\n",
    "#     text = ' '.join(corrected_words)\n",
    "    \n",
    "    return text.strip()  # Remove leading and trailing whitespace\n",
    "\n",
    "# Example usage\n",
    "raw_text = \"therea are some known \\u201celementary\\u201d building blocks for lagrangian cobordisms...\"\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(cleaned_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27726824-1cbe-40da-bdb4-3e6639b45d33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
