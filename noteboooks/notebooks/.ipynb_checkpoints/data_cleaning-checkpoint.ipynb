{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3faf0e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !setx HTTP_PROXY \"http://33566:mon@www-proxy-west.aero.org:8080\"\n",
    "# !setx HTTPS_PROXY \"http://33566:mon@www-proxy-west.aero.org:8080\"\n",
    "# !echo %HTTP_PROXY%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6d0cb6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: detect-secrets[gibberish]\n"
     ]
    }
   ],
   "source": [
    "# !pip install detect-secrets[gibberish] --proxy \"http://33566:mon@www-proxy-west.aero.org:8080\"\n",
    "!pip install detect-secrets[gibberish]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3e8620d-d112-405c-bb2f-c83a853106d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gibberish-detector\n",
      "  Downloading gibberish_detector-0.1.1-py3-none-any.whl (10 kB)\n",
      "Installing collected packages: gibberish-detector\n",
      "Successfully installed gibberish-detector-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gibberish-detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea49e45",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Obtaining dependency information for PyMuPDF from https://files.pythonhosted.org/packages/01/f2/c1481c1cc179b9a501a60a1b00906b0e1bdb5fa22034d6f4c91d93c505ad/PyMuPDF-1.22.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading PyMuPDF-1.22.5-cp38-cp38-macosx_10_9_x86_64.whl.metadata (8.3 kB)\n",
      "Downloading PyMuPDF-1.22.5-cp38-cp38-macosx_10_9_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.22.5\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDFdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5484d478",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datefinder\n",
      "  Downloading datefinder-0.7.3-py2.py3-none-any.whl (10 kB)\n",
      "Collecting regex>=2017.02.08 (from datefinder)\n",
      "  Obtaining dependency information for regex>=2017.02.08 from https://files.pythonhosted.org/packages/84/e7/2ae56083cd0da3da1fe79d61fe4fe6e3ab5f114660a751fecefa77a78225/regex-2023.6.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading regex-2023.6.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m281.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.4.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from datefinder) (2.8.2)\n",
      "Requirement already satisfied: pytz in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from datefinder) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from python-dateutil>=2.4.2->datefinder) (1.16.0)\n",
      "Downloading regex-2023.6.3-cp38-cp38-macosx_10_9_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex, datefinder\n",
      "Successfully installed datefinder-0.7.3 regex-2023.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datefinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77b036da",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.3.1\n",
      "  latest version: 23.7.2\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=23.7.2\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "zsh:1: command not found: y\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f1abf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#user your credentials and url to authenticate\n",
    "# proxy_user = '33566'\n",
    "\n",
    "# my_date = date.today()\n",
    "# proxy_pass = calendar.day_name[my_date.weekday()].lower()[:3]\n",
    "# print(proxy_pass)\n",
    "\n",
    "# os.environ['http_proxy'] = \"http://%s:%s@www-proxy-west.aero.org:8080\"%(proxy_user, proxy_pass)\n",
    "# os.environ['https_proxy'] = \"http://%s:%s@www-proxy-west.aero.org:8080\"%(proxy_user, proxy_pass)\n",
    "\n",
    "# print(req.getproxies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2feffc5b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/17/f2/a2edd907ab4c11b749b73c803482f99e49e3cc6afe4958a8ce3e0cd8efff/spacy-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading spacy-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.7-cp38-cp38-macosx_10_9_x86_64.whl (32 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.8-cp38-cp38-macosx_10_9_x86_64.whl (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8 (from spacy)\n",
      "  Downloading thinc-8.1.10-cp38-cp38-macosx_10_9_x86_64.whl (850 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m850.2/850.2 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Obtaining dependency information for wasabi<1.2.0,>=0.9.1 from https://files.pythonhosted.org/packages/8f/69/26cbf0bad11703241cb84d5324d868097f7a8faf2f1888354dac8883f3fc/wasabi-1.1.2-py3-none-any.whl.metadata\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Obtaining dependency information for srsly<3.0.0,>=2.4.3 from https://files.pythonhosted.org/packages/ca/91/d1a78aaaa8f57b69bc4b6f44d671a0947739c38e3d53de1e2cd07838f454/srsly-2.4.7-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading srsly-2.4.7-cp38-cp38-macosx_10_9_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Obtaining dependency information for catalogue<2.1.0,>=2.0.6 from https://files.pythonhosted.org/packages/45/8f/5b73efc14e0373d9bb0de6ce1ab04a8f77420dc473f1f3ef270caf085cff/catalogue-2.0.9-py3-none-any.whl.metadata\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting typer<0.10.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0 (from spacy)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<7.0.0,>=5.2.1 (from spacy)\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.15.0 (from spacy)\n",
      "  Obtaining dependency information for numpy>=1.15.0 from https://files.pythonhosted.org/packages/11/10/943cfb579f1a02909ff96464c69893b1d25be3731b5d3652c2e0cf1281ea/numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 (from spacy)\n",
      "  Obtaining dependency information for pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 from https://files.pythonhosted.org/packages/06/46/1da67901c12513c34bf853ad6a89abe991496bc1171f72c41fb78a15238f/pydantic-1.10.12-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading pydantic-1.10.12-cp38-cp38-macosx_10_9_x86_64.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/48/8e/36718a905e9d9b94c14199e6a6206709d4d4d8a9e3dea3ff61e1a56020e8/blis-0.7.10-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading blis-0.7.10-cp38-cp38-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/63/05/2d7acb83610e0c5171dfcb4a2968d676800da8082fc456e284fa8e0924f4/confection-0.1.0-py3-none-any.whl.metadata\n",
      "  Downloading confection-0.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting click<9.0.0,>=7.1.1 (from typer<0.10.0,>=0.3.0->spacy)\n",
      "  Obtaining dependency information for click<9.0.0,>=7.1.1 from https://files.pythonhosted.org/packages/1a/70/e63223f8116931d365993d4a6b7ef653a4d920b41d03de7c59499962821f/click-8.1.6-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Downloading spacy-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Downloading numpy-1.24.4-cp38-cp38-macosx_10_9_x86_64.whl (19.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-1.10.12-cp38-cp38-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading srsly-2.4.7-cp38-cp38-macosx_10_9_x86_64.whl (490 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.7/490.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Downloading blis-0.7.10-cp38-cp38-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading click-8.1.6-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: cymem, wasabi, tqdm, spacy-loggers, spacy-legacy, smart-open, pydantic, numpy, murmurhash, langcodes, click, catalogue, typer, srsly, preshed, blis, pathy, confection, thinc, spacy\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dabac902",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (23.2.1)\n",
      "Requirement already satisfied: setuptools in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (68.0.0)\n",
      "Requirement already satisfied: wheel in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (0.38.4)\n",
      "Collecting wheel\n",
      "  Obtaining dependency information for wheel from https://files.pythonhosted.org/packages/17/11/f139e25018ea2218aeedbedcf85cd0dd8abeed29a38ac1fda7f5a8889382/wheel-0.41.0-py3-none-any.whl.metadata\n",
      "  Downloading wheel-0.41.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached wheel-0.41.0-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: wheel\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.38.4\n",
      "    Uninstalling wheel-0.38.4:\n",
      "      Successfully uninstalled wheel-0.38.4\n",
      "Successfully installed wheel-0.41.0\n",
      "Collecting spacy\n",
      "  Obtaining dependency information for spacy from https://files.pythonhosted.org/packages/17/f2/a2edd907ab4c11b749b73c803482f99e49e3cc6afe4958a8ce3e0cd8efff/spacy-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached spacy-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.8-cp38-cp38-macosx_10_9_x86_64.whl (107 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy)\n",
      "  Using cached thinc-8.1.10-cp38-cp38-macosx_10_9_x86_64.whl (850 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Collecting pathy>=0.10.0 (from spacy)\n",
      "  Obtaining dependency information for pathy>=0.10.0 from https://files.pythonhosted.org/packages/b5/c3/04a002ace658133f5ac48d30258ed9ceab720595dc1ac36df02fe52018af/pathy-0.10.2-py3-none-any.whl.metadata\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (1.24.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for blis<0.8.0,>=0.7.8 from https://files.pythonhosted.org/packages/48/8e/36718a905e9d9b94c14199e6a6206709d4d4d8a9e3dea3ff61e1a56020e8/blis-0.7.10-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Using cached blis-0.7.10-cp38-cp38-macosx_10_9_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.2.0,>=8.1.8->spacy)\n",
      "  Obtaining dependency information for confection<1.0.0,>=0.0.1 from https://files.pythonhosted.org/packages/63/05/2d7acb83610e0c5171dfcb4a2968d676800da8082fc456e284fa8e0924f4/confection-0.1.0-py3-none-any.whl.metadata\n",
      "  Using cached confection-0.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Using cached spacy-3.6.0-cp38-cp38-macosx_10_9_x86_64.whl (6.8 MB)\n",
      "Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Using cached blis-0.7.10-cp38-cp38-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "Using cached confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: preshed, blis, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.10 confection-0.1.0 pathy-0.10.2 preshed-3.0.8 spacy-3.6.0 thinc-8.1.10\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bec079b9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m765.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from en-core-web-lg==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f5d6cc6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from en-core-web-md==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb8feb17",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Obtaining dependency information for inflect from https://files.pythonhosted.org/packages/fb/c6/d9feb758be584f729424390af24687d3a4363d968164f94079f83cd536b4/inflect-7.0.0-py3-none-any.whl.metadata\n",
      "  Downloading inflect-7.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from inflect) (1.10.12)\n",
      "Requirement already satisfied: typing-extensions in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from inflect) (4.7.1)\n",
      "Downloading inflect-7.0.0-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-7.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "547f4895",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting swifter\n",
      "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pandas>=1.0.0 (from swifter)\n",
      "  Obtaining dependency information for pandas>=1.0.0 from https://files.pythonhosted.org/packages/78/a8/07dd10f90ca915ed914853cd57f79bfc22e1ef4384ab56cb4336d2fc1f2a/pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: psutil>=5.6.6 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from swifter) (5.9.0)\n",
      "Collecting dask[dataframe]>=2.10.0 (from swifter)\n",
      "  Obtaining dependency information for dask[dataframe]>=2.10.0 from https://files.pythonhosted.org/packages/07/93/32d3e317fec6d0fc130284f922ad9bd13d9ae0d52245e6ff6e57647e924c/dask-2023.5.0-py3-none-any.whl.metadata\n",
      "  Downloading dask-2023.5.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from swifter) (4.65.0)\n",
      "Requirement already satisfied: click>=8.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (8.1.6)\n",
      "Collecting cloudpickle>=1.5.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Obtaining dependency information for fsspec>=2021.09.0 from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (23.0)\n",
      "Collecting partd>=1.2.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading partd-1.4.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (6.0)\n",
      "Collecting toolz>=0.10.0 (from dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.13.0 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from dask[dataframe]>=2.10.0->swifter) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from pandas>=1.0.0->swifter) (2022.7)\n",
      "Collecting tzdata>=2022.1 (from pandas>=1.0.0->swifter)\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from importlib-metadata>=4.13.0->dask[dataframe]>=2.10.0->swifter) (3.11.0)\n",
      "Collecting locket (from partd>=1.2.0->dask[dataframe]>=2.10.0->swifter)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl (11.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dask-2023.5.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: swifter\n",
      "  Building wheel for swifter (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16510 sha256=2b5679caa8efcd6ff5e828c74f898b35d19ed6bc810e594a5623e2920190b0d4\n",
      "  Stored in directory: /Users/peterargo/Library/Caches/pip/wheels/86/c2/17/58370efa4aaa58f50055499d12379d806b3b5dd8579d4c33e4\n",
      "Successfully built swifter\n",
      "Installing collected packages: tzdata, toolz, locket, fsspec, cloudpickle, partd, pandas, dask, swifter\n",
      "Successfully installed cloudpickle-2.2.1 dask-2023.5.0 fsspec-2023.6.0 locket-1.0.0 pandas-2.0.3 partd-1.4.0 swifter-1.4.0 toolz-0.12.0 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54d60206",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo\n",
      "  Obtaining dependency information for pymongo from https://files.pythonhosted.org/packages/83/65/7caa0a8a878ee99683b6d865d83e2ae19466af23295e99b25ec8bed12002/pymongo-4.4.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading pymongo-4.4.1-cp38-cp38-macosx_10_9_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Obtaining dependency information for dnspython<3.0.0,>=1.16.0 from https://files.pythonhosted.org/packages/71/30/deee2ffb94194437c730a1c6230d9310ab5f73926a2549cdab91453616bb/dnspython-2.4.1-py3-none-any.whl.metadata\n",
      "  Downloading dnspython-2.4.1-py3-none-any.whl.metadata (4.9 kB)\n",
      "Downloading pymongo-4.4.1-cp38-cp38-macosx_10_9_x86_64.whl (413 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.4.1-py3-none-any.whl (300 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.4.1 pymongo-4.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aa36628",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=3bea5d5d6cd479852df8a301163d7cb7594916e97a7282c98182158e40681d7d\n",
      "  Stored in directory: /Users/peterargo/Library/Caches/pip/wheels/13/c7/b0/79f66658626032e78fc1a83103690ef6797d551cb22e56e734\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55177ca0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement language_parsers (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for language_parsers\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install language_parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "021a8926-d95a-49aa-8c94-88b6039ee8fa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from nltk) (8.1.6)\n",
      "Collecting joblib (from nltk)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/28/08/9dcdaa5aac4634e4c23af26d92121f7ce445c630efa0d3037881ae2407fb/joblib-1.3.1-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.3.1 nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0eb9ff33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https': 'http://33566:wed@proxy-west.aero.org:8080',\n",
       " 'http': 'http://33566:wed@proxy-west.aero.org:8080',\n",
       " 'no': '169.254.169.254'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# req.getproxies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f765aadb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19.2\n",
      "Requirement already satisfied: numpy in c:\\aeroapps\\anacondap3x64-2020.11\\lib\\site-packages (1.21.4)1.19.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\PA33566\\\\AppData\\\\Local\\\\Temp\\\\pip-uninstall-46syvtav\\\\core\\\\_multiarray_tests.cp38-win_amd64.pyd'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.21.4\n",
      "    Uninstalling numpy-1.21.4:\n",
      "      Successfully uninstalled numpy-1.21.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Aeroapps\\AnacondaP3x64-2020.11\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# import numpy\n",
    "# print(numpy.__version__)\n",
    "# !pip install --upgrade numpy\n",
    "# print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf36532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_url = 'https://raw.githubusercontent.com/rrenaud/Gibberish-Detector/master/big.txt'\n",
    "# # response = requests.get(target_url, proxies = {'http' : \"https://%s:%s@www-proxy-west.aero.org:8080\"%(proxy_user, proxy_pass), 'https' : \"https://%s:%s@www-proxy-west.aero.org:8080\"%(proxy_user, proxy_pass)})\n",
    "# # data = response.text\n",
    "# file = req.urlopen(target_url)\n",
    "# data = ' '.join([line.decode('utf-8') for line in file])\n",
    "\n",
    "Detector = detector.Detector(trainer.train_on_content(data, 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890'), threshold=4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d8cbf58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men_core_web_lg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_web_md\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize, word_tokenize\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/spacy/util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request as req\n",
    "from datetime import date\n",
    "import calendar\n",
    "from gibberish_detector import trainer\n",
    "import requests\n",
    "from gibberish_detector import detector\n",
    "import fitz\n",
    "import datefinder\n",
    "import subprocess\n",
    "import datetime\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import json\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "from collections import Counter\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "import string\n",
    "import gensim\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import en_core_web_md\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer, SnowballStemmer \n",
    "from nltk.corpus import wordnet\n",
    "from tqdm import tqdm\n",
    "import inflect \n",
    "import swifter\n",
    "import regex\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "from langdetect import detect_langs\n",
    "# from language_parsers.russian_parser.russian_to_english import ru_to_en\n",
    "# from language_parsers.chinese_parser.chinese_to_english import zh_to_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8930c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_date(filepath, is_arxiv=False):\n",
    "    if filepath.endswith('.pdf'):\n",
    "\n",
    "        if is_arxiv:\n",
    "            doc = fitz.open(filepath)\n",
    "            page1 = doc[0].get_text('text')\n",
    "            dates = []\n",
    "            for p in page1.split('\\n'):\n",
    "                if p.startswith('arXiv'):\n",
    "                    for ent in entity_finder(p):\n",
    "                        if ent['label'] == \"DATE\":\n",
    "                            try:\n",
    "                                date = parser.parse(ent['text'])\n",
    "                            except:\n",
    "                                continue\n",
    "\n",
    "                            if date < datetime.now():\n",
    "                                dates.append(date)\n",
    "                    break\n",
    "            \n",
    "        else:\n",
    "            text = get_title_text(filepath)\n",
    "            dates = []\n",
    "            for ent in entity_finder(text):\n",
    "                if ent['label'] == \"DATE\":\n",
    "                    try:\n",
    "                        date = parser.parse(ent['text'])\n",
    "                    except:\n",
    "                        continue\n",
    "                    \n",
    "                    if date < datetime.now():\n",
    "                        dates.append(date)\n",
    "\n",
    "        if dates == []:\n",
    "            if is_arxiv:\n",
    "                return get_document_date(filepath, is_arxiv=False)\n",
    "            else:\n",
    "                return ''\n",
    "        else:\n",
    "            return min(dates, key=lambda x: abs(x - datetime.now())).strftime('%m/%d/%Y')\n",
    "\n",
    "def entity_finder(text):\n",
    "    # 2.6 Entity Names Annotation\n",
    "    # Names (often referred to as “Named Entities”) are annotated according to the following\n",
    "    # set of types:\n",
    "    # PERSON People, including fictional\n",
    "    # NORP Nationalities or religious or political groups\n",
    "    # FACILITY Buildings, airports, highways, bridges, etc.\n",
    "    # ORGANIZATION Companies, agencies, institutions, etc.\n",
    "    # GPE Countries, cities, states\n",
    "    # LOCATION Non-GPE locations, mountain ranges, bodies of water\n",
    "    # PRODUCT Vehicles, weapons, foods, etc. (Not services)\n",
    "    # EVENT Named hurricanes, battles, wars, sports events, etc.\n",
    "    # WORK OF ART Titles of books, songs, etc.\n",
    "    # LAW Named documents made into laws \n",
    "    #  OntoNotes Release 5.0\n",
    "    # 22\n",
    "    # LANGUAGE Any named language\n",
    "    # The following values are also annotated in a style similar to names:\n",
    "    # DATE Absolute or relative dates or periods\n",
    "    # TIME Times smaller than a day\n",
    "    # PERCENT Percentage (including “%”)\n",
    "    # MONEY Monetary values, including unit\n",
    "    # QUANTITY Measurements, as of weight or distance\n",
    "    # ORDINAL “first”, “second”\n",
    "    # CARDINAL Numerals that do not fall under another typ\n",
    "    doc = nlp(text)\n",
    "    ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in ['CARDINAL', 'PERCENT', 'MONEY', 'ORDINAL', 'QUANTITY'] or '\\n' in ent.text:\n",
    "            continue\n",
    "        else:\n",
    "            ent_chars = {'text': ent.text, # The str of the named entity phrase.\n",
    "                         'start': ent.start_char, # Source str index of the first char.\n",
    "                         'end': ent.end_char, # Source str index of the last+1 char.\n",
    "                         'label': ent.label_} # A str label for the entity type.\n",
    "            #print(ent_chars)\n",
    "            ents.append(ent_chars)\n",
    "    return ents\n",
    "\n",
    "def get_arxiv_entities(arxiv_code):\n",
    "    url = 'https://arxiv.org/abs/%s'%arxiv_code\n",
    "    with libreq.urlopen(url) as url_a:\n",
    "        r = url_a.read()\n",
    "    page = str(r)\n",
    "\n",
    "    soup = BeautifulSoup(page)\n",
    "\n",
    "    entities = {}\n",
    "    # URL\n",
    "    entities['url'] = url\n",
    "\n",
    "    # Title\n",
    "    entities['Title'] = soup.find('h1', {'class': 'title mathjax'}).text.strip().split(':')[-1]\n",
    "\n",
    "    # Authors\n",
    "    div = soup.find('div', {'class' : 'authors'})\n",
    "    entities['Authors'] = [a.text.strip() for a in div.findAll('a')]\n",
    "\n",
    "    # Abstract\n",
    "    entities['Abstract'] = soup.find('blockquote', {'class' : 'abstract mathjax'}).text.replace('\\\\n', '').replace('Abstract:', '').strip()\n",
    "\n",
    "    # Subjects\n",
    "    entities['Subjects'] = soup.find('td', {'tablecell subjects'}).text.replace('\\\\n', '').strip().split('; ')\n",
    "\n",
    "    # Submission Date\n",
    "    submissions = {}\n",
    "    submission_strings = list(soup.find('div', {'class' : 'submission-history'}).stripped_strings)\n",
    "    for i, s in enumerate(submission_strings):\n",
    "        v = re.match(r'\\[v\\d\\]', s.replace('\\\\n', '').strip())\n",
    "        if v:\n",
    "            submissions[v.group()] = submission_strings[i+1].replace('\\\\n', '').strip()\n",
    "            \n",
    "    entities['Submissions'] = submissions\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e1ed65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(filepath, ru_model_path=None, zh_model_path=None):\n",
    "    '''\n",
    "    Gets raw text from pdf file. Will translate russian and chinese documents to english.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath (str) - the path to the pdf file\n",
    "    ru_model_path (str, None) - the path to the model to translate russian to english (default = None).  If None, model loaded from \"language_parsers\" directory.\n",
    "    zh_model_path (str, None) - the path to the model to translate chinese to english (default = None). If None, model loaded from \"language_parsers\" directory.\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    str, str, str - returns the document text, the abstract of the text, and the language of the text\n",
    "    '''\n",
    "    if not filepath.endswith('.pdf'):\n",
    "        print('File path not a pdf')\n",
    "        return None\n",
    "    \n",
    "    doc = fitz.open(filepath)\n",
    "\n",
    "    full_text = ''\n",
    "    for page in doc:\n",
    "        full_text += page.get_text('text')\n",
    "    \n",
    "    try:\n",
    "        langs = detect_langs(full_text)\n",
    "        language = langs[0].lang\n",
    "        if language != 'en':\n",
    "            print('not english:', language)\n",
    "            if language == 'zh-cn':\n",
    "                print('\\ttranslating zh to en...')\n",
    "                full_text = zh_to_en(zh_model_path, full_text)\n",
    "                print('\\ttranslated.')\n",
    "            elif language == 'ru':\n",
    "                print('\\ttranslating ru to en...')\n",
    "                full_text = ru_to_en(ru_model_path, full_text)\n",
    "                print('\\ttranslated.')\n",
    "            else:\n",
    "                print('translation model to english not created...yet')\n",
    "                return '', '', langs\n",
    "    except:\n",
    "        print('\\tlanguage detection error!')\n",
    "        langs = ''\n",
    "\n",
    "    re_section = r'ABSTRACT|Abstract|Summary|SUMMARY' \n",
    "    p = re.compile(re_section)\n",
    "    indices = [m.end() for m in p.finditer(full_text)]\n",
    "    if len(indices) > 0:\n",
    "        i_start = indices[0]\n",
    "    else:\n",
    "        i_start = 0\n",
    "        print('\\thas no \\'Abstract\\' section.')\n",
    "        \n",
    "    re_section = r'REFERENCE|Reference|Bibliography|BIBLIOGRAPHY|ACKNOWLEDGMENTS|Acknowledgments' \n",
    "    p = re.compile(re_section)\n",
    "    indices = [m.start(0) for m in p.finditer(full_text)]\n",
    "    if len(indices) > 0:\n",
    "        i_end = indices[-1]\n",
    "    else:\n",
    "        i_end = -1\n",
    "        print('\\thas no \\'References\\' section.')\n",
    "        \n",
    "    re_section = r'INTRODUCTION|Introduction|\\n1\\s' \n",
    "    p = re.compile(re_section)\n",
    "    indices = [m.start(0) for m in p.finditer(full_text)]\n",
    "    print(\"Introduction indicies: \", indices)\n",
    "    if len(indices) > 0:\n",
    "        i_end_abstract = indices[0]\n",
    "    else:\n",
    "        i_end_abstract = i_start\n",
    "        print('\\thas no \\'introduction\\' section.')\n",
    "\n",
    "    return full_text[i_start:i_end], full_text[i_start:i_end_abstract], langs\n",
    "\n",
    "\n",
    "\n",
    "class MongoDB(object):\n",
    "\n",
    "    def __init__(self, mongoclient_dict, database, collection):\n",
    "        if 'host' not in mongoclient_dict.keys():\n",
    "            mongoclient_dict['host'] = 'localhost'\n",
    "\n",
    "        client = MongoClient(**mongoclient_dict)\n",
    "        db = client[database]\n",
    "        col = db[collection]\n",
    "\n",
    "        self.mongoclient_dict = mongoclient_dict\n",
    "        self.client = client\n",
    "        self.db = db\n",
    "        self.col = col\n",
    "        return\n",
    "\n",
    "    def load_jsons(self, directory):\n",
    "        print('creating dataset...')\n",
    "        parsed_docs = {}\n",
    "        file_counter = 0\n",
    "        files = os.listdir(directory)\n",
    "        files.sort()\n",
    "        for filename in files:\n",
    "            #print('File: ' + str(file_counter) + '/' + str(len(os.listdir(directory))-1))\n",
    "            filepath = os.path.join(directory,filename)\n",
    "            #print(filepath)\n",
    "            file_counter += 1\n",
    "            if filename.endswith('.json'):\n",
    "                a_file = open(filepath, \"r\")\n",
    "                data = json.load(a_file)\n",
    "                #if data['content_normalized_compound_sentiment'] != 0:\n",
    "                parsed_docs[data[filename.replace('.json', '')]] = data\n",
    "                a_file.close()\n",
    "        print('dataset created.\\n') \n",
    "        self.jsons = parsed_docs\n",
    "        return parsed_docs\n",
    "\n",
    "    def jsons_update_db(self, directory=None):\n",
    "        if directory is not None:\n",
    "            self.load_jsons(directory)\n",
    "\n",
    "        vals = list(self.jsons.values())\n",
    "        for val in vals:\n",
    "            self.col.update_one(\n",
    "                {'sha_256' : val['sha_256']},\n",
    "                {'$set' : {k: v for k, v in val.items() if k != '_id'}},\n",
    "                upsert=True\n",
    "            )\n",
    "\n",
    "    def update_document(self, doc_dict):\n",
    "        self.col.update_one(\n",
    "                {'sha_256' : doc_dict['sha_256']},\n",
    "                {'$set' : {k: v for k, v in doc_dict.items() if k != '_id'}},\n",
    "                upsert=True\n",
    "            )\n",
    "\n",
    "    def get_document(self, sha256):\n",
    "        return list(self.col.find({'sha_256' : sha256}))\n",
    "\n",
    "    def get_all_documents(self):\n",
    "        cursor = self.col.find({})\n",
    "        return {document['sha_256'] : document for document in cursor}\n",
    "\n",
    "    def update_many_documents(self, parsed_docs):\n",
    "        if isinstance(parsed_docs, dict):\n",
    "            for val in parsed_docs.values():\n",
    "                self.update_document(val)\n",
    "        else:\n",
    "            for val in parsed_docs:\n",
    "                self.update_document(val)\n",
    "\n",
    "    def query_documents(self, query_dict):\n",
    "        return self.col.find(query_dict)\n",
    "\n",
    "    def remove_fields(self, fields, query_dict={}):\n",
    "        if isinstance(fields, dict):\n",
    "            self.col.update(\n",
    "                query_dict,\n",
    "                {'$unset' : fields}\n",
    "            )\n",
    "        else:\n",
    "            self.col.update(\n",
    "                query_dict,\n",
    "                {'$unset' : {field:'' for field in fields if field != '_id'}}\n",
    "            )\n",
    "        \n",
    "def get_title_text(filepath):\n",
    "    if not filepath.endswith('.pdf'):\n",
    "        print('File path not a pdf')\n",
    "        return None\n",
    "    \n",
    "    doc = fitz.open(filepath)\n",
    "\n",
    "    full_text = ''\n",
    "    page1 = ''\n",
    "    for i, page in enumerate(doc):\n",
    "        full_text += page.get_text('text')\n",
    "        if i == 0:\n",
    "            page1 = full_text\n",
    "        \n",
    "    re_section = r'ABSTRACT|Abstract|Summary|SUMMARY' \n",
    "    p = re.compile(re_section)\n",
    "    indices = [m.start() for m in p.finditer(full_text)]\n",
    "    if len(indices) > 0:\n",
    "        i_start = indices[0]\n",
    "        text = full_text[:i_start]\n",
    "    else:\n",
    "        print('\\thas no \\'Abstract\\' section.')\n",
    "        re_section = r'INTRODUCTION|Introduction|\\n1\\s' \n",
    "        p = re.compile(re_section)\n",
    "        indices = [m.start() for m in p.finditer(full_text)]\n",
    "        if len(indices) > 0:\n",
    "            i_start = indices[0]\n",
    "            text = full_text[:i_start]\n",
    "        else:\n",
    "            text = page1\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a30eecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    word (str) - word to find POS\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str - the POS tag\n",
    "    \"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "61ea27f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removing_meta_data(nlp, text, remove_tags=['PERSON', 'FAC', 'GPE', 'LOC', 'EVENT', 'WORK_OF_ART', 'LAW', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']):\n",
    "    '''\n",
    "    Removes meta data from text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nlp (spacy.en_core) - the spacy nlp model to find english tags/meta data\n",
    "    text (str) - text of which needs meta data removed\n",
    "    remove_tags (list) - list of entity name annotations/tags that are to be removed from \"text\" (default = ['PERSON', 'FAC', 'GPE', 'LOC', 'EVENT', 'WORK_OF_ART', 'LAW', 'DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL'])\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    str - the text with removed meta data\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    # 2.6 Entity Names Annotation\n",
    "    # Names (often referred to as “Named Entities”) are annotated according to the following\n",
    "    # set of types:\n",
    "    # PERSON People, including fictional\n",
    "    # NORP Nationalities or religious or political groups\n",
    "    # FACILITY Buildings, airports, highways, bridges, etc.\n",
    "    # ORGANIZATION Companies, agencies, institutions, etc.\n",
    "    # GPE Countries, cities, states\n",
    "    # LOCATION Non-GPE locations, mountain ranges, bodies of water\n",
    "    # PRODUCT Vehicles, weapons, foods, etc. (Not services)\n",
    "    # EVENT Named hurricanes, battles, wars, sports events, etc.\n",
    "    # WORK OF ART Titles of books, songs, etc.\n",
    "    # LAW Named documents made into laws \n",
    "    #  OntoNotes Release 5.0\n",
    "    # 22\n",
    "    # LANGUAGE Any named language\n",
    "    # The following values are also annotated in a style similar to names:\n",
    "    # DATE Absolute or relative dates or periods\n",
    "    # TIME Times smaller than a day\n",
    "    # PERCENT Percentage (including “%”)\n",
    "    # MONEY Monetary values, including unit\n",
    "    # QUANTITY Measurements, as of weight or distance\n",
    "    # ORDINAL “first”, “second”\n",
    "    # CARDINAL Numerals that do not fall under another typ\n",
    "    '''\n",
    "        \n",
    "    doc = nlp(text)\n",
    "    #str_tags = [(X.text, X.label_) for X in doc.ents]\n",
    "    removed = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PERSON' and len(ent.text.strip().split(' ')) < 2:\n",
    "            continue\n",
    "        elif ent.label_ in remove_tags:\n",
    "            ent_chars = {'text': ent.text, # The str of the named entity phrase.\n",
    "                         'start': ent.start_char, # Source str index of the first char.\n",
    "                         'end': ent.end_char, # Source str index of the last+1 char.\n",
    "                         'label': ent.label_} # A str label for the entity type.\n",
    "            text = text.replace(ent.text, ' ')\n",
    "            removed.append(ent_chars)\n",
    "    \n",
    "    return text, removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec763b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(df, col='Text', remove_meta_data=True, remove_punct=True, add_acronym_periods=False, lemmatize=True, replace_math=True, \n",
    "                            remove_non_alpha=True, replace_numbers=True, check_pos=True, remove_SW=True, remove_gibberish=True):\n",
    "    \"\"\"\n",
    "    Clean text according to input parameters\n",
    "    Input: df with intended text in \"Text\" column, as space-delimited string of words (Standard sentence)\n",
    "    \"\"\"\n",
    "    # df.Text stays as string through function until lemmatized \n",
    "    #print(\"Input:\")\n",
    "    #print(df.Text.head())\n",
    "    \n",
    "    df[col] = df[col].apply(lambda x: x.replace('-\\n', ''))\n",
    "    df[col] = df[col].apply(lambda x: x.replace('\\n', ' '))\n",
    "    \n",
    "    if replace_math:\n",
    "        print('removing math symbols and letters...')\n",
    "        df[col] = df[col].apply(lambda z: ' '.join(['' if regex.search(r'\\p{Greek}|\\p{S}', word) else word for word in z.split()]))\n",
    "        print('math symbols and letters removed.\\n')\n",
    "        \n",
    "    if replace_numbers:\n",
    "        print('removing stand allow digits (or figrue table numbers)...')\n",
    "        #p = inflect.engine()\n",
    "        #df[col] = df[col].apply(lambda x: ' '.join([re.sub(r'[−–-]\\d', '', word) for word in x.split()]))\n",
    "        df[col] = df[col].apply(lambda z: ' '.join(['' if re.search(r'\\d', word) else word for word in z.split()]))\n",
    "        #df[col] = df[col].apply(lambda z: ' '.join(['' if word.isdigit() or re.search(r'([0-9]){2,}[a-z]([0-9]){1,}', word) else word for word in z.split()]))\n",
    "        \n",
    "        print('stand alone digits removed.\\n')\n",
    "    \n",
    "    if check_pos:\n",
    "        print('keep only nouns...')\n",
    "#         tag_dict = {\"J\": wordnet.ADJ,\n",
    "#                     \"N\": wordnet.NOUN,\n",
    "#                     \"V\": wordnet.VERB,\n",
    "#                     \"R\": wordnet.ADV}\n",
    "        # Nouns only\n",
    "        tag_dict = {'N': wordnet.NOUN}\n",
    "\n",
    "        df[col] = df[col].swifter.apply(lambda z: ' '.join([word if tag_dict.get(nltk.pos_tag([word])[0][1][0].upper(), False) else '' for word in z.split()]))\n",
    "        print('nouns kept; everything else removed\\n')\n",
    "    \n",
    "    if remove_meta_data:\n",
    "        print('removing meta data...(Names, Dates, Places...)')\n",
    "        nlp = en_core_web_md.load()\n",
    "        df[col] = df[col].swifter.apply(lambda x: removing_meta_data(nlp, x)[0])\n",
    "        print('removed meta data.\\n')\n",
    "        \n",
    "    if remove_punct:\n",
    "        # Remove punctuation\n",
    "        print('removing punctuation...')\n",
    "        df[col] = df[col].apply(lambda x: x.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation))).replace('  ', ' ').strip())\n",
    "        print('punctuation removed.\\n')\n",
    "        \n",
    "    if add_acronym_periods:\n",
    "        # Add periods to any capital letter sequence since thats probably an acronym (This is for normalization package)\n",
    "        print('adding acronym periods...')\n",
    "        df[col] = df[col].apply(lambda s: ' '.join([''.join([c + \".\" if c.isalpha() else c for c in w])\n",
    "                                                              if w.isupper()\n",
    "                                                              else w\n",
    "                                                              for w in s.split()]))\n",
    "        print('acronym periods added.\\n')\n",
    "        \n",
    "        \n",
    "    if remove_non_alpha:\n",
    "        print('removing words with chars not a-zA-Z0-9...')\n",
    "        df[col] = df[col].apply(lambda x: ' '.join(['' if re.search(r'[^a-zA-Z\\d\\s]', word) else word for word in x.split()]))\n",
    "        print('removed words with chars not a-zA-Z0-9.\\n')\n",
    "        \n",
    "\n",
    "    # Lowercase \n",
    "    print('making lower-case...')\n",
    "    df[col] = df[col].map(lambda x: x.lower())\n",
    "    print(\"lower-cased.\\n\")\n",
    "    \n",
    "    \n",
    "    # It likes the stopwords to be lowercase before removing them\n",
    "    \n",
    "    if remove_SW:\n",
    "        # Remove le stop words\n",
    "        print('removing stop words...')\n",
    "        nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "        nltk_stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'table', 'figure', 'arxiv', 'sin', 'cos', 'tan', 'log', 'fx', 'ft', 'dx', 'dt', 'xt'])\n",
    "        # Tokenized text with standard stopwords and punct removed\n",
    "        df[col] = df[col].swifter.apply(lambda z: ' '.join([t for t in word_tokenize(z)\n",
    "                                                          if t not in nltk_stop_words]))\n",
    "        print('stop words removed.\\n')\n",
    "        \n",
    "    if lemmatize:\n",
    "        # Lemmatize\n",
    "        print('lemmitizing...')\n",
    "        lemmatizer=WordNetLemmatizer()\n",
    "        df[col] = df[col].swifter.apply(lambda z: ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in z.split()]))\n",
    "        print(\"lemmatized.\\n\")\n",
    "\n",
    "    \n",
    "    if remove_gibberish:\n",
    "        print('removing gibberish...')\n",
    "        \n",
    "        df[col] = df[col].apply(lambda z: ' '.join(['' if Detector.is_gibberish(word) else word for word in z.split()]))\n",
    "        print('gibberish removed.\\n')\n",
    "    \n",
    "    print('remove single letters or super large words (so big they don\\'t make sense)...')\n",
    "    df[col] = df[col].apply(lambda z: ' '.join([word for word in z.split() if len(word) < 20 and len(word) > 1]))\n",
    "    print('removed single letters and super large words.\\n')\n",
    "    print('done cleaning.\\n')\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b0f6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_most_frequent_words(top_n, list_text):\n",
    "    '''\n",
    "    Removes the most frequent words from the corpus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    top_n (int) - number of words to be removed\n",
    "    list_text (list of str) - list of texts representing the corpus\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of str, list of str - returns the list of texts from \"list_text\" and the list of the words removed.\n",
    "    '''\n",
    "    print('removing top %s most frequent words...'%top_n)\n",
    "    top_words = Counter(\" \".join(list_text).split()).most_common(top_n)\n",
    "    print(top_words)\n",
    "    for i in range(len(list_text)):\n",
    "        for top_word in top_words:\n",
    "            list_text[i] = re.sub(r'\\s%s\\s'%top_word[0], ' ', list_text[i]).replace('  ', ' ')\n",
    "    print('top words removed.\\n')\n",
    "    return list_text, top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c98d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_words(list_text):\n",
    "    '''\n",
    "    Stem words in corpus.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    list_text (list of str) - list of texts representing the corpus\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str - the list of the text with words stemmed from \"list_text\"\n",
    "    '''\n",
    "    print('stemming words...')\n",
    "    stem_dict = {}\n",
    "    snowball = SnowballStemmer(\"english\")\n",
    "    for i in range(len(list_text)):\n",
    "        temp = []\n",
    "        for w in list_text[i].split():\n",
    "            stemmed = snowball.stem(w)\n",
    "            temp.append(stemmed)\n",
    "            if stemmed in stem_dict.keys():\n",
    "                if w not in stem_dict[stemmed]:\n",
    "                    stem_dict[stemmed].append(w)\n",
    "            else:\n",
    "                stem_dict[stemmed] = [w]\n",
    "        list_text[i] = ' '.join(temp).replace('  ', ' ')\n",
    "    print('words stemmed.\\n')\n",
    "    return list_text, stem_dict\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "29214be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(texts):\n",
    "    '''\n",
    "    Calculates the IDF score of a corpus\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts (list of str) - the list of texts that represent the corpus\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple - a tuple of the idf score and the term associated with the score\n",
    "    '''\n",
    "    N = len(texts)\n",
    "    tD = Counter()\n",
    "    for d in texts:\n",
    "        features = d.split()\n",
    "        for f in features:\n",
    "            tD[f] += 1\n",
    "    IDF = []\n",
    "    for (term,term_frequency) in tD.items():\n",
    "        term_IDF = math.log(float(N) / term_frequency)\n",
    "        #term_IDF = term_frequency / float(N)\n",
    "        IDF.append(( term_IDF, term ))\n",
    "    IDF.sort(reverse=True)\n",
    "    return IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89ae6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms_idf_terms_to_remove(list_text, idf_thresh=-1.5):\n",
    "    '''\n",
    "    Get the terms to remove from idf score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_text (list of str) - the list of texts that represent the corpus\n",
    "    idf_thresh (float, optional) - the threshold for acceptable idf scores to keep; lower than threshold will return list to remove (default = -1.5)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list - the list of terms to remove\n",
    "    '''\n",
    "    print('remove idf terms less than', idf_thresh)\n",
    "    terms_to_remove = []\n",
    "    for (IDF, term) in calculate_idf(list_text):\n",
    "        if IDF < idf_thresh:\n",
    "            terms_to_remove.append((IDF, term))\n",
    "    return terms_to_remove\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a72de33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(list_text, terms_to_remove):\n",
    "    '''\n",
    "    Removes words from the corpus.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_text (list of str) - the list of texts that represent the corpus\n",
    "    terms_to_remove (list) - the list of words that are to be removed from the corpus\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of str, list - returns the list of texts with terms removed from terms_to_remove list and also the terms_to_remove list as well \n",
    "    '''\n",
    "    print('removing terms...')\n",
    "    for i in range(len(list_text)):\n",
    "        for term in terms_to_remove:\n",
    "            list_text[i] = re.sub(r'\\s%s\\s'%term[1], ' ', list_text[i]).replace('  ', ' ')\n",
    "    print('terms removed.\\n')\n",
    "    return list_text, terms_to_remove\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33c7ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(list_text, n=2, min_count=5, threshold=100):\n",
    "    '''\n",
    "    N-grams words from a corpus\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_text (list of str) - the list of texts that represent the corpus\n",
    "    n (int) - the number of \"grams\" allowed, for example, 2 = bigram and 3 = trigram ... (default = 2)\n",
    "    min_count (int) - the minimum number of times a pharse has to show up to be considered a phrase (defualt = 5).\n",
    "    threshold (int) - Represent a score threshold for forming the phrases (higher means fewer phrases). A phrase of words a followed by b is accepted if the score of the phrase is greater than threshold. Heavily depends on concrete scoring-function, see the scoring parameter. (default = 100)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list of str - the list of texts with the \"ngrammed\" words\n",
    "    '''\n",
    "    print('ngramming words...')\n",
    "    numbers = {2:'bigram', 3:'trigram', 4:'quadgram'}\n",
    "    texts = [sentence.split(' ') for sentence in list_text]\n",
    "    init_gram = {}\n",
    "    for i in range(2,n+1):\n",
    "        try:\n",
    "            print(numbers[i], 'model started...')\n",
    "        except:\n",
    "            print('%s-gram model started...')\n",
    "        ngram = gensim.models.Phrases(texts, min_count=min_count, threshold=threshold)\n",
    "        ngram_mod = gensim.models.phrases.Phraser(ngram)\n",
    "        texts = [ngram_mod[doc] for doc in texts]\n",
    "        \n",
    "#         if i == 2:\n",
    "#             ngram = gensim.models.Phrases(list_text, min_count=min_count, threshold=threshold)\n",
    "#             ngram_mod = gensim.models.phrases.Phraser(ngram)\n",
    "#             texts = [ngram_mod[doc] for doc in texts]\n",
    "#         else:\n",
    "#             ngram = gensim.models.Phrases(init_gram[i-1]['ngram'][text], threshold=threshold)\n",
    "#             ngram_mod = gensim.models.phrases.Phraser(ngram)\n",
    "#             texts = [ngram_mod[init_gram[i-1]['ngram_mod'][doc]] for doc in texts]\n",
    "            \n",
    "        try:\n",
    "            print(numbers[i], 'model finished.\\n')\n",
    "        except:\n",
    "            print('%s-gram model finished.\\n')\n",
    "            \n",
    "        init_gram[i] = {'ngram':ngram, 'ngram_mod':ngram_mod, 'texts':texts}\n",
    "    \n",
    "    return [' '.join(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf64cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sha256(content):\n",
    "    '''\n",
    "    Gets the sha256 code from the content of the entry\n",
    "    \n",
    "    Args:\n",
    "        content : the end of week entry content, specifically a string\n",
    "        \n",
    "    Returns:\n",
    "        string : sha256 code\n",
    "    '''\n",
    "    return hashlib.sha256(content.encode('utf-8')).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "16d3157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_dict(directory, filename):\n",
    "    '''\n",
    "    Extracts document name, abstract, original text, path, sha256 hash, language, language probability, and date (if arxiv document authors, title, and url as well).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory (str) - directory that contains the document\n",
    "    filename (str) - the name of the pdf file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict - a dictionary with the following keys: Document, Abstract, Text, Abstract_Original, Original_Text, Path, sha_256, laguage, language_probability, Authors, Tilte, url, date\n",
    "    '''\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    text, abstract_text, langs = get_text(filepath)\n",
    "    if text == '':\n",
    "        return {}\n",
    "    if langs == '':\n",
    "        language = ''\n",
    "        language_prob = 0.0\n",
    "    else:\n",
    "        language = langs[0].lang\n",
    "        language_prob = langs[0].prob\n",
    "    sha256 = get_sha256(text)\n",
    "    date = get_document_date(filepath, is_arxiv=True)\n",
    "    section_dict = {'Document': filename,\n",
    "                    'Abstract': abstract_text,\n",
    "                    'Text' : text,\n",
    "                    'Abstract_Original': abstract_text,\n",
    "                    'Original_Text' : text,\n",
    "                    'Path' : filepath,\n",
    "                    'sha_256': sha256,\n",
    "                    'language' : language,\n",
    "                    'language_probability' : language_prob,\n",
    "                    'Authors' : '',\n",
    "                    'Title' : '',\n",
    "                    'url' : '',\n",
    "                    'date' : date}\n",
    "    if os.path.join(directory, filename.replace('.pdf', '')+'_entities.json') in glob.glob(os.path.join(directory, '*.json')):\n",
    "        print(\"Entities file already exists\")\n",
    "        with open(os.path.join(directory, filename.replace('.pdf', '')+'_entities.json'), 'r') as f:\n",
    "            arxiv_entities = json.load(f)\n",
    "        return {**section_dict, **arxiv_entities}\n",
    "    elif re.search(r'(\\d){4}\\.(\\d){5}', filename):\n",
    "        try:\n",
    "            print(\"trying to get entities\")\n",
    "            arxiv_entities = get_arxiv_entities(re.search(r'(\\d){4}\\.(\\d){5}', filename)[0])\n",
    "            return {**section_dict, **arxiv_entities}\n",
    "        except:\n",
    "            print(\"failed to get entities\")\n",
    "            pass\n",
    "    return section_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6e8b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_clean_pdf(directory, filename, stem=False, abstract=True, idf_thresh=-1.5):\n",
    "    '''\n",
    "    Parses and cleans a pdf file by extracting document name, abstract, normalized text, original text, normalized abstract, path, sha256 hash, language, language probability, and date (if arxiv document authors, title, and url as well).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory (str) - directory that contains the document\n",
    "    filename (str) - the name of the pdf file\n",
    "    stem (bool, optional) - True means to stem the words and False is to leave as is (default = False)\n",
    "    abstract (bool, optional) - True means to normalize the abstract (default = False)\n",
    "    idf_thresh (float, optional) - the threshold for acceptable idf scores to keep; lower than threshold will return list to remove (default = -1.5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict - a dictionary with the following keys: Document, Abstract, Text, Abstract_Original, Original_Text, Path, sha_256, laguage, language_probability, Authors, Tilte, url, date\n",
    "    '''\n",
    "    filepath = os.path.join(directory,filename)\n",
    "    if filepath.endswith('.pdf'):\n",
    "        pdf_dict = pdf_to_dict(directory, filename)\n",
    "        if pdf_dict == {}:\n",
    "            return {}\n",
    "        df = pd.DataFrame([pdf_dict])\n",
    "        df = cleanup(df, col='Text', remove_meta_data=True, remove_punct=True, add_acronym_periods=False, lemmatize=True, replace_math=True, \n",
    "                            remove_non_alpha=True, replace_numbers=True, check_pos=False, remove_SW=True, remove_gibberish=True)\n",
    "        if stem:\n",
    "            df.Text, _ = stem_words(df.Text)\n",
    "        if idf_thresh is not None:\n",
    "            idf_terms_to_remove = get_terms_idf_terms_to_remove(df.Text, idf_thresh=idf_thresh)\n",
    "            df.Text, idf_terms_to_remove = remove_words(df.Text, idf_terms_to_remove)\n",
    "\n",
    "        if abstract:\n",
    "            df = cleanup(df, col='Abstract', remove_meta_data=True, remove_punct=True, add_acronym_periods=False, lemmatize=True, replace_math=True, \n",
    "                                remove_non_alpha=True, replace_numbers=True, check_pos=True, remove_SW=True, remove_gibberish=True)\n",
    "            if stem:\n",
    "                df.Abstract, _ = stem_words(df.Abstract)\n",
    "            if idf_thresh is not None:\n",
    "                df.Abstract, _ = remove_words(df.Abstract, idf_terms_to_remove)\n",
    "        \n",
    "        return df.T.to_dict().values()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def parse_clean_pdfs(directory, for_training=True, n_most_freq_words_remove=10, stem=False, abstract=True, idf_thresh=-1.5):#, storage_path=None):\n",
    "    '''\n",
    "    Parses and cleans a directory of pdfs by extracting from each: document name, abstract, normalized text, original text, normalized abstract, path, sha256 hash, language, language probability, and date (if arxiv document authors, title, and url as well).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory (str) - directory that contains the pdf documents\n",
    "    for_training (bool, optional) - True if this is parse and cleaning is done for training models, False if this is used for testing or production (default = True)\n",
    "    n_most_freq_words_remove (int, optional) - the number of most popular words to be removed from the corpus, which will only run if for_training = True (default = 10)\n",
    "    stem (bool, optional) - True means to stem the words and False is to leave as is (default = False)\n",
    "    abstract (bool, optional) - True means to normalize the abstract (default = False)\n",
    "    idf_thresh (float, optional) - the threshold for acceptable idf scores to keep; lower than threshold will return list to remove (default = -1.5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dicts - a list of dictionary with the following keys: Document, Abstract, Text, Abstract_Original, Original_Text, Path, sha_256, laguage, language_probability, Authors, Tilte, url, date\n",
    "    '''\n",
    "    \n",
    "    all_docx = os.listdir(directory)\n",
    "    #all_docx.sort()\n",
    "    #print(all_docx)\n",
    "    \n",
    "    docs = []\n",
    "    \n",
    "    \n",
    "    for filepath in all_docx:\n",
    "        if filepath.endswith('.pdf'):\n",
    "            print(os.path.join(directory, filepath))\n",
    "            pdf_dict = pdf_to_dict(directory, filepath)\n",
    "            if pdf_dict == {}:\n",
    "                continue\n",
    "            docs.append(pdf_dict)\n",
    "            \n",
    "    df = pd.DataFrame(docs)\n",
    "    print('check point: df made')\n",
    "    print('processing text...')\n",
    "    df = cleanup(df, col='Text', remove_meta_data=True, remove_punct=True, add_acronym_periods=False, lemmatize=True, replace_math=True, \n",
    "                            remove_non_alpha=True, replace_numbers=True, check_pos=True, remove_SW=True, remove_gibberish=True)\n",
    "    if stem:\n",
    "        df.Text, stem_dict = stem_words(df.Text)\n",
    "    if idf_thresh is not None:\n",
    "        idf_terms_to_remove = get_terms_idf_terms_to_remove(df.Text, idf_thresh=idf_thresh)\n",
    "        df.Text, idf_terms_to_remove = remove_words(df.Text, idf_terms_to_remove)\n",
    "    df.Text = ngram(df.Text, n=3)\n",
    "    print('processed text.\\n')\n",
    "    \n",
    "    if abstract:\n",
    "        print('processing abstract...')\n",
    "        df = cleanup(df, col='Abstract', remove_meta_data=True, remove_punct=True, add_acronym_periods=False, lemmatize=True, replace_math=True, \n",
    "                            remove_non_alpha=True, replace_numbers=True, check_pos=True, remove_SW=True, remove_gibberish=True)\n",
    "        if stem:\n",
    "            df.Abstract, stem_dict = stem_words(df.Abstract)\n",
    "        if idf_thresh is not None:\n",
    "                df.Abstract, _ = remove_words(df.Abstract, idf_terms_to_remove)\n",
    "        df.Abstract = ngram(df.Abstract, n=3)\n",
    "        print('processed abstract.\\n')\n",
    "        \n",
    "    if for_training:\n",
    "        if isinstance(n_most_freq_words_remove, int) and n_most_freq_words_remove > 0:\n",
    "            df.Text, removed_words = remove_most_frequent_words(n_most_freq_words_remove, df.Text)\n",
    "            print('removed words:', removed_words)\n",
    "            with open(os.path.join(directory, 'removed_top_n_words.json'), 'w') as j_file:\n",
    "                json.dump(removed_words, j_file, indent=4)\n",
    "        if stem:\n",
    "            with open(os.path.join(directory, 'stem_dict.json'), 'w') as j_file:\n",
    "                json.dump(stem_dict, j_file, indent=4)\n",
    "            \n",
    "    return df.T.to_dict().values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ae46f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsed_pdf_to_dict(directory, for_training=True, n_most_freq_words_remove=10, stem=False, abstract=True, idf_thresh=-1.5):\n",
    "    '''\n",
    "    Parses and cleans a directory of pdfs by extracting from each: document name, abstract, normalized text, original text, normalized abstract, path, sha256 hash, language, language probability, and date (if arxiv document authors, title, and url as well).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory (str) - directory that contains the pdf documents\n",
    "    for_training (bool, optional) - True if this is parse and cleaning is done for training models, False if this is used for testing or production (default = True)\n",
    "    n_most_freq_words_remove (int, optional) - the number of most popular words to be removed from the corpus, which will only run if for_training = True (default = 10)\n",
    "    stem (bool, optional) - True means to stem the words and False is to leave as is (default = False)\n",
    "    abstract (bool, optional) - True means to normalize the abstract (default = False)\n",
    "    idf_thresh (float, optional) - the threshold for acceptable idf scores to keep; lower than threshold will return list to remove (default = -1.5)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict of dicts - a dictionary with the each sha256 hash of the orginal text as the key of each dictionary with the following keys: Document, Abstract, Text, Abstract_Original, Original_Text, Path, sha_256, laguage, language_probability, Authors, Tilte, url, date\n",
    "    '''\n",
    "    if directory.endswith('.pdf'):\n",
    "        parsed_list = [parse_clean_pdf(directory='.', filename=directory, stem=stem, abstract=abstract, idf_thresh=idf_thresh)]\n",
    "    else:\n",
    "        parsed_list = parse_clean_pdfs(directory, for_training, n_most_freq_words_remove, stem=stem, abstract=abstract, idf_thresh=idf_thresh)\n",
    "    \n",
    "    return {doc['sha_256'] : doc for doc in parsed_list}\n",
    "\n",
    "def parsed_pdf_to_json(directory, storage_dir='./parsed_cleaned_pdfs', for_training=True, n_most_freq_words_remove=10, stem=False, abstract=True, idf_thresh=-1.5):\n",
    "    '''\n",
    "    Parses and cleans and stores jsons of a directory of pdfs by extracting from each: document name, abstract, normalized text, original text, normalized abstract, path, sha256 hash, language, language probability, and date (if arxiv document authors, title, and url as well).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory (str) - directory that contains the pdf documents\n",
    "    storage_dir (str, optional) - the directory that the jsons of parsed documents will be stored\n",
    "    for_training (bool, optional) - True if this is parse and cleaning is done for training models, False if this is used for testing or production (default = True)\n",
    "    n_most_freq_words_remove (int, optional) - the number of most popular words to be removed from the corpus, which will only run if for_training = True (default = 10)\n",
    "    stem (bool, optional) - True means to stem the words and False is to leave as is (default = False)\n",
    "    abstract (bool, optional) - True means to normalize the abstract (default = False)\n",
    "    idf_thresh (float, optional) - the threshold for acceptable idf scores to keep; lower than threshold will return list to remove (default = -1.5)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(storage_dir)\n",
    "    except (FileExistsError):\n",
    "        pass\n",
    "        \n",
    "    parsed_dict = parsed_pdf_to_dict(directory, for_training, n_most_freq_words_remove, stem=stem, abstract=abstract, idf_thresh=idf_thresh)\n",
    "    \n",
    "    for doc in parsed_dict.values():\n",
    "        with open(os.path.join(storage_dir, str(doc['sha_256'])+'.json'), 'w') as j_file:\n",
    "                json.dump(doc, j_file, indent=4)\n",
    "\n",
    "def parsed_pdf_to_db(directory, mongoclient_dict, database, collection, for_training=True, n_most_freq_words_remove=10, stem=False, abstract=True, idf_thresh=-1.5):\n",
    "    '''\n",
    "    Parses and cleans and stores dictionaries in mongoDB database of a directory of pdfs by extracting from each: document name, abstract, normalized text, original text, normalized abstract, path, sha256 hash, language, language probability, and date (if arxiv document authors, title, and url as well).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory (str) - directory that contains the pdf documents\n",
    "    mongoclient_dict (dict) - the mongoDB client dictionary containing exceptable inputs to pymongo.MongoClient class.  Examples are host, username, password, authSource, etc. \n",
    "    database (str) - the database in the mongoDB host\n",
    "    collection (str) - the collection in the database to populate\n",
    "    for_training (bool, optional) - True if this is parse and cleaning is done for training models, False if this is used for testing or production (default = True)\n",
    "    n_most_freq_words_remove (int, optional) - the number of most popular words to be removed from the corpus, which will only run if for_training = True (default = 10)\n",
    "    stem (bool, optional) - True means to stem the words and False is to leave as is (default = False)\n",
    "    abstract (bool, optional) - True means to normalize the abstract (default = False)\n",
    "    idf_thresh (float, optional) - the threshold for acceptable idf scores to keep; lower than threshold will return list to remove (default = -1.5)\n",
    "\n",
    "    '''\n",
    "    parsed_dict = parsed_pdf_to_dict(directory, for_training, n_most_freq_words_remove, stem=stem, abstract=abstract, idf_thresh=idf_thresh)\n",
    "\n",
    "    mongo = MongoDB(mongoclient_dict, database, collection)\n",
    "    mongo.update_many_documents(parsed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9d1d0386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction indicies:  [424, 1057, 4557, 4758, 4803, 4814, 7869, 8596, 8718, 8869]\n",
      "trying to get entities\n",
      "failed to get entities\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Document': '2101.00005.pdf',\n",
       " 'Abstract': '\\nWhile teaching a course on integral equations, I noticed that a straight-\\nforward combination of Neumann series and Fourier series for the resolvent\\n(or the solution) of an integral equation has good approximation qualities.\\nThis short article presents and investigates this combination of approxi-\\nmating series.',\n",
       " 'Text': '\\nWhile teaching a course on integral equations, I noticed that a straight-\\nforward combination of Neumann series and Fourier series for the resolvent\\n(or the solution) of an integral equation has good approximation qualities.\\nThis short article presents and investigates this combination of approxi-\\nmating series.\\n1\\nThe mixed series\\nFredhol’ms integral equation of the second kind\\ny(x) = f(x) + λ\\n� b\\na\\nK(x, t)y(t)dt\\n(1)\\nwith K(x, t) ∈ L2([a, b]2) can be solved [Kan97, Ch. 3] in the resolvent form\\ny(x) = f(x) + λ\\n� b\\na\\nR(x, t, λ)f(t)dt.\\n(2)\\nThe resolvent R(x, t, λ) can be expressed as the Neumann series\\nR(x, t, λ) =\\n∞\\n�\\nn=1\\nλn−1Kn(x, t)\\n(3)\\nin terms of the iterated kernels:\\nK1(x, t) = K(x, t),\\n(4)\\nKn(x, t) =\\n� b\\na\\nK(x, ξ)Kn−1(ξ, t)dξ.\\n(5)\\nThe resolvent is a meromorphic function of λ ∈ C, with only λ = ∞ as a possible\\nlimiting point. The poles are the eigenvalues of the corresponding homogeneous\\nequation\\nψ(x) = λ\\n� b\\na\\nK(x, t)ψ(t)dt.\\n(6)\\n1\\narXiv:2101.00005v1  [math.CA]  29 Dec 2020\\nThe Neumann series converges for |λ| < 1/||K||, or up to the nearest pole. The\\nintegral equation (1) has a unique solution when λ is not an eigenvalue [Kan97,\\nCh. 4.2].\\nIf the kernel K(x, t) ∈ L2([a, b]2) is symmetric, that is, if K(x, t) = K(t, x),\\nthen we have the Fourier series expression\\nR(x, t, λ) = K(x, t) + λ\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλk (λk − λ)\\n(7)\\nin terms of an orthonormal system ψ1(x), ψ2(x), . . . of eigenfunctions [Kan97,\\nCh. 7] with the corresponding eigenvalues λ1, λ2, . . . of the homogeneous Fred-\\nholm equation (6). The Fourier series converges absolutely and almost uniformly\\nwhen λ is not an eigenvalue [Tri57, p. 115]. And we have the bilinear series\\nK(x, t) =\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλk\\n(8)\\nthat converges in the L2-norm generally. Accordingly, we can modify (7) to\\nR(x, t, λ) =\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλk − λ\\n,\\n(9)\\nbut the convergence is weaker.\\nOn the other hand, we can improve convergence of (7) by subtracting known\\nFourier series whose terms behave asymptotically as ∼ 1/λ2\\nk. Suitable Fourier\\nseries are for the iterated kernels:\\nKn(x, t) =\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλn\\nk\\n.\\n(10)\\nFor n ≥ 2, these series converges absolutely and almost uniformly. Employing\\nthe series with n = 2, we obtain\\nR(x, t, λ) = K(x, t) + λK2(x, t) + λ2\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλ2\\nk (λk − λ),\\n(11)\\nwith the Fourier terms behaving asymptotically as ∼ 1/λ3\\nk. Next, employing\\nK3(x, t) we obtain\\nR(x, t, λ) = K(x, t) + λK2(x, t) + λ2K3(x, t) + λ3\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλ3\\nk (λk − λ).\\nGenerally, we have\\nR(x, t, λ) =\\nm\\n�\\nn=1\\nλn−1Kn(x, t) + λm\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλm\\nk (λk − λ).\\n(12)\\n2\\nThe obvious practical advantage is that convergence of the Fourier series im-\\nproves with increased m. The resolvent can be eﬀectively approximated by a\\nfew terms of the Neumann series plus several terms of Fourier series. Even if λ\\nis near some large eigenvalue λn, the terms with |λk| > |λn| diminish quickly.\\nFrom the probed literature it appears that the closest topic that may suggest\\nformulas (11), (12) is “acceleration of convergence of Fourier series” or “over-\\ncoming the Gibbs phenomenon” by means of polynomial subtraction [Adc11,\\n§5.1], [Ner19]. The Neumann series part in (12) is often polynomial, as the ex-\\nample of the following section illustrates. The term “Fourier-Neumann series”\\nhas been used to refer to Fourier series in terms of Bessel functions [CV09].\\n2\\nAn example\\nConsider the integral equation\\ny(x) = x + λ\\n� 1\\n0\\nmin(x, t)y(t)dt,\\n(13)\\nas in [Kle11, p. 124–127].\\nThe eigenvalues and eigenfunctions are found by\\nsolving the homogeneous equation\\nψ(x) = λ\\n� 1\\n0\\nmin(x, t)ψ(t)dt,\\n(14)\\nor the equivalent diﬀerential equation with boundary conditions:\\nψ′′(x) + λψ(x) = 0,\\nψ(0) = 0,\\nψ′(1) = 0.\\n(15)\\nThe eigenvalues and the corresponding normalized eigenfunctions are parametrized\\nby n = 1, 2, . . .:\\nλn =\\n�\\nn − 1\\n2\\n�2 π2,\\nψn =\\n√\\n2 sin\\n�\\nn − 1\\n2\\n�\\nπx.\\n(16)\\nBy (9), the resolvent is\\nR(x, t, λ) = 2\\n∞\\n�\\nn=1\\nsin\\n�\\nn − 1\\n2\\n�\\nπx sin\\n�\\nn − 1\\n2\\n�\\nπt\\n�\\nn − 1\\n2\\n�2 π2 − λ\\n.\\n(17)\\nBy (2), we have\\ny(x) = x + λ\\n∞\\n�\\nn=1\\n2(−1)n+1 sin\\n�\\nn − 1\\n2\\n�\\nπx\\n�\\nn − 1\\n2\\n�2π2\\n��\\nn − 1\\n2\\n�2π2 − λ\\n�.\\n(18)\\nTo compute the iterated kernels for (12), let �Kn(x, t) denote the restriction of\\nKn(x, t) to t ≤ x. The iterated kernels are symmetric just as K(x, t), and\\n�Kn+1(x, t) =\\n� t\\n0\\nξ �Kn(t, ξ)dξ +\\n� x\\nt\\nξ �Kn(ξ, t)dξ + x\\n� 1\\nx\\n�Kn(ξ, t)dξ\\n(19)\\n3\\nby (5). Starting from �K1(x, t) = t, we compute\\n�K2(x, t) = xt − 1\\n2 x2t − 1\\n6 t3,\\n(20)\\n�K3(x, t) = 1\\n3 xt − 1\\n6 x3t − 1\\n6 xt3 + 1\\n24 x4t + 1\\n12 x2t3 +\\n1\\n120 t5.\\n(21)\\nWe compute the initial integrals\\nJn(x) =\\n� 1\\n0\\nKn(x, t)f(t)dt =\\n� x\\n0\\n�Kn(x, t)f(t)dt +\\n� 1\\nx\\n�Kn(t, x)f(t)dt\\n(22)\\nwith f(x) = x:\\nJ1(x) = 1\\n2 x − 1\\n6 x3,\\n(23)\\nJ2(x) =\\n5\\n24 x − 1\\n12 x3 +\\n1\\n120 x5,\\n(24)\\nJ3(x) =\\n61\\n720 x −\\n5\\n144 x3 +\\n1\\n240 x5 −\\n1\\n5040 x7.\\n(25)\\nWe get the solution in subsequently quicker approximations:\\ny(x) = x + λJ1(x) + λ2\\n∞\\n�\\nn=1\\n2(−1)n+1 sin\\n�\\nn − 1\\n2\\n�\\nπx\\n�\\nn − 1\\n2\\n�4π4\\n��\\nn − 1\\n2\\n�2π2 − λ\\n�,\\n(26)\\ny(x) = x + λJ1(x) + λ2J2(x) + λ3\\n∞\\n�\\nn=1\\n2(−1)n+1 sin\\n�\\nn − 1\\n2\\n�\\nπx\\n�\\nn − 1\\n2\\n�6π6\\n��\\nn − 1\\n2\\n�2π2 − λ\\n�,\\n(27)\\ny(x) = x + λJ1(x) + λ2J2(x) + λ3J3(x)\\n+ λ4\\n∞\\n�\\nn=1\\n2(−1)n+1 sin\\n�\\nn − 1\\n2\\n�\\nπx\\n�\\nn − 1\\n2\\n�8π8\\n��\\nn − 1\\n2\\n�2π2 − λ\\n�.\\n(28)\\nThe solution exists only if and only if λ is not an eigenvalue. For λ > 0 or λ < 0,\\nan elementary expression for the solution is, respectively,\\ny(x) =\\nsin\\n√\\nλ x\\n√\\nλ cos\\n√\\nλ\\nor\\ny(x) =\\nsinh\\n√\\n−λ x\\n√\\n−λ cosh\\n√\\n−λ.\\n(29)\\nThe poles of these solutions at the eigenvalues λ are well captured by the re-\\nspective Fourier terms. With those Fourier terms, the approximation error can\\nbe seen as a continuous function of λ.\\nWith explicit solutions at hand, we may analyze approximation quality of\\ndiﬀerently truncated mixed Neumann-Fourier series (18), (26)–(28). Figure 1\\nand Table 1 represent a batch of relevant data. Figure 1 demonstrates how\\nadding just one Neumann series term diminishes the approximation error by a\\nfactor ≈ 100. The approximation error is largest at the endpoint x = 1, because\\nthe non-homogeneous term f(x) = x in (13) does not satisfy the boundary\\ncondition (15) at x = 1, resulting in a weak version of the Gibbs phenomenon\\n[Adc11, §4.2]. Table 1 suggests that adding a Neumann series term is more\\nworthwhile than adding a Fourier term, unless λ becomes large while the number\\nof Fourier terms lags the number of Neumann terms. The approximation errors\\nfor negative λ are of the same order as respectively those for |λ|.\\n4\\nFigure 1: Approximation error for the truncated series in (18) and (26) with 4\\n(dashed line), 5 (dotted line) or 6 (solid line) Fourier terms, for λ = 4, x ∈ [0, 1].\\nn\\nλ = 2\\nλ = 5\\nλ = 10\\nλ = 20\\nλ = 50\\nλ = 100\\n(18)\\n3\\n4.9 10−4\\n1.2 10−3\\n2.6 10−3\\n5.5 10−3\\n1.8 10−2\\n9.6 10−2\\n4\\n2.1 10−4\\n5.3 10−4\\n1.1 10−3\\n2.2 10−3\\n6.4 10−3\\n1.7 10−2\\n5\\n1.1 10−4\\n2.7 10−4\\n5.5 10−4\\n1.1 10−3\\n3.1 10−3\\n7.1 10−3\\n6\\n6.3 10−5\\n1.6 10−4\\n3.2 10−4\\n6.5 10−4\\n1.7 10−3\\n3.8 10−3\\n(26)\\n3\\n6.1 10−6\\n3.9 10−5\\n1.6 10−4\\n7.1 10−4\\n6.0 10−3\\n7.2 10−2\\n4\\n1.5 10−6\\n9.6 10−6\\n3.9 10−5\\n1.7 10−4\\n1.2 10−3\\n6.8 10−3\\n5\\n5.1 10−7\\n3.2 10−6\\n1.3 10−5\\n5.4 10−5\\n3.7 10−4\\n1.8 10−3\\n6\\n2.1 10−7\\n1.3 10−6\\n5.3 10−6\\n2.2 10−5\\n1.4 10−4\\n6.5 10−4\\n(27)\\n3\\n8.9 10−8\\n1.4 10−6\\n1.2 10−5\\n1.0 10−4\\n2.3 10−3\\n5.7 10−2\\n4\\n1.3 10−8\\n2.1 10−7\\n1.7 10−6\\n1.4 10−5\\n2.6 10−4\\n3.0 10−3\\n5\\n2.8 10−9\\n4.5 10−8\\n3.6 10−7\\n3.0 10−6\\n5.2 10−5\\n5.1 10−4\\n6\\n8.1 10−10\\n1.3 10−8\\n1.0 10−7\\n8.4 10−7\\n1.4 10−5\\n1.3 10−4\\n(28)\\n3\\n1.4 10−9\\n5.5 10−8\\n9.2 10−7\\n1.6 10−5\\n8.9 10−4\\n4.6 10−2\\n4\\n1.2 10−10\\n4.7 10−9\\n7.7 10−8\\n1.3 10−6\\n6.0 10−5\\n1.4 10−3\\n5\\n1.7 10−11\\n6.7 10−10\\n1.1 10−8\\n1.8 10−7\\n7.8 10−6\\n1.5 10−4\\n6\\n3.4 10−12\\n1.3 10−10\\n2.2 10−9\\n3.6 10−8\\n1.5 10−6\\n2.7 10−5\\nTable 1: Approximation errors at x = 1 for the truncated series (18), (26)–(28)\\nwith n ∈ {3, 4, 5, 6} Fourier terms.\\n5\\n3\\nMore formulas\\nLet us consider the interplay of Fourier series with Fredholm’s representation\\nR(x, t, λ) = D(x, t, λ)\\n∆(λ)\\n,\\n(30)\\nwhere\\n∆(λ) =\\n∞\\n�\\nk=1\\n�\\n1 − λ\\nλk\\n�\\n(31)\\nis an analytic function with the zeroes exactly at the eigenvalues [Kan97, Ch. 4].\\nThe analytic series in λ of ∆(x) and D(x, t, λ) converge on the whole C when the\\nkernel K(x, t) is L2-integrable. The results are unremarkable for approximation\\npurposes, but there is some representational appeal.\\nThe power series coeﬃcients of\\n∆(λ) =\\n∞\\n�\\nn=0\\ncnλn,\\nD(x, t, λ) =\\n∞\\n�\\nn=0\\nCn(x, t)λn,\\n(32)\\ncan be computed [Kan97, (4.2.9)–(4.2.13a)] from the recurrence relations\\ncn = − 1\\nn\\n� b\\na\\nCn−1(x, x)dx,\\n(33)\\nCn(x, t) = cnK(x, t) +\\n� b\\na\\nK(x, ξ)Cn−1(ξ, t)dξ,\\n(34)\\nand the initial conditions c0 = 1, C0(x, t) = K(x, t). Keeping in mind orthonor-\\nmality of the ψk(ξ)’s, manipulation of the Fourier series gives\\nc1 = −\\n∞\\n�\\nk=1\\n1\\nλk\\n,\\n(35)\\nC1(x, t) = −\\n∞\\n�\\nk=1\\n�\\nj̸=k\\nψj(x)ψj(t)\\nλkλj\\n(36)\\n= −\\n� �\\nj<k\\nψj(x)ψj(t) + ψk(t)ψk(t)\\nλjλk\\n,\\n(37)\\nc2 =\\n� �\\nj<k\\n1\\nλjλk\\n,\\n(38)\\nC2(x, t) =\\n� �\\nj<k\\n�\\ni̸∈{j,k}\\nψi(x)ψi(t)\\nλjλkλi\\n(39)\\n=\\n� � �\\ni<j<k\\nψi(x)ψi(t) + ψj(t)ψj(t) + ψk(t)ψk(t)\\nλiλjλk\\n,\\n(40)\\nc3 = −\\n� � �\\ni<j<k\\n1\\nλiλjλk\\n,\\n(41)\\n6\\netc. The pattern\\ncn = (−1)n\\n�\\nS⊂N & |S|=n\\n� �\\nj∈S\\nλj\\n�−1\\n,\\n(42)\\nCn(x, t) = (−1)n\\n�\\nS⊂N & |S|=n\\n� �\\nj∈S\\nλj\\n�−1 �\\nj∈S\\nψj(x)ψj(t).\\n(43)\\nis apparent from (31) and eventually from D(x, t, λ) = ∆(λ)R(x, t, λ) as well.\\nTowards an analogy to (12), we have\\nD(x, t, λ) =\\nm−1\\n�\\nn=0\\nλnCn(x, t) +\\n�\\nS⊂N & |S|≥m\\n(−λ)|S|\\n�\\nj∈S λj\\n�\\nj∈S\\nψj(x)ψj(t).\\n(44)\\nBut convergence is expected to be poor generally. If the series (35) for c1 does\\nnot converge, the series for the other cn should not converge (absolutely at least)\\neither. The series for Cn(x, t) should then converge only in the L2-norm.\\nAs an intermediate alternative to (12) and (44) one may consider\\n(1 − αλ) R(x, t, λ) = K(x, t) +\\nm−1\\n�\\nn=1\\nλn�\\nKn+1(x, t) − αKn(x, t)\\n�\\n+ λm\\n∞\\n�\\nk=1\\n(1 − αλk) ψk(x) ψk(t)\\nλm\\nk (λk − λ)\\n.\\n(45)\\nThis modiﬁcation could be useful for λ near some large eigenvalue λk, taking\\nα = 1/λk and annihilating the kth term in the Fourier series. Similar expressions\\ncan be obtained for R(x, t, λ) multiplied by several such factors (1−αjλ) annihi-\\nlating several peaking Fourier terms. But the Fourier terms in (45) decrease as\\nO(1/λm\\nj ) rather than O(1/λm+1\\nj\\n) in (12). Eliminating a peaking Fourier term\\ncomes at the cost of one added Neumann series term for the purpose of better\\nconvergence of the Fourier series.\\n',\n",
       " 'Abstract_Original': '\\nWhile teaching a course on integral equations, I noticed that a straight-\\nforward combination of Neumann series and Fourier series for the resolvent\\n(or the solution) of an integral equation has good approximation qualities.\\nThis short article presents and investigates this combination of approxi-\\nmating series.',\n",
       " 'Original_Text': '\\nWhile teaching a course on integral equations, I noticed that a straight-\\nforward combination of Neumann series and Fourier series for the resolvent\\n(or the solution) of an integral equation has good approximation qualities.\\nThis short article presents and investigates this combination of approxi-\\nmating series.\\n1\\nThe mixed series\\nFredhol’ms integral equation of the second kind\\ny(x) = f(x) + λ\\n� b\\na\\nK(x, t)y(t)dt\\n(1)\\nwith K(x, t) ∈ L2([a, b]2) can be solved [Kan97, Ch. 3] in the resolvent form\\ny(x) = f(x) + λ\\n� b\\na\\nR(x, t, λ)f(t)dt.\\n(2)\\nThe resolvent R(x, t, λ) can be expressed as the Neumann series\\nR(x, t, λ) =\\n∞\\n�\\nn=1\\nλn−1Kn(x, t)\\n(3)\\nin terms of the iterated kernels:\\nK1(x, t) = K(x, t),\\n(4)\\nKn(x, t) =\\n� b\\na\\nK(x, ξ)Kn−1(ξ, t)dξ.\\n(5)\\nThe resolvent is a meromorphic function of λ ∈ C, with only λ = ∞ as a possible\\nlimiting point. The poles are the eigenvalues of the corresponding homogeneous\\nequation\\nψ(x) = λ\\n� b\\na\\nK(x, t)ψ(t)dt.\\n(6)\\n1\\narXiv:2101.00005v1  [math.CA]  29 Dec 2020\\nThe Neumann series converges for |λ| < 1/||K||, or up to the nearest pole. The\\nintegral equation (1) has a unique solution when λ is not an eigenvalue [Kan97,\\nCh. 4.2].\\nIf the kernel K(x, t) ∈ L2([a, b]2) is symmetric, that is, if K(x, t) = K(t, x),\\nthen we have the Fourier series expression\\nR(x, t, λ) = K(x, t) + λ\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλk (λk − λ)\\n(7)\\nin terms of an orthonormal system ψ1(x), ψ2(x), . . . of eigenfunctions [Kan97,\\nCh. 7] with the corresponding eigenvalues λ1, λ2, . . . of the homogeneous Fred-\\nholm equation (6). The Fourier series converges absolutely and almost uniformly\\nwhen λ is not an eigenvalue [Tri57, p. 115]. And we have the bilinear series\\nK(x, t) =\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλk\\n(8)\\nthat converges in the L2-norm generally. Accordingly, we can modify (7) to\\nR(x, t, λ) =\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλk − λ\\n,\\n(9)\\nbut the convergence is weaker.\\nOn the other hand, we can improve convergence of (7) by subtracting known\\nFourier series whose terms behave asymptotically as ∼ 1/λ2\\nk. Suitable Fourier\\nseries are for the iterated kernels:\\nKn(x, t) =\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλn\\nk\\n.\\n(10)\\nFor n ≥ 2, these series converges absolutely and almost uniformly. Employing\\nthe series with n = 2, we obtain\\nR(x, t, λ) = K(x, t) + λK2(x, t) + λ2\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλ2\\nk (λk − λ),\\n(11)\\nwith the Fourier terms behaving asymptotically as ∼ 1/λ3\\nk. Next, employing\\nK3(x, t) we obtain\\nR(x, t, λ) = K(x, t) + λK2(x, t) + λ2K3(x, t) + λ3\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλ3\\nk (λk − λ).\\nGenerally, we have\\nR(x, t, λ) =\\nm\\n�\\nn=1\\nλn−1Kn(x, t) + λm\\n∞\\n�\\nk=1\\nψk(x) ψk(t)\\nλm\\nk (λk − λ).\\n(12)\\n2\\nThe obvious practical advantage is that convergence of the Fourier series im-\\nproves with increased m. The resolvent can be eﬀectively approximated by a\\nfew terms of the Neumann series plus several terms of Fourier series. Even if λ\\nis near some large eigenvalue λn, the terms with |λk| > |λn| diminish quickly.\\nFrom the probed literature it appears that the closest topic that may suggest\\nformulas (11), (12) is “acceleration of convergence of Fourier series” or “over-\\ncoming the Gibbs phenomenon” by means of polynomial subtraction [Adc11,\\n§5.1], [Ner19]. The Neumann series part in (12) is often polynomial, as the ex-\\nample of the following section illustrates. The term “Fourier-Neumann series”\\nhas been used to refer to Fourier series in terms of Bessel functions [CV09].\\n2\\nAn example\\nConsider the integral equation\\ny(x) = x + λ\\n� 1\\n0\\nmin(x, t)y(t)dt,\\n(13)\\nas in [Kle11, p. 124–127].\\nThe eigenvalues and eigenfunctions are found by\\nsolving the homogeneous equation\\nψ(x) = λ\\n� 1\\n0\\nmin(x, t)ψ(t)dt,\\n(14)\\nor the equivalent diﬀerential equation with boundary conditions:\\nψ′′(x) + λψ(x) = 0,\\nψ(0) = 0,\\nψ′(1) = 0.\\n(15)\\nThe eigenvalues and the corresponding normalized eigenfunctions are parametrized\\nby n = 1, 2, . . .:\\nλn =\\n�\\nn − 1\\n2\\n�2 π2,\\nψn =\\n√\\n2 sin\\n�\\nn − 1\\n2\\n�\\nπx.\\n(16)\\nBy (9), the resolvent is\\nR(x, t, λ) = 2\\n∞\\n�\\nn=1\\nsin\\n�\\nn − 1\\n2\\n�\\nπx sin\\n�\\nn − 1\\n2\\n�\\nπt\\n�\\nn − 1\\n2\\n�2 π2 − λ\\n.\\n(17)\\nBy (2), we have\\ny(x) = x + λ\\n∞\\n�\\nn=1\\n2(−1)n+1 sin\\n�\\nn − 1\\n2\\n�\\nπx\\n�\\nn − 1\\n2\\n�2π2\\n��\\nn − 1\\n2\\n�2π2 − λ\\n�.\\n(18)\\nTo compute the iterated kernels for (12), let �Kn(x, t) denote the restriction of\\nKn(x, t) to t ≤ x. The iterated kernels are symmetric just as K(x, t), and\\n�Kn+1(x, t) =\\n� t\\n0\\nξ �Kn(t, ξ)dξ +\\n� x\\nt\\nξ �Kn(ξ, t)dξ + x\\n� 1\\nx\\n�Kn(ξ, t)dξ\\n(19)\\n3\\nby (5). Starting from �K1(x, t) = t, we compute\\n�K2(x, t) = xt − 1\\n2 x2t − 1\\n6 t3,\\n(20)\\n�K3(x, t) = 1\\n3 xt − 1\\n6 x3t − 1\\n6 xt3 + 1\\n24 x4t + 1\\n12 x2t3 +\\n1\\n120 t5.\\n(21)\\nWe compute the initial integrals\\nJn(x) =\\n� 1\\n0\\nKn(x, t)f(t)dt =\\n� x\\n0\\n�Kn(x, t)f(t)dt +\\n� 1\\nx\\n�Kn(t, x)f(t)dt\\n(22)\\nwith f(x) = x:\\nJ1(x) = 1\\n2 x − 1\\n6 x3,\\n(23)\\nJ2(x) =\\n5\\n24 x − 1\\n12 x3 +\\n1\\n120 x5,\\n(24)\\nJ3(x) =\\n61\\n720 x −\\n5\\n144 x3 +\\n1\\n240 x5 −\\n1\\n5040 x7.\\n(25)\\nWe get the solution in subsequently quicker approximations:\\ny(x) = x + λJ1(x) + λ2\\n∞\\n�\\nn=1\\n2(−1)n+1 sin\\n�\\nn − 1\\n2\\n�\\nπx\\n�\\nn − 1\\n2\\n�4π4\\n��\\nn − 1\\n2\\n�2π2 − λ\\n�,\\n(26)\\ny(x) = x + λJ1(x) + λ2J2(x) + λ3\\n∞\\n�\\nn=1\\n2(−1)n+1 sin\\n�\\nn − 1\\n2\\n�\\nπx\\n�\\nn − 1\\n2\\n�6π6\\n��\\nn − 1\\n2\\n�2π2 − λ\\n�,\\n(27)\\ny(x) = x + λJ1(x) + λ2J2(x) + λ3J3(x)\\n+ λ4\\n∞\\n�\\nn=1\\n2(−1)n+1 sin\\n�\\nn − 1\\n2\\n�\\nπx\\n�\\nn − 1\\n2\\n�8π8\\n��\\nn − 1\\n2\\n�2π2 − λ\\n�.\\n(28)\\nThe solution exists only if and only if λ is not an eigenvalue. For λ > 0 or λ < 0,\\nan elementary expression for the solution is, respectively,\\ny(x) =\\nsin\\n√\\nλ x\\n√\\nλ cos\\n√\\nλ\\nor\\ny(x) =\\nsinh\\n√\\n−λ x\\n√\\n−λ cosh\\n√\\n−λ.\\n(29)\\nThe poles of these solutions at the eigenvalues λ are well captured by the re-\\nspective Fourier terms. With those Fourier terms, the approximation error can\\nbe seen as a continuous function of λ.\\nWith explicit solutions at hand, we may analyze approximation quality of\\ndiﬀerently truncated mixed Neumann-Fourier series (18), (26)–(28). Figure 1\\nand Table 1 represent a batch of relevant data. Figure 1 demonstrates how\\nadding just one Neumann series term diminishes the approximation error by a\\nfactor ≈ 100. The approximation error is largest at the endpoint x = 1, because\\nthe non-homogeneous term f(x) = x in (13) does not satisfy the boundary\\ncondition (15) at x = 1, resulting in a weak version of the Gibbs phenomenon\\n[Adc11, §4.2]. Table 1 suggests that adding a Neumann series term is more\\nworthwhile than adding a Fourier term, unless λ becomes large while the number\\nof Fourier terms lags the number of Neumann terms. The approximation errors\\nfor negative λ are of the same order as respectively those for |λ|.\\n4\\nFigure 1: Approximation error for the truncated series in (18) and (26) with 4\\n(dashed line), 5 (dotted line) or 6 (solid line) Fourier terms, for λ = 4, x ∈ [0, 1].\\nn\\nλ = 2\\nλ = 5\\nλ = 10\\nλ = 20\\nλ = 50\\nλ = 100\\n(18)\\n3\\n4.9 10−4\\n1.2 10−3\\n2.6 10−3\\n5.5 10−3\\n1.8 10−2\\n9.6 10−2\\n4\\n2.1 10−4\\n5.3 10−4\\n1.1 10−3\\n2.2 10−3\\n6.4 10−3\\n1.7 10−2\\n5\\n1.1 10−4\\n2.7 10−4\\n5.5 10−4\\n1.1 10−3\\n3.1 10−3\\n7.1 10−3\\n6\\n6.3 10−5\\n1.6 10−4\\n3.2 10−4\\n6.5 10−4\\n1.7 10−3\\n3.8 10−3\\n(26)\\n3\\n6.1 10−6\\n3.9 10−5\\n1.6 10−4\\n7.1 10−4\\n6.0 10−3\\n7.2 10−2\\n4\\n1.5 10−6\\n9.6 10−6\\n3.9 10−5\\n1.7 10−4\\n1.2 10−3\\n6.8 10−3\\n5\\n5.1 10−7\\n3.2 10−6\\n1.3 10−5\\n5.4 10−5\\n3.7 10−4\\n1.8 10−3\\n6\\n2.1 10−7\\n1.3 10−6\\n5.3 10−6\\n2.2 10−5\\n1.4 10−4\\n6.5 10−4\\n(27)\\n3\\n8.9 10−8\\n1.4 10−6\\n1.2 10−5\\n1.0 10−4\\n2.3 10−3\\n5.7 10−2\\n4\\n1.3 10−8\\n2.1 10−7\\n1.7 10−6\\n1.4 10−5\\n2.6 10−4\\n3.0 10−3\\n5\\n2.8 10−9\\n4.5 10−8\\n3.6 10−7\\n3.0 10−6\\n5.2 10−5\\n5.1 10−4\\n6\\n8.1 10−10\\n1.3 10−8\\n1.0 10−7\\n8.4 10−7\\n1.4 10−5\\n1.3 10−4\\n(28)\\n3\\n1.4 10−9\\n5.5 10−8\\n9.2 10−7\\n1.6 10−5\\n8.9 10−4\\n4.6 10−2\\n4\\n1.2 10−10\\n4.7 10−9\\n7.7 10−8\\n1.3 10−6\\n6.0 10−5\\n1.4 10−3\\n5\\n1.7 10−11\\n6.7 10−10\\n1.1 10−8\\n1.8 10−7\\n7.8 10−6\\n1.5 10−4\\n6\\n3.4 10−12\\n1.3 10−10\\n2.2 10−9\\n3.6 10−8\\n1.5 10−6\\n2.7 10−5\\nTable 1: Approximation errors at x = 1 for the truncated series (18), (26)–(28)\\nwith n ∈ {3, 4, 5, 6} Fourier terms.\\n5\\n3\\nMore formulas\\nLet us consider the interplay of Fourier series with Fredholm’s representation\\nR(x, t, λ) = D(x, t, λ)\\n∆(λ)\\n,\\n(30)\\nwhere\\n∆(λ) =\\n∞\\n�\\nk=1\\n�\\n1 − λ\\nλk\\n�\\n(31)\\nis an analytic function with the zeroes exactly at the eigenvalues [Kan97, Ch. 4].\\nThe analytic series in λ of ∆(x) and D(x, t, λ) converge on the whole C when the\\nkernel K(x, t) is L2-integrable. The results are unremarkable for approximation\\npurposes, but there is some representational appeal.\\nThe power series coeﬃcients of\\n∆(λ) =\\n∞\\n�\\nn=0\\ncnλn,\\nD(x, t, λ) =\\n∞\\n�\\nn=0\\nCn(x, t)λn,\\n(32)\\ncan be computed [Kan97, (4.2.9)–(4.2.13a)] from the recurrence relations\\ncn = − 1\\nn\\n� b\\na\\nCn−1(x, x)dx,\\n(33)\\nCn(x, t) = cnK(x, t) +\\n� b\\na\\nK(x, ξ)Cn−1(ξ, t)dξ,\\n(34)\\nand the initial conditions c0 = 1, C0(x, t) = K(x, t). Keeping in mind orthonor-\\nmality of the ψk(ξ)’s, manipulation of the Fourier series gives\\nc1 = −\\n∞\\n�\\nk=1\\n1\\nλk\\n,\\n(35)\\nC1(x, t) = −\\n∞\\n�\\nk=1\\n�\\nj̸=k\\nψj(x)ψj(t)\\nλkλj\\n(36)\\n= −\\n� �\\nj<k\\nψj(x)ψj(t) + ψk(t)ψk(t)\\nλjλk\\n,\\n(37)\\nc2 =\\n� �\\nj<k\\n1\\nλjλk\\n,\\n(38)\\nC2(x, t) =\\n� �\\nj<k\\n�\\ni̸∈{j,k}\\nψi(x)ψi(t)\\nλjλkλi\\n(39)\\n=\\n� � �\\ni<j<k\\nψi(x)ψi(t) + ψj(t)ψj(t) + ψk(t)ψk(t)\\nλiλjλk\\n,\\n(40)\\nc3 = −\\n� � �\\ni<j<k\\n1\\nλiλjλk\\n,\\n(41)\\n6\\netc. The pattern\\ncn = (−1)n\\n�\\nS⊂N & |S|=n\\n� �\\nj∈S\\nλj\\n�−1\\n,\\n(42)\\nCn(x, t) = (−1)n\\n�\\nS⊂N & |S|=n\\n� �\\nj∈S\\nλj\\n�−1 �\\nj∈S\\nψj(x)ψj(t).\\n(43)\\nis apparent from (31) and eventually from D(x, t, λ) = ∆(λ)R(x, t, λ) as well.\\nTowards an analogy to (12), we have\\nD(x, t, λ) =\\nm−1\\n�\\nn=0\\nλnCn(x, t) +\\n�\\nS⊂N & |S|≥m\\n(−λ)|S|\\n�\\nj∈S λj\\n�\\nj∈S\\nψj(x)ψj(t).\\n(44)\\nBut convergence is expected to be poor generally. If the series (35) for c1 does\\nnot converge, the series for the other cn should not converge (absolutely at least)\\neither. The series for Cn(x, t) should then converge only in the L2-norm.\\nAs an intermediate alternative to (12) and (44) one may consider\\n(1 − αλ) R(x, t, λ) = K(x, t) +\\nm−1\\n�\\nn=1\\nλn�\\nKn+1(x, t) − αKn(x, t)\\n�\\n+ λm\\n∞\\n�\\nk=1\\n(1 − αλk) ψk(x) ψk(t)\\nλm\\nk (λk − λ)\\n.\\n(45)\\nThis modiﬁcation could be useful for λ near some large eigenvalue λk, taking\\nα = 1/λk and annihilating the kth term in the Fourier series. Similar expressions\\ncan be obtained for R(x, t, λ) multiplied by several such factors (1−αjλ) annihi-\\nlating several peaking Fourier terms. But the Fourier terms in (45) decrease as\\nO(1/λm\\nj ) rather than O(1/λm+1\\nj\\n) in (12). Eliminating a peaking Fourier term\\ncomes at the cost of one added Neumann series term for the purpose of better\\nconvergence of the Fourier series.\\n',\n",
       " 'Path': './test_pdfs\\\\2101.00005.pdf',\n",
       " 'sha_256': 'ae4f346ffef31e8ed886b5b5bcbc9b6c6a809d71aceb890f2a614e29a5986855',\n",
       " 'language': 'en',\n",
       " 'language_probability': 0.9999976084597738,\n",
       " 'Authors': '',\n",
       " 'Title': '',\n",
       " 'url': '',\n",
       " 'date': ''}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_to_dict(directory=directory, filename='2101.00005.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cfcdabba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_pdfs\\2101.00005.pdf\n",
      "./test_pdfs\\2101.00031.pdf\n",
      "check point: df made\n",
      "processing text...\n",
      "removing math symbols and letters...\n",
      "math symbols and letters removed.\n",
      "\n",
      "removing stand allow digits (or figrue table numbers)...\n",
      "stand alone digits removed.\n",
      "\n",
      "keep only nouns...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf37d8c1b7bd400d8462ab52afcf0cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns kept; everything else removed\n",
      "\n",
      "removing meta data...(Names, Dates, Places...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd3880a5d58b43109b6c2da69abea734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed meta data.\n",
      "\n",
      "removing punctuation...\n",
      "punctuation removed.\n",
      "\n",
      "removing words with chars not a-zA-Z0-9...\n",
      "removed words with chars not a-zA-Z0-9.\n",
      "\n",
      "making lower-case...\n",
      "lower-cased.\n",
      "\n",
      "removing stop words...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6aadae011374066a9d0f3086427297e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words removed.\n",
      "\n",
      "lemmitizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3584d4363b433cb1e413abea3b1e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized.\n",
      "\n",
      "removing gibberish...\n",
      "gibberish removed.\n",
      "\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "removed single letters and super large words.\n",
      "\n",
      "done cleaning.\n",
      "\n",
      "remove idf terms less than -1.5\n",
      "removing terms...\n",
      "terms removed.\n",
      "\n",
      "ngramming words...\n",
      "bigram model started...\n",
      "bigram model finished.\n",
      "\n",
      "trigram model started...\n",
      "trigram model finished.\n",
      "\n",
      "processed text.\n",
      "\n",
      "processing abstract...\n",
      "removing math symbols and letters...\n",
      "math symbols and letters removed.\n",
      "\n",
      "removing stand allow digits (or figrue table numbers)...\n",
      "stand alone digits removed.\n",
      "\n",
      "keep only nouns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PA33566\\AppData\\Local\\Temp/ipykernel_54792/3421644238.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  list_text[i] = re.sub(r'\\s%s\\s'%term[1], ' ', list_text[i]).replace('  ', ' ')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44801211bdbf48709d1bddd4f8442b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nouns kept; everything else removed\n",
      "\n",
      "removing meta data...(Names, Dates, Places...)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da1f1ca4b5b4ee9b9e7acce6abc257b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed meta data.\n",
      "\n",
      "removing punctuation...\n",
      "punctuation removed.\n",
      "\n",
      "removing words with chars not a-zA-Z0-9...\n",
      "removed words with chars not a-zA-Z0-9.\n",
      "\n",
      "making lower-case...\n",
      "lower-cased.\n",
      "\n",
      "removing stop words...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6891ca76a074b328850ac882a386621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop words removed.\n",
      "\n",
      "lemmitizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e391ce71e62f4054bc3abe778d7c9afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized.\n",
      "\n",
      "removing gibberish...\n",
      "gibberish removed.\n",
      "\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "removed single letters and super large words.\n",
      "\n",
      "done cleaning.\n",
      "\n",
      "removing terms...\n",
      "terms removed.\n",
      "\n",
      "ngramming words...\n",
      "bigram model started...\n",
      "bigram model finished.\n",
      "\n",
      "trigram model started...\n",
      "trigram model finished.\n",
      "\n",
      "processed abstract.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PA33566\\AppData\\Local\\Temp/ipykernel_54792/3421644238.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  list_text[i] = re.sub(r'\\s%s\\s'%term[1], ' ', list_text[i]).replace('  ', ' ')\n"
     ]
    }
   ],
   "source": [
    "directory = './test_pdfs'\n",
    "storage_dir = './parsed_cleaned_pdfs'\n",
    "\n",
    "parsed_pdf_to_json(directory, storage_dir, for_training=True, n_most_freq_words_remove=0, stem=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432b705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
