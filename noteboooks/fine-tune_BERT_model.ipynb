{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20180d59-bcc7-40a1-930b-f232caee62af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utils import pdfs_to_df, tokenize_df_of_texts\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ec854f-c7e9-42df-bbd8-971976111eae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bd7f22d-683a-4ff6-b560-e282ba601151",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_pdfs/2101.00031.pdf\n",
      "./test_pdfs/2101.01089.pdf\n",
      "./test_pdfs/2101.00182.pdf\n",
      "./test_pdfs/2101.00525.pdf\n",
      "./test_pdfs/2101.01017.pdf\n",
      "./test_pdfs/2101.00005.pdf\n",
      "./test_pdfs/2101.00763.pdf\n",
      "./test_pdfs/2101.01291.pdf\n",
      "./test_pdfs/2101.00831.pdf\n",
      "./test_pdfs/2101.01094.pdf\n",
      "./test_pdfs/2101.00572.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "['Abstract', 'Abstract_Original', 'sha_256', 'language', 'language_probability', 'Authors', 'Title', 'url', 'date', 'token_embeddings']\n"
     ]
    }
   ],
   "source": [
    "directory = './test_pdfs'\n",
    "df = pdfs_to_df(directory)\n",
    "df = tokenize_df_of_texts(df)\n",
    "\n",
    "drop_cols = [col for col in df.columns if col not in ['Document', 'Text', 'Original_Text', 'Path', 'tokens']]\n",
    "print(drop_cols)\n",
    "\n",
    "df = df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f382f90c-6e02-4e38-a8ff-1eae94c1c4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example data\n",
    "train_texts = df['tokens'].to_list()[0:8]  # List of your tokenized texts\n",
    "train_labels = df['tokens'].to_list()[8:]  # List of corresponding labels (e.g., relevance to query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e0d5d4b-9cd9-4cc5-a691-794d5671feb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize text data and create input features\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for text in train_texts:\n",
    "    encoded_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    input_ids.append(encoded_text['input_ids'])\n",
    "    attention_masks.append(encoded_text['attention_mask'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa7eb8a-4989-4ba4-82f9-db1d378d23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f9583-ca50-4cc2-ba3f-e59279d1722a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(train_labels)\n",
    "\n",
    "# Create a dataset and dataloader\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
