{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2596aca2-ff60-4e23-b444-e27cbbd68662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "from utils import pdfs_to_df, tokenize_df_of_texts\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e480f6b-4dc3-48f8-8365-3134c336af15",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf7e76b-7f4a-496b-90e1-8eeb8fb421b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TOKENIZER\": \"roberta\",\n",
      "    \"input_folder\": \"space_based_pdfs\",\n",
      "    \"embedding_model_type\": \"Word2Vec\",\n",
      "    \"embedding_model_fname\": \"roberta_space_based_pdfs_Word2Vec_model.bin\",\n",
      "    \"vector_size\": 50,\n",
      "    \"window\": 3,\n",
      "    \"min_count\": 3,\n",
      "    \"sg\": 0,\n",
      "    \"TOKENS_TPYE\": \"tokens_less_sw\",\n",
      "    \"chunk_size\": 350,\n",
      "    \"chunk_overlap\": 0,\n",
      "    \"max_query_length\": 20,\n",
      "    \"top_N\": 20,\n",
      "    \"TOKENS_EMBEDDINGS\": \"query_search_less_sw\",\n",
      "    \"DOCUMENT_EMBEDDING\": \"token_embeddings_less_sw\",\n",
      "    \"DOCUMENT_TOKENS\": \"tokens_less_sw\",\n",
      "    \"METHOD\": \"MEAN_MAX\",\n",
      "    \"transformer_model_name\": \"deepset/roberta-base-squad2\",\n",
      "    \"context_size\": 350\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")) as json_file:\n",
    "    hyperparams = json.load(json_file)\n",
    "    print(json.dumps(hyperparams, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf0e55-acc2-4e5b-a775-92f02f003171",
   "metadata": {},
   "source": [
    "#### Get text from test corpus\n",
    "- Specify tokenizer, keep consistent with downstream Q&A model (TOKENIZER)\n",
    "- Secify the data filepath (directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d4590f6-6818-43f2-b182-e45c2dc4a1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer\n",
    "TOKENIZER = hyperparams['TOKENIZER']\n",
    "\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "# Set the directory \n",
    "input_folder = hyperparams['input_folder']\n",
    "directory = os.path.join(\"..\", \"data\", input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0680a587-3fb8-40fa-bbcd-b8ac8efb3012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '://', 'https', '\"', '\"...', '/)', 'www', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '.[', ',[', '-,', '][', 'com', '),', ',\"', 'Ġ!', 'Ġ\"', 'Ġ#', 'Ġ$', 'Ġ%', 'Ġ&', \"Ġ'\", 'Ġ(', 'Ġ)', 'Ġ*', 'Ġ+', 'Ġ,', 'Ġ-', 'Ġ.', 'Ġ/', 'Ġ://', 'Ġhttps', 'Ġ\"', 'Ġ\"...', 'Ġ/)', 'Ġwww', 'Ġ:', 'Ġ;', 'Ġ<', 'Ġ=', 'Ġ>', 'Ġ?', 'Ġ@', 'Ġ[', 'Ġ\\\\', 'Ġ]', 'Ġ^', 'Ġ_', 'Ġ`', 'Ġ{', 'Ġ|', 'Ġ}', 'Ġ~', 'Ġ.[', 'Ġ,[', 'Ġ-,', 'Ġ][', 'Ġcom', 'Ġ),', 'Ġ,\"']\n"
     ]
    }
   ],
   "source": [
    "# Specify additional stopwords to remove from the chunk cleaned for the candidate document search\n",
    "special_characters = [\n",
    "    \"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \"://\", \"https\",'\"', '\"...', \"/)\",\"www\",\n",
    "    \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\", \".[\", \",[\", \"-,\", \"][\", \"com\",\n",
    "    \"),\", ',\"'\n",
    "]\n",
    "\n",
    "special_characters += list(map(lambda x: \"Ġ\" + x, special_characters))\n",
    "print(special_characters)\n",
    "\n",
    "# Add numbers to remove\n",
    "special_characters += list(map(lambda x: str(x), range(100000)))\n",
    "special_characters += list(map(lambda x: \"Ġ\" + str(x), range(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb659758-9325-4bdf-8a73-be97bad99559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/space_based_pdfs/Galaxy 15 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Swarm Technologies - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Fengyun - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Falcon 9 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Cygnus NG-19 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Atlas V - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Inmarsat - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Kepler-11 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/James Webb Space Telescope - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Space-Based Infrared System - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Yaogan - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Starlink - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Atlas (rocket family) - Wikipedia.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "['sha_256', 'language', 'language_probability', 'chunk_text', 'chunk_text_less_sw', 'token_embeddings', 'token_embeddings_less_sw']\n"
     ]
    }
   ],
   "source": [
    "# From the test pdf dir, extract the text and tokenize it. Store in pandas dataframe\n",
    "df = pdfs_to_df(directory)\n",
    "df = tokenize_df_of_texts(df, tokenizers[TOKENIZER], REMOVE_SW_COL=True, additional_stopwords=special_characters)\n",
    "\n",
    "drop_cols = [col for col in df.columns if col not in ['Document', 'Text', 'Original_Text', 'Path', 'tokens', 'tokens_less_sw']]\n",
    "print(drop_cols)\n",
    "\n",
    "df = df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af54031c-07ba-456d-983a-e4e1a996174a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Path</th>\n",
       "      <th>Text</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_less_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Galaxy 15 - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Galaxy 15 - Wikipedia...</td>\n",
       "      <td>8/27/23, 9:28 galaxy 15 wikipedia 1/8 galaxy 1...</td>\n",
       "      <td>8/27/23, 9:28 PM\\nGalaxy 15 - Wikipedia\\nhttps...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 28, Ġgalaxy, Ġ15,...</td>\n",
       "      <td>[Ġgalaxy, Ġwik, ipedia, Ġgalaxy, Ġanimation, Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swarm Technologies - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Swarm Technologies - ...</td>\n",
       "      <td>8/27/23, 9:31 swarm technologies wikipedia 1/5...</td>\n",
       "      <td>8/27/23, 9:31 PM\\nSwarm Technologies - Wikiped...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 31, Ġswarm, Ġtech...</td>\n",
       "      <td>[Ġswarm, Ġtechnologies, Ġwik, ipedia, Ġswarm, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Document  \\\n",
       "0           Galaxy 15 - Wikipedia.pdf   \n",
       "1  Swarm Technologies - Wikipedia.pdf   \n",
       "\n",
       "                                                Path  \\\n",
       "0  ../data/space_based_pdfs/Galaxy 15 - Wikipedia...   \n",
       "1  ../data/space_based_pdfs/Swarm Technologies - ...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  8/27/23, 9:28 galaxy 15 wikipedia 1/8 galaxy 1...   \n",
       "1  8/27/23, 9:31 swarm technologies wikipedia 1/5...   \n",
       "\n",
       "                                       Original_Text  \\\n",
       "0  8/27/23, 9:28 PM\\nGalaxy 15 - Wikipedia\\nhttps...   \n",
       "1  8/27/23, 9:31 PM\\nSwarm Technologies - Wikiped...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Ġ8, /, 27, /, 23, ,, Ġ9, :, 28, Ġgalaxy, Ġ15,...   \n",
       "1  [Ġ8, /, 27, /, 23, ,, Ġ9, :, 31, Ġswarm, Ġtech...   \n",
       "\n",
       "                                      tokens_less_sw  \n",
       "0  [Ġgalaxy, Ġwik, ipedia, Ġgalaxy, Ġanimation, Ġ...  \n",
       "1  [Ġswarm, Ġtechnologies, Ġwik, ipedia, Ġswarm, ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65c0f4-f289-42c6-a0e1-dcfb76cff4ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train model on tokenized text\n",
    "- Set:\n",
    "    - Input data: Either \"tokens\" or \"tokens_less_sw\" (TOKENS_TYPE)\n",
    "    - Vector Size: length of word embeddings\n",
    "    - Window Size: span of sorrounding words to train model\n",
    "    - Min Count: minimum number of occurances of word to be be viable\n",
    "    - Ouput model file name: (model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdd18fea-f5f9-462f-bb46-cba3d6955806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta_space_based_pdfs_Word2Vec_model.bin\n"
     ]
    }
   ],
   "source": [
    "# TOKENS_TPYE = \"tokens\"\n",
    "TOKENS_TPYE = hyperparams[\"TOKENS_TPYE\"]\n",
    "\n",
    "kwargs = {\n",
    "     'sentences':df[TOKENS_TPYE].to_list(),\n",
    "     'vector_size':hyperparams[\"vector_size\"],\n",
    "     'window':hyperparams[\"window\"],\n",
    "     'min_count':hyperparams[\"min_count\"],\n",
    "     'sg':hyperparams[\"sg\"]\n",
    "    }\n",
    "\n",
    "# Train Word2Vec model\n",
    "embedding_model_type = hyperparams['embedding_model_type']\n",
    "if embedding_model_type == 'Word2Vec':\n",
    "    model = Word2Vec(**kwargs)\n",
    "\n",
    "embedding_model_fname = hyperparams[\"embedding_model_fname\"]\n",
    "print(embedding_model_fname)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eff82a-9997-49e8-ada3-e99515c4e964",
   "metadata": {},
   "source": [
    "#### Examine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7a83fa-eff0-4458-8903-ff0a3ff5cc6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count token frequencies\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m token_frequencies \u001b[38;5;241m=\u001b[39m \u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print the frequency of \"number\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_frequencies[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/collections/__init__.py:552\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/collections/__init__.py:637\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(iterable) \u001b[38;5;66;03m# fast path when counter is empty\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count token frequencies\n",
    "token_frequencies = Counter(df['tokens'].to_list())\n",
    "\n",
    "# Print the frequency of \"number\"\n",
    "print(\"Frequency of 'number':\", token_frequencies[\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c141a3dd-47ed-41c5-a6b2-543083323093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Ġrevenue' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĠ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TOKENIZER \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Access the embedding of a word\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Find similar words based on embedding similarity\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Ġrevenue' not present\""
     ]
    }
   ],
   "source": [
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname))\n",
    "\n",
    "word = \"revenue\"\n",
    "\n",
    "# Add the special preface character if the tokenizer for roberta was used\n",
    "word = f\"Ġ{word}\" if TOKENIZER == 'roberta' else word\n",
    "\n",
    "# Access the embedding of a word\n",
    "embedding = model.wv[word]\n",
    "print(embedding)\n",
    "# Find similar words based on embedding similarity\n",
    "similar_words = model.wv.most_similar(word)\n",
    "print(similar_words)\n",
    "\n",
    "# You can also perform vector arithmetic operations\n",
    "# result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c82ce892-41cf-41e2-9ab8-b84043f24fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 3410\n",
      "Is 'number' in vocabulary? True\n"
     ]
    }
   ],
   "source": [
    "vocabulary = model.wv.index_to_key\n",
    "print(\"Number of words in vocabulary:\", len(vocabulary))\n",
    "print(\"Is 'number' in vocabulary?\", 'number' in vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441fb34d-7f82-42bc-bf05-750c8996dcf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '[PAD]' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[PAD]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '[PAD]' not present\""
     ]
    }
   ],
   "source": [
    "token = '[PAD]'\n",
    "\n",
    "print(model.wv['[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291b722-ea15-4915-a40b-6eb6b1c3c1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
