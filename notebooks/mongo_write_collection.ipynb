{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26705118-b459-4fb3-a59d-cf16ebda2cba",
   "metadata": {},
   "source": [
    "#### Notebook: Write data to the Mongodb collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa98beec-b811-42f6-964b-e91a825c8137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Set proxy information if windows\n",
    "if platform.system() == \"Windows\":\n",
    "    print(\"Platform is Windows, setting proxy\")\n",
    "    # Get the current date and time\n",
    "    now = datetime.now()\n",
    "    day = now.strftime(\"%A\")\n",
    "    proxy_url = f\"http://33566:{day[0:3]}@proxy-west.aero.org:8080\"\n",
    "\n",
    "    # Set proxy environment variables\n",
    "    os.environ['HTTP_PROXY'] = proxy_url\n",
    "    os.environ['HTTPS_PROXY'] = proxy_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb0ce09b-04e8-4cf6-943f-a30eb8868fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "import json\n",
    "import sys\n",
    "# Get the current working directory (notebooks directory)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project directory\n",
    "project_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Assuming your project structure is as described before\n",
    "src_path = os.path.abspath(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from question_answer_site.question_answer.parse_document import parse_document, update_collection\n",
    "from question_answer_site.question_answer.mongodb import MongoDb\n",
    "from question_answer_site.question_answer.config import TOKENIZER, EMBEDDING_MODEL_FNAME, EMBEDDING_MODEL_TYPE, TOKENS_EMBEDDINGS, DOCUMENT_EMBEDDING, \\\n",
    "    DOCUMENT_TOKENS, TOP_N, TRANSFORMER_MODEL_NAME, METHOD, MAX_QUERY_LENGTH, username, password, cluster_url, INPUT_FOLDER, \\\n",
    "    database_name, special_characters, CHUNK_SIZE, CHUNK_OVERLAP\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "400d6994-e275-4f10-a21b-7e26315a6369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d102555-fb50-4c80-8607-57e02e93147b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "if EMBEDDING_MODEL_TYPE == 'Word2Vec':\n",
    "    embedding_model = Word2Vec.load(\n",
    "        os.path.join(os.getcwd(), \"question_answer\", \"embedding_models\", EMBEDDING_MODEL_FNAME))\n",
    "elif EMBEDDING_MODEL_TYPE.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    embedding_model = spacy.load(os.path.join(\"..\",\"src\",\"question_answer_site\", \"question_answer\", \"embedding_models\",\n",
    "                                         EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36ee2c39-73dd-4841-a370-e41d575d0ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_path = os.path.join(\"..\", \"data\", \"space_based_pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d86ae211-ba46-466d-b720-fea5599646a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path: ../data/space_based_pdfs/Galaxy 15 - Wikipedia.pdf,\n",
      "file name: Galaxy 15 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Reconnaissance satellite - Wikipedia.pdf,\n",
      "file name: Reconnaissance satellite - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Wideband Global SATCOM - Wikipedia.pdf,\n",
      "file name: Wideband Global SATCOM - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/.DS_Store,\n",
      "file name: .DS_Store\n",
      "File not a recognized format\n",
      "\tlanguage detection error!\n",
      "file path: ../data/space_based_pdfs/Swarm Technologies - Wikipedia.pdf,\n",
      "file name: Swarm Technologies - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Fengyun - Wikipedia.pdf,\n",
      "file name: Fengyun - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Advanced Extremely High Frequency - Wikipedia.pdf,\n",
      "file name: Advanced Extremely High Frequency - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Falcon 9 - Wikipedia.pdf,\n",
      "file name: Falcon 9 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Rocket Lab Electron - Wikipedia.pdf,\n",
      "file name: Rocket Lab Electron - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Cygnus NG-19 - Wikipedia.pdf,\n",
      "file name: Cygnus NG-19 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Falcon 9 Full Thrust - Wikipedia.pdf,\n",
      "file name: Falcon 9 Full Thrust - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Atlas V - Wikipedia.pdf,\n",
      "file name: Atlas V - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Boeing X-37 - Wikipedia.pdf,\n",
      "file name: Boeing X-37 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Inmarsat - Wikipedia.pdf,\n",
      "file name: Inmarsat - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Kepler-11 - Wikipedia.pdf,\n",
      "file name: Kepler-11 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Technology demonstration - Wikipedia.pdf,\n",
      "file name: Technology demonstration - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Autonomous Nanosatellite Guardian for Evaluating Local Space - Wikipedia.pdf,\n",
      "file name: Autonomous Nanosatellite Guardian for Evaluating Local Space - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Communications satellite - Wikipedia.pdf,\n",
      "file name: Communications satellite - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Falcon Heavy - Wikipedia.pdf,\n",
      "file name: Falcon Heavy - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Delta IV Heavy - Wikipedia.pdf,\n",
      "file name: Delta IV Heavy - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Space Based Space Surveillance - Wikipedia.pdf,\n",
      "file name: Space Based Space Surveillance - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/James Webb Space Telescope - Wikipedia.pdf,\n",
      "file name: James Webb Space Telescope - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Space-Based Infrared System - Wikipedia.pdf,\n",
      "file name: Space-Based Infrared System - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Yaogan - Wikipedia.pdf,\n",
      "file name: Yaogan - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Starlink - Wikipedia.pdf,\n",
      "file name: Starlink - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Atlas (rocket family) - Wikipedia.pdf,\n",
      "file name: Atlas (rocket family) - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Signals intelligence - Wikipedia.pdf,\n",
      "file name: Signals intelligence - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/GPS Block IIF - Wikipedia.pdf,\n",
      "file name: GPS Block IIF - Wikipedia.pdf\n",
      "processing text...\n",
      "(27, 13)\n",
      "Index(['Document', 'Path', 'Text', 'Original_Text', 'sha_256', 'language',\n",
      "       'language_probability', 'chunk_text', 'chunk_text_less_sw', 'tokens',\n",
      "       'tokens_less_sw', 'token_embeddings', 'token_embeddings_less_sw'],\n",
      "      dtype='object')\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "Chunking the tokenized text...\n",
      "\n",
      "printing the shape of chunked dataframe\n",
      "(502, 13)\n",
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "data = parse_document(document_path, embedding_model, tokenizer, CHUNK_SIZE, CHUNK_OVERLAP, special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62407af-4724-48c2-9aa0-d56a853e3226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a98f2bc-0a27-4d3c-9962-181248bc9514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Updating the 'parsed_documents' collection\n",
      "0 documents in 'parsed_documents' before adding\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "update_collection(\"parsed_documents\", copy.deepcopy(data))\n",
    "\n",
    "start_time = time.time()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Total execution time: {execution_time} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e894cd5-76fc-45fa-a06f-c282e34e5fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n",
      "Updating the 'extracted_text' collection\n",
      "0 documents in 'extracted_text' before adding\n",
      "27 documents in 'extracted_text' after adding\n"
     ]
    }
   ],
   "source": [
    "# Should be +1 for adding one document\n",
    "update_collection(\"extracted_text\", copy.deepcopy(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26f94531-99fc-4ff0-a19f-48fdb0f8480f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list\n"
     ]
    }
   ],
   "source": [
    "if type(list()) == list:\n",
    "    print('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbefcc-b4aa-44fd-b16e-d52dcfdd3352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
