{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2596aca2-ff60-4e23-b444-e27cbbd68662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "\n",
    "from utils import pdfs_to_df, tokenize_df_of_texts\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e11d4516-6ada-4679-baab-07ca8464c0db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your GloVe vectors into a custom spaCy model\n",
    "def load_custom_vectors(vectors_path):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    with open(vectors_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\" \")\n",
    "            word = parts[0]\n",
    "            vector = [float(val) for val in parts[1:]]\n",
    "            vector = np.array(vector)\n",
    "            nlp.vocab.set_vector(word, vector)\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e480f6b-4dc3-48f8-8365-3134c336af15",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf7e76b-7f4a-496b-90e1-8eeb8fb421b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TOKENIZER\": \"roberta\",\n",
      "    \"input_folder\": \"space_based_pdfs\",\n",
      "    \"embedding_model_type\": \"glove\",\n",
      "    \"embedding_model_fname\": \"roberta_space_based_pdfs_glove_model.bin\",\n",
      "    \"vector_size\": 50,\n",
      "    \"window\": 3,\n",
      "    \"min_count\": 3,\n",
      "    \"sg\": 0,\n",
      "    \"TOKENS_TPYE\": \"tokens_less_sw\",\n",
      "    \"chunk_size\": 350,\n",
      "    \"chunk_overlap\": 0,\n",
      "    \"max_query_length\": 20,\n",
      "    \"top_N\": 20,\n",
      "    \"TOKENS_EMBEDDINGS\": \"query_search_less_sw\",\n",
      "    \"DOCUMENT_EMBEDDING\": \"token_embeddings_less_sw\",\n",
      "    \"DOCUMENT_TOKENS\": \"tokens_less_sw\",\n",
      "    \"METHOD\": \"MEAN_MAX\",\n",
      "    \"transformer_model_name\": \"deepset/roberta-base-squad2\",\n",
      "    \"context_size\": 350\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")) as json_file:\n",
    "    hyperparams = json.load(json_file)\n",
    "    print(json.dumps(hyperparams, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf0e55-acc2-4e5b-a775-92f02f003171",
   "metadata": {},
   "source": [
    "#### Get text from test corpus\n",
    "- Specify tokenizer, keep consistent with downstream Q&A model (TOKENIZER)\n",
    "- Secify the data filepath (directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d4590f6-6818-43f2-b182-e45c2dc4a1a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer\n",
    "TOKENIZER = hyperparams['TOKENIZER']\n",
    "\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "# Set the directory \n",
    "input_folder = hyperparams['input_folder']\n",
    "directory = os.path.join(\"..\", \"data\", input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0680a587-3fb8-40fa-bbcd-b8ac8efb3012",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '://', 'https', '\"', '\"...', '/)', 'www', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '.[', ',[', '-,', '][', 'com', '),', ',\"', 'Ġ!', 'Ġ\"', 'Ġ#', 'Ġ$', 'Ġ%', 'Ġ&', \"Ġ'\", 'Ġ(', 'Ġ)', 'Ġ*', 'Ġ+', 'Ġ,', 'Ġ-', 'Ġ.', 'Ġ/', 'Ġ://', 'Ġhttps', 'Ġ\"', 'Ġ\"...', 'Ġ/)', 'Ġwww', 'Ġ:', 'Ġ;', 'Ġ<', 'Ġ=', 'Ġ>', 'Ġ?', 'Ġ@', 'Ġ[', 'Ġ\\\\', 'Ġ]', 'Ġ^', 'Ġ_', 'Ġ`', 'Ġ{', 'Ġ|', 'Ġ}', 'Ġ~', 'Ġ.[', 'Ġ,[', 'Ġ-,', 'Ġ][', 'Ġcom', 'Ġ),', 'Ġ,\"']\n"
     ]
    }
   ],
   "source": [
    "# Specify additional stopwords to remove from the chunk cleaned for the candidate document search\n",
    "special_characters = [\n",
    "    \"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \"://\", \"https\",'\"', '\"...', \"/)\",\"www\",\n",
    "    \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\", \".[\", \",[\", \"-,\", \"][\", \"com\",\n",
    "    \"),\", ',\"'\n",
    "]\n",
    "\n",
    "special_characters += list(map(lambda x: \"Ġ\" + x, special_characters))\n",
    "print(special_characters)\n",
    "\n",
    "# Add numbers to remove\n",
    "special_characters += list(map(lambda x: str(x), range(100000)))\n",
    "special_characters += list(map(lambda x: \"Ġ\" + str(x), range(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb659758-9325-4bdf-8a73-be97bad99559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/space_based_pdfs/Galaxy 15 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Swarm Technologies - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Fengyun - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Falcon 9 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Cygnus NG-19 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Atlas V - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Inmarsat - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Kepler-11 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/James Webb Space Telescope - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Space-Based Infrared System - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Yaogan - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Starlink - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Atlas (rocket family) - Wikipedia.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "['sha_256', 'language', 'language_probability', 'chunk_text', 'chunk_text_less_sw', 'token_embeddings', 'token_embeddings_less_sw']\n"
     ]
    }
   ],
   "source": [
    "# From the test pdf dir, extract the text and tokenize it. Store in pandas dataframe\n",
    "df = pdfs_to_df(directory)\n",
    "df = tokenize_df_of_texts(df, tokenizers[TOKENIZER], REMOVE_SW_COL=True, additional_stopwords=special_characters)\n",
    "\n",
    "drop_cols = [col for col in df.columns if col not in ['Document', 'Text', 'Original_Text', 'Path', 'tokens', 'tokens_less_sw']]\n",
    "print(drop_cols)\n",
    "\n",
    "df = df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65c0f4-f289-42c6-a0e1-dcfb76cff4ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train model on tokenized text\n",
    "- Set:\n",
    "    - Input data: Either \"tokens\" or \"tokens_less_sw\" (TOKENS_TYPE)\n",
    "    - Vector Size: length of word embeddings\n",
    "    - Window Size: span of sorrounding words to train model\n",
    "    - Min Count: minimum number of occurances of word to be be viable\n",
    "    - Ouput model file name: (model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdd18fea-f5f9-462f-bb46-cba3d6955806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "BUILDDIR path: /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/vocab_count -min-count 3 -verbose 2 < training_data.txt > vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[0GProcessed 72779 tokens.\n",
      "Counted 7751 unique words.\n",
      "Truncating vocabulary at min count 3.\n",
      "Using vocabulary of size 3410.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 3 < training_data.txt > cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 3\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"vocab.txt\"...loaded 3410 words.\n",
      "Building lookup table...table contains 11628101 elements.\n",
      "Processing token: 0\u001b[0GProcessed 72779 tokens.\n",
      "Writing cooccurrences to disk.......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G0 lines.\u001b[39G100000 lines.\u001b[0GMerging cooccurrence files: processed 199198 lines.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\n",
      "Using random seed 1693813655\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 199198 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G199198 lines.\u001b[0GMerging temp files: processed 199198 lines.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 30 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 199198 lines.\n",
      "Initializing parameters...Using random seed 1693813655\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 3410\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "09/04/23 - 12:47.36AM, iter: 001, cost: 0.106122\n",
      "09/04/23 - 12:47.36AM, iter: 002, cost: 0.085468\n",
      "09/04/23 - 12:47.36AM, iter: 003, cost: 0.079532\n",
      "09/04/23 - 12:47.36AM, iter: 004, cost: 0.071120\n",
      "09/04/23 - 12:47.36AM, iter: 005, cost: 0.066502\n",
      "09/04/23 - 12:47.36AM, iter: 006, cost: 0.062162\n",
      "09/04/23 - 12:47.36AM, iter: 007, cost: 0.058245\n",
      "09/04/23 - 12:47.36AM, iter: 008, cost: 0.054548\n",
      "09/04/23 - 12:47.36AM, iter: 009, cost: 0.050748\n",
      "09/04/23 - 12:47.36AM, iter: 010, cost: 0.046913\n",
      "09/04/23 - 12:47.36AM, iter: 011, cost: 0.043250\n",
      "09/04/23 - 12:47.36AM, iter: 012, cost: 0.039924\n",
      "09/04/23 - 12:47.36AM, iter: 013, cost: 0.036945\n",
      "09/04/23 - 12:47.36AM, iter: 014, cost: 0.034343\n",
      "09/04/23 - 12:47.36AM, iter: 015, cost: 0.032013\n",
      "09/04/23 - 12:47.36AM, iter: 016, cost: 0.029932\n",
      "09/04/23 - 12:47.36AM, iter: 017, cost: 0.028049\n",
      "09/04/23 - 12:47.36AM, iter: 018, cost: 0.026357\n",
      "09/04/23 - 12:47.36AM, iter: 019, cost: 0.024820\n",
      "09/04/23 - 12:47.36AM, iter: 020, cost: 0.023434\n",
      "09/04/23 - 12:47.36AM, iter: 021, cost: 0.022168\n",
      "09/04/23 - 12:47.36AM, iter: 022, cost: 0.021012\n",
      "09/04/23 - 12:47.36AM, iter: 023, cost: 0.019962\n",
      "09/04/23 - 12:47.36AM, iter: 024, cost: 0.019006\n",
      "09/04/23 - 12:47.36AM, iter: 025, cost: 0.018117\n",
      "09/04/23 - 12:47.36AM, iter: 026, cost: 0.017310\n",
      "09/04/23 - 12:47.36AM, iter: 027, cost: 0.016570\n",
      "09/04/23 - 12:47.36AM, iter: 028, cost: 0.015883\n",
      "09/04/23 - 12:47.36AM, iter: 029, cost: 0.015246\n",
      "09/04/23 - 12:47.36AM, iter: 030, cost: 0.014648\n"
     ]
    }
   ],
   "source": [
    "TOKENS_TPYE = hyperparams[\"TOKENS_TPYE\"]    # \"tokens\" or \"tokens_less_sw\"\n",
    "embedding_model_fname = hyperparams['embedding_model_fname']\n",
    "\n",
    "# Train Word2Vec model\n",
    "embedding_model_type = hyperparams['embedding_model_type']\n",
    "if embedding_model_type == 'Word2Vec':\n",
    "    kwargs = {\n",
    "     'sentences':df[TOKENS_TPYE].to_list(),\n",
    "     'vector_size':hyperparams[\"vector_size\"],\n",
    "     'window':hyperparams[\"window\"],\n",
    "     'min_count':hyperparams[\"min_count\"],\n",
    "     'sg':hyperparams[\"sg\"]\n",
    "    }\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    model = Word2Vec(**kwargs)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname))\n",
    "    \n",
    "elif embedding_model_type == 'glove':\n",
    "    # Specify the file path for the output text file\n",
    "    output_file = os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\", 'training_data.txt')\n",
    "\n",
    "    # Write the \"tokens\" column to a text file with each row on a separate line\n",
    "    if os.getcwd().endswith('glove'):\n",
    "        os.chdir(os.path.join(\"..\", \"..\", \"..\", \"notebooks\"))\n",
    "    df[TOKENS_TPYE].apply(lambda x: ' '.join(x)).to_csv(output_file, header=False, index=False, sep='\\n', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    os.environ[\"VECTOR_SIZE\"] = str(hyperparams[\"vector_size\"])\n",
    "    os.environ[\"WINDOW_SIZE\"] = str(hyperparams[\"window\"])\n",
    "    os.environ[\"VOCAB_MIN_COUNT\"] = str(hyperparams[\"min_count\"])\n",
    "    sys.path.append(os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\"))\n",
    "    \n",
    "    # Train the model\n",
    "    os.chdir(os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\"))\n",
    "    !./demo.sh\n",
    "    if os.getcwd().endswith('glove'):\n",
    "        os.chdir(os.path.join(\"..\", \"..\", \"..\", \"notebooks\"))\n",
    "    \n",
    "    # Path to your GloVe vectors file\n",
    "    vectors_file = os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\", \"vectors.txt\")\n",
    "\n",
    "    # Load the custom spaCy model with GloVe vectors\n",
    "    custom_nlp = load_custom_vectors(vectors_file)\n",
    "\n",
    "    # Save the custom spaCy model to a directory\n",
    "    custom_nlp.to_disk(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname.split(\".bin\")[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325ca7b9-10a1-4759-8c58-b5edd2019c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta_space_based_pdfs_glove_model.bin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eff82a-9997-49e8-ada3-e99515c4e964",
   "metadata": {},
   "source": [
    "#### Examine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7a83fa-eff0-4458-8903-ff0a3ff5cc6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count token frequencies\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m token_frequencies \u001b[38;5;241m=\u001b[39m \u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print the frequency of \"number\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_frequencies[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/collections/__init__.py:552\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/collections/__init__.py:637\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(iterable) \u001b[38;5;66;03m# fast path when counter is empty\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count token frequencies\n",
    "token_frequencies = Counter(df['tokens'].to_list())\n",
    "\n",
    "# Print the frequency of \"number\"\n",
    "print(\"Frequency of 'number':\", token_frequencies[\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c141a3dd-47ed-41c5-a6b2-543083323093",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Ġrevenue' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĠ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TOKENIZER \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Access the embedding of a word\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Find similar words based on embedding similarity\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Ġrevenue' not present\""
     ]
    }
   ],
   "source": [
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname))\n",
    "\n",
    "word = \"revenue\"\n",
    "\n",
    "# Add the special preface character if the tokenizer for roberta was used\n",
    "word = f\"Ġ{word}\" if TOKENIZER == 'roberta' else word\n",
    "\n",
    "# Access the embedding of a word\n",
    "embedding = model.wv[word]\n",
    "print(embedding)\n",
    "# Find similar words based on embedding similarity\n",
    "similar_words = model.wv.most_similar(word)\n",
    "print(similar_words)\n",
    "\n",
    "# You can also perform vector arithmetic operations\n",
    "# result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c82ce892-41cf-41e2-9ab8-b84043f24fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 3410\n",
      "Is 'number' in vocabulary? True\n"
     ]
    }
   ],
   "source": [
    "vocabulary = model.wv.index_to_key\n",
    "print(\"Number of words in vocabulary:\", len(vocabulary))\n",
    "print(\"Is 'number' in vocabulary?\", 'number' in vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441fb34d-7f82-42bc-bf05-750c8996dcf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '[PAD]' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[PAD]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '[PAD]' not present\""
     ]
    }
   ],
   "source": [
    "token = '[PAD]'\n",
    "\n",
    "print(model.wv['[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291b722-ea15-4915-a40b-6eb6b1c3c1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
