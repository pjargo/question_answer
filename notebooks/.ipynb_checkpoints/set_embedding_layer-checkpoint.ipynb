{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb98cbd3-44b1-4ccb-a6f7-feca7b440a1d",
   "metadata": {},
   "source": [
    "### Notebook: Set the embedding layer, update Mongo database\n",
    "1. From Mongodb\n",
    "2. From Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2596aca2-ff60-4e23-b444-e27cbbd68662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterargo/anaconda3/envs/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "# Get the current working directory (notebooks directory)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project directory\n",
    "project_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Assuming your project structure is as described before\n",
    "src_path = os.path.abspath(os.path.join(project_dir, 'src'))\n",
    "hyperparam_path = os.path.abspath(os.path.join(project_dir, 'vars'))\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from question_answer_site.question_answer.parse_document import pdfs_to_df, tokenize_df_of_texts\n",
    "from question_answer_site.question_answer.embedding_layer import load_custom_vectors, update_mongo_document\n",
    "from question_answer_site.question_answer.mongodb import MongoDb\n",
    "from question_answer_site.question_answer.utils import tokens_to_embeddings\n",
    "from question_answer_site.question_answer.config import TOKENIZER, TOKENS_TYPE, EMBEDDING_MODEL_TYPE, EMBEDDING_MODEL_FNAME,\\\n",
    "VECTOR_SIZE, WINDOW, MIN_COUNT, SG, DOCUMENT_EMBEDDING, INPUT_FOLDER, special_characters\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "import numpy as np\n",
    "from urllib.parse import quote_plus\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdf0e55-acc2-4e5b-a775-92f02f003171",
   "metadata": {},
   "source": [
    "#### Set embedding layer from mongodb\n",
    "- Specify tokenizer, keep consistent with downstream Q&A model (TOKENIZER)\n",
    "- Secify the data filepath (directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5986f51a-6216-44ae-a4a8-8dd1f2e632bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "username = \"new_user_1\"\n",
    "password = \"password33566\"\n",
    "# Escape the username and password\n",
    "escaped_username = quote_plus(username)\n",
    "escaped_password = quote_plus(password)\n",
    "\n",
    "cluster_url = \"cluster0\"\n",
    "database_name = \"question_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1434f9-bdbe-49a4-84e8-7ebc49515bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get tokens and counter values from all documents in the mongodb\n",
    "collection_name = \"parsed_documents\"\n",
    "\n",
    "# Create a MongoClient and connect to the server\n",
    "mongodb = MongoDb(escaped_username, escaped_password, cluster_url, database_name, collection_name)\n",
    "if mongodb.connect():\n",
    "    cursor = mongodb.get_collection().find({}, {TOKENS_TYPE:1, 'counter':1, '_id':0})\n",
    "\n",
    "# Assuming 'data' is your list of dictionaries\n",
    "df = pd.DataFrame(list(cursor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7814b40c-c9a5-4be9-84da-a37cebcc2e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_less_sw</th>\n",
       "      <th>counter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Ġcommercial, Ġaviation, Ġcustomers, Ġmarch, Ġ...</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Ġoriginal, mber, Ġretrieved, Ġse, pt, ember, ...</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Ġj, ames, Ġweb, b, Ġspace, Ġtelescope, Ġwik, ...</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ember, )., activ, ating, Ġstar, link, ...\", w...</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[priority, Ġscience, Ġgoal, Ġbeyond, Ġh, st, '...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      tokens_less_sw  counter\n",
       "0  [Ġcommercial, Ġaviation, Ġcustomers, Ġmarch, Ġ...      115\n",
       "1  [Ġoriginal, mber, Ġretrieved, Ġse, pt, ember, ...      284\n",
       "2  [Ġj, ames, Ġweb, b, Ġspace, Ġtelescope, Ġwik, ...      142\n",
       "3  [ember, )., activ, ating, Ġstar, link, ...\", w...      292\n",
       "4  [priority, Ġscience, Ġgoal, Ġbeyond, Ġh, st, '...      154"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63895e3e-8222-45d9-a0d5-9b6338db2871",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "BUILDDIR path: /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/vocab_count -min-count 3 -verbose 2 < training_data.txt > vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[0GProcessed 83741 tokens.\n",
      "Counted 8056 unique words.\n",
      "Truncating vocabulary at min count 3.\n",
      "Using vocabulary of size 3672.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 3 < training_data.txt > cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 3\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"vocab.txt\"...loaded 3672 words.\n",
      "Building lookup table...table contains 13483585 elements.\n",
      "Processing token: 0\u001b[0GProcessed 83741 tokens.\n",
      "Writing cooccurrences to disk.......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[0GMerging cooccurrence files: processed 222077 lines.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\n",
      "Using random seed 1699395868\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 222077 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G222077 lines.\u001b[0GMerging temp files: processed 222077 lines.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 30 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 222077 lines.\n",
      "Initializing parameters...Using random seed 1699395868\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 3672\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "11/07/23 - 02:24.28PM, iter: 001, cost: 0.108278\n",
      "11/07/23 - 02:24.28PM, iter: 002, cost: 0.087567\n",
      "11/07/23 - 02:24.28PM, iter: 003, cost: 0.079085\n",
      "11/07/23 - 02:24.28PM, iter: 004, cost: 0.071549\n",
      "11/07/23 - 02:24.28PM, iter: 005, cost: 0.067139\n",
      "11/07/23 - 02:24.28PM, iter: 006, cost: 0.062581\n",
      "11/07/23 - 02:24.28PM, iter: 007, cost: 0.058601\n",
      "11/07/23 - 02:24.28PM, iter: 008, cost: 0.054745\n",
      "11/07/23 - 02:24.28PM, iter: 009, cost: 0.050791\n",
      "11/07/23 - 02:24.28PM, iter: 010, cost: 0.046798\n",
      "11/07/23 - 02:24.28PM, iter: 011, cost: 0.043034\n",
      "11/07/23 - 02:24.28PM, iter: 012, cost: 0.039600\n",
      "11/07/23 - 02:24.28PM, iter: 013, cost: 0.036581\n",
      "11/07/23 - 02:24.28PM, iter: 014, cost: 0.033915\n",
      "11/07/23 - 02:24.28PM, iter: 015, cost: 0.031553\n",
      "11/07/23 - 02:24.28PM, iter: 016, cost: 0.029456\n",
      "11/07/23 - 02:24.29PM, iter: 017, cost: 0.027590\n",
      "11/07/23 - 02:24.29PM, iter: 018, cost: 0.025920\n",
      "11/07/23 - 02:24.29PM, iter: 019, cost: 0.024414\n",
      "11/07/23 - 02:24.29PM, iter: 020, cost: 0.023040\n",
      "11/07/23 - 02:24.29PM, iter: 021, cost: 0.021820\n",
      "11/07/23 - 02:24.29PM, iter: 022, cost: 0.020693\n",
      "11/07/23 - 02:24.29PM, iter: 023, cost: 0.019682\n",
      "11/07/23 - 02:24.29PM, iter: 024, cost: 0.018758\n",
      "11/07/23 - 02:24.29PM, iter: 025, cost: 0.017915\n",
      "11/07/23 - 02:24.29PM, iter: 026, cost: 0.017142\n",
      "11/07/23 - 02:24.29PM, iter: 027, cost: 0.016426\n",
      "11/07/23 - 02:24.29PM, iter: 028, cost: 0.015776\n",
      "11/07/23 - 02:24.29PM, iter: 029, cost: 0.015160\n",
      "11/07/23 - 02:24.29PM, iter: 030, cost: 0.014595\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "if EMBEDDING_MODEL_TYPE == 'Word2Vec':\n",
    "    kwargs = {\n",
    "     'sentences':df[TOKENS_TYPE].to_list(),\n",
    "     'vector_size':VECTOR_SIZE,\n",
    "     'window':WINDOW,\n",
    "     'min_count':MIN_COUNT,\n",
    "     'sg':SG\n",
    "    }\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    model = Word2Vec(**kwargs)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(os.path.join(\"..\", \"models\", \"word_embeddings\", EMBEDDING_MODEL_FNAME))\n",
    "    \n",
    "elif EMBEDDING_MODEL_TYPE == 'glove':\n",
    "    # Specify the file path for the output text file\n",
    "    output_file = os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\", 'training_data.txt')\n",
    "\n",
    "    # Write the \"tokens\" column to a text file with each row on a separate line\n",
    "    if os.getcwd().endswith('glove'):\n",
    "        os.chdir(os.path.join(\"..\", \"..\", \"..\", \"notebooks\"))\n",
    "    df[TOKENS_TYPE].apply(lambda x: ' '.join(x)).to_csv(output_file, header=False, index=False, sep='\\n', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    os.environ[\"VECTOR_SIZE\"] = str(VECTOR_SIZE)\n",
    "    os.environ[\"WINDOW_SIZE\"] = str(WINDOW)\n",
    "    os.environ[\"VOCAB_MIN_COUNT\"] = str(MIN_COUNT)\n",
    "    sys.path.append(os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\"))\n",
    "    \n",
    "    # Train the model\n",
    "    os.chdir(os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\"))\n",
    "    !./demo.sh\n",
    "    if os.getcwd().endswith('glove'):\n",
    "        os.chdir(os.path.join(\"..\", \"..\", \"..\", \"notebooks\"))\n",
    "    \n",
    "    # Path to your GloVe vectors file\n",
    "    vectors_file = os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\", \"vectors.txt\")\n",
    "\n",
    "    # Load the custom spaCy model with GloVe vectors\n",
    "    custom_nlp = load_custom_vectors(vectors_file)\n",
    "\n",
    "    # Save the custom spaCy model to a directory\n",
    "    custom_nlp.to_disk(os.path.join(\"..\", \"models\", \"word_embeddings\", EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe546fc-fc69-4fa1-aa79-403acc82ef3a",
   "metadata": {},
   "source": [
    "##### Add the embeddings model to to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2603a188-4480-4128-8494-d00b5d9b6ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "if EMBEDDING_MODEL_TYPE == 'Word2Vec':\n",
    "    model = Word2Vec.load(os.path.join(\"..\", \"models\", \"word_embeddings\", EMBEDDING_MODEL_FNAME))\n",
    "\n",
    "elif EMBEDDING_MODEL_TYPE.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    model = spacy.load(os.path.join(\"..\", \"models\", \"word_embeddings\", EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))\n",
    "\n",
    "# Update dataframe with token embeddings\n",
    "df[DOCUMENT_EMBEDDING] = df[TOKENS_TYPE].apply(tokens_to_embeddings, args=(model,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df627653-2ee4-42d9-acc8-9a80b3936c45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      None\n",
       "1      None\n",
       "2      None\n",
       "3      None\n",
       "4      None\n",
       "       ... \n",
       "364    None\n",
       "365    None\n",
       "366    None\n",
       "367    None\n",
       "368    None\n",
       "Length: 369, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to update MongoDB for each row in the DataFrame\n",
    "df.apply(update_mongo_document, args=(mongodb,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0701cb82-4b17-4e1a-a1e3-72ac7427e882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777fb34f-f7b1-401b-a92f-5437d9fbc384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41fbd552-04e6-4db4-b37c-a1eeb7e7af3c",
   "metadata": {},
   "source": [
    "#### Set the embedding layer from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6710e48-8747-471c-a3ff-eebfb2989323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory \n",
    "directory = os.path.join(\"..\", \"data\", INPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb659758-9325-4bdf-8a73-be97bad99559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/space_based_pdfs/Galaxy 15 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Swarm Technologies - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Fengyun - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Falcon 9 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Cygnus NG-19 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Atlas V - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Inmarsat - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Kepler-11 - Wikipedia.pdf\n",
      "../data/space_based_pdfs/James Webb Space Telescope - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Space-Based Infrared System - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Yaogan - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Starlink - Wikipedia.pdf\n",
      "../data/space_based_pdfs/Atlas (rocket family) - Wikipedia.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "['sha_256', 'language', 'language_probability', 'chunk_text', 'chunk_text_less_sw', 'token_embeddings', 'token_embeddings_less_sw']\n"
     ]
    }
   ],
   "source": [
    "# From the test pdf dir, extract the text and tokenize it. Store in pandas dataframe\n",
    "df = pdfs_to_df(directory)\n",
    "df = tokenize_df_of_texts(df, tokenizers[TOKENIZER], REMOVE_SW_COL=True, additional_stopwords=special_characters)\n",
    "\n",
    "drop_cols = [col for col in df.columns if col not in ['Document', 'Text', 'Original_Text', 'Path', 'tokens', 'tokens_less_sw']]\n",
    "print(drop_cols)\n",
    "\n",
    "df = df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14a54847-c44d-4d9e-8b1c-57f5fb6f6fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Path</th>\n",
       "      <th>Text</th>\n",
       "      <th>Original_Text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_less_sw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Galaxy 15 - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Galaxy 15 - Wikipedia...</td>\n",
       "      <td>8/27/23, 9:28 galaxy 15 wikipedia 1/8 galaxy 1...</td>\n",
       "      <td>8/27/23, 9:28 PM\\nGalaxy 15 - Wikipedia\\nhttps...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 28, Ġgalaxy, Ġ15,...</td>\n",
       "      <td>[Ġgalaxy, Ġwik, ipedia, Ġgalaxy, Ġanimation, Ġ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Swarm Technologies - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Swarm Technologies - ...</td>\n",
       "      <td>8/27/23, 9:31 swarm technologies wikipedia 1/5...</td>\n",
       "      <td>8/27/23, 9:31 PM\\nSwarm Technologies - Wikiped...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 31, Ġswarm, Ġtech...</td>\n",
       "      <td>[Ġswarm, Ġtechnologies, Ġwik, ipedia, Ġswarm, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fengyun - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Fengyun - Wikipedia.pdf</td>\n",
       "      <td>8/27/23, 9:29 fengyun wikipedia 1/4 fengyun ⻛云...</td>\n",
       "      <td>8/27/23, 9:29 PM\\nFengyun - Wikipedia\\nhttps:/...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 29, Ġf, en, gy, u...</td>\n",
       "      <td>[Ġf, en, gy, un, Ġwik, ipedia, Ġf, en, gy, un,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Falcon 9 - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Falcon 9 - Wikipedia.pdf</td>\n",
       "      <td>8/27/23, 9:33 falcon wikipedia 1/24 falcon fal...</td>\n",
       "      <td>8/27/23, 9:33 PM\\nFalcon 9 - Wikipedia\\nhttps:...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 33, Ġfal, con, Ġw...</td>\n",
       "      <td>[Ġfal, con, Ġwik, ipedia, Ġfal, con, Ġfal, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cygnus NG-19 - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Cygnus NG-19 - Wikipe...</td>\n",
       "      <td>8/27/23, 9:29 cygnus ng-19 wikipedia 1/4 ng-19...</td>\n",
       "      <td>8/27/23, 9:29 PM\\nCygnus NG-19 - Wikipedia\\nht...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 29, Ġcy, gn, us, ...</td>\n",
       "      <td>[Ġcy, gn, us, Ġng, Ġwik, ipedia, Ġng, Ġartists...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Atlas V - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Atlas V - Wikipedia.pdf</td>\n",
       "      <td>8/27/23, 9:36 atlas wikipedia 1/23 atlas launc...</td>\n",
       "      <td>8/27/23, 9:36 PM\\nAtlas V - Wikipedia\\nhttps:/...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 36, Ġat, las, Ġwi...</td>\n",
       "      <td>[las, Ġwik, ipedia, las, Ġlaunch, las, Ġcarryi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Inmarsat - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Inmarsat - Wikipedia.pdf</td>\n",
       "      <td>8/27/23, 9:36 inmarsat wikipedia 1/20 inmarsat...</td>\n",
       "      <td>8/27/23, 9:36 PM\\nInmarsat - Wikipedia\\nhttps:...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 36, Ġin, m, ars, ...</td>\n",
       "      <td>[ars, Ġwik, ipedia, ars, Ġglobal, Ġtype, Ġsubs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kepler-11 - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Kepler-11 - Wikipedia...</td>\n",
       "      <td>8/27/23, 9:34 kepler-11 wikipedia 1/3 kepler-1...</td>\n",
       "      <td>8/27/23, 9:34 PM\\nKepler-11 - Wikipedia\\nhttps...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 34, Ġke, pler, -,...</td>\n",
       "      <td>[Ġke, pler, Ġwik, ipedia, Ġke, pler, Ġartist, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>James Webb Space Telescope - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/James Webb Space Tele...</td>\n",
       "      <td>8/27/23, 9:40 james webb space telescope wikip...</td>\n",
       "      <td>8/27/23, 9:40 PM\\nJames Webb Space Telescope -...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 40, Ġj, ames, Ġwe...</td>\n",
       "      <td>[Ġj, ames, Ġweb, b, Ġspace, Ġtelescope, Ġwik, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Space-Based Infrared System - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Space-Based Infrared ...</td>\n",
       "      <td>8/27/23, 8:44 space-based infrared system wiki...</td>\n",
       "      <td>8/27/23, 8:44 PM\\nSpace-Based Infrared System ...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ8, :, 44, Ġspace, -, ba...</td>\n",
       "      <td>[Ġspace, based, Ġinfrared, Ġsystem, Ġwik, iped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yaogan - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Yaogan - Wikipedia.pdf</td>\n",
       "      <td>8/27/23, 9:39 yaogan wikipedia 1/9 yaogan 遥感卫星...</td>\n",
       "      <td>8/27/23, 9:39 PM\\nYaogan - Wikipedia\\nhttps://...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 39, Ġya, ogan, Ġw...</td>\n",
       "      <td>[Ġya, ogan, Ġwik, ipedia, Ġya, ogan, Ġé, ģ, ¥,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Starlink - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Starlink - Wikipedia.pdf</td>\n",
       "      <td>8/27/23, 9:25 starlink wikipedia 1/47 starlink...</td>\n",
       "      <td>8/27/23, 9:25 PM\\nStarlink - Wikipedia\\nhttps:...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 25, Ġstar, link, ...</td>\n",
       "      <td>[Ġstar, link, Ġwik, ipedia, Ġstar, link, Ġstar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Atlas (rocket family) - Wikipedia.pdf</td>\n",
       "      <td>../data/space_based_pdfs/Atlas (rocket family)...</td>\n",
       "      <td>8/27/23, 9:26 atlas (rocket family) wikipedia ...</td>\n",
       "      <td>8/27/23, 9:26 PM\\nAtlas (rocket family) - Wiki...</td>\n",
       "      <td>[Ġ8, /, 27, /, 23, ,, Ġ9, :, 26, Ġat, las, Ġ(,...</td>\n",
       "      <td>[las, rocket, Ġfamily, Ġwik, ipedia, las, Ġfam...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Document  \\\n",
       "0                     Galaxy 15 - Wikipedia.pdf   \n",
       "1            Swarm Technologies - Wikipedia.pdf   \n",
       "2                       Fengyun - Wikipedia.pdf   \n",
       "3                      Falcon 9 - Wikipedia.pdf   \n",
       "4                  Cygnus NG-19 - Wikipedia.pdf   \n",
       "5                       Atlas V - Wikipedia.pdf   \n",
       "6                      Inmarsat - Wikipedia.pdf   \n",
       "7                     Kepler-11 - Wikipedia.pdf   \n",
       "8    James Webb Space Telescope - Wikipedia.pdf   \n",
       "9   Space-Based Infrared System - Wikipedia.pdf   \n",
       "10                       Yaogan - Wikipedia.pdf   \n",
       "11                     Starlink - Wikipedia.pdf   \n",
       "12        Atlas (rocket family) - Wikipedia.pdf   \n",
       "\n",
       "                                                 Path  \\\n",
       "0   ../data/space_based_pdfs/Galaxy 15 - Wikipedia...   \n",
       "1   ../data/space_based_pdfs/Swarm Technologies - ...   \n",
       "2    ../data/space_based_pdfs/Fengyun - Wikipedia.pdf   \n",
       "3   ../data/space_based_pdfs/Falcon 9 - Wikipedia.pdf   \n",
       "4   ../data/space_based_pdfs/Cygnus NG-19 - Wikipe...   \n",
       "5    ../data/space_based_pdfs/Atlas V - Wikipedia.pdf   \n",
       "6   ../data/space_based_pdfs/Inmarsat - Wikipedia.pdf   \n",
       "7   ../data/space_based_pdfs/Kepler-11 - Wikipedia...   \n",
       "8   ../data/space_based_pdfs/James Webb Space Tele...   \n",
       "9   ../data/space_based_pdfs/Space-Based Infrared ...   \n",
       "10    ../data/space_based_pdfs/Yaogan - Wikipedia.pdf   \n",
       "11  ../data/space_based_pdfs/Starlink - Wikipedia.pdf   \n",
       "12  ../data/space_based_pdfs/Atlas (rocket family)...   \n",
       "\n",
       "                                                 Text  \\\n",
       "0   8/27/23, 9:28 galaxy 15 wikipedia 1/8 galaxy 1...   \n",
       "1   8/27/23, 9:31 swarm technologies wikipedia 1/5...   \n",
       "2   8/27/23, 9:29 fengyun wikipedia 1/4 fengyun ⻛云...   \n",
       "3   8/27/23, 9:33 falcon wikipedia 1/24 falcon fal...   \n",
       "4   8/27/23, 9:29 cygnus ng-19 wikipedia 1/4 ng-19...   \n",
       "5   8/27/23, 9:36 atlas wikipedia 1/23 atlas launc...   \n",
       "6   8/27/23, 9:36 inmarsat wikipedia 1/20 inmarsat...   \n",
       "7   8/27/23, 9:34 kepler-11 wikipedia 1/3 kepler-1...   \n",
       "8   8/27/23, 9:40 james webb space telescope wikip...   \n",
       "9   8/27/23, 8:44 space-based infrared system wiki...   \n",
       "10  8/27/23, 9:39 yaogan wikipedia 1/9 yaogan 遥感卫星...   \n",
       "11  8/27/23, 9:25 starlink wikipedia 1/47 starlink...   \n",
       "12  8/27/23, 9:26 atlas (rocket family) wikipedia ...   \n",
       "\n",
       "                                        Original_Text  \\\n",
       "0   8/27/23, 9:28 PM\\nGalaxy 15 - Wikipedia\\nhttps...   \n",
       "1   8/27/23, 9:31 PM\\nSwarm Technologies - Wikiped...   \n",
       "2   8/27/23, 9:29 PM\\nFengyun - Wikipedia\\nhttps:/...   \n",
       "3   8/27/23, 9:33 PM\\nFalcon 9 - Wikipedia\\nhttps:...   \n",
       "4   8/27/23, 9:29 PM\\nCygnus NG-19 - Wikipedia\\nht...   \n",
       "5   8/27/23, 9:36 PM\\nAtlas V - Wikipedia\\nhttps:/...   \n",
       "6   8/27/23, 9:36 PM\\nInmarsat - Wikipedia\\nhttps:...   \n",
       "7   8/27/23, 9:34 PM\\nKepler-11 - Wikipedia\\nhttps...   \n",
       "8   8/27/23, 9:40 PM\\nJames Webb Space Telescope -...   \n",
       "9   8/27/23, 8:44 PM\\nSpace-Based Infrared System ...   \n",
       "10  8/27/23, 9:39 PM\\nYaogan - Wikipedia\\nhttps://...   \n",
       "11  8/27/23, 9:25 PM\\nStarlink - Wikipedia\\nhttps:...   \n",
       "12  8/27/23, 9:26 PM\\nAtlas (rocket family) - Wiki...   \n",
       "\n",
       "                                               tokens  \\\n",
       "0   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 28, Ġgalaxy, Ġ15,...   \n",
       "1   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 31, Ġswarm, Ġtech...   \n",
       "2   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 29, Ġf, en, gy, u...   \n",
       "3   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 33, Ġfal, con, Ġw...   \n",
       "4   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 29, Ġcy, gn, us, ...   \n",
       "5   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 36, Ġat, las, Ġwi...   \n",
       "6   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 36, Ġin, m, ars, ...   \n",
       "7   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 34, Ġke, pler, -,...   \n",
       "8   [Ġ8, /, 27, /, 23, ,, Ġ9, :, 40, Ġj, ames, Ġwe...   \n",
       "9   [Ġ8, /, 27, /, 23, ,, Ġ8, :, 44, Ġspace, -, ba...   \n",
       "10  [Ġ8, /, 27, /, 23, ,, Ġ9, :, 39, Ġya, ogan, Ġw...   \n",
       "11  [Ġ8, /, 27, /, 23, ,, Ġ9, :, 25, Ġstar, link, ...   \n",
       "12  [Ġ8, /, 27, /, 23, ,, Ġ9, :, 26, Ġat, las, Ġ(,...   \n",
       "\n",
       "                                       tokens_less_sw  \n",
       "0   [Ġgalaxy, Ġwik, ipedia, Ġgalaxy, Ġanimation, Ġ...  \n",
       "1   [Ġswarm, Ġtechnologies, Ġwik, ipedia, Ġswarm, ...  \n",
       "2   [Ġf, en, gy, un, Ġwik, ipedia, Ġf, en, gy, un,...  \n",
       "3   [Ġfal, con, Ġwik, ipedia, Ġfal, con, Ġfal, con...  \n",
       "4   [Ġcy, gn, us, Ġng, Ġwik, ipedia, Ġng, Ġartists...  \n",
       "5   [las, Ġwik, ipedia, las, Ġlaunch, las, Ġcarryi...  \n",
       "6   [ars, Ġwik, ipedia, ars, Ġglobal, Ġtype, Ġsubs...  \n",
       "7   [Ġke, pler, Ġwik, ipedia, Ġke, pler, Ġartist, ...  \n",
       "8   [Ġj, ames, Ġweb, b, Ġspace, Ġtelescope, Ġwik, ...  \n",
       "9   [Ġspace, based, Ġinfrared, Ġsystem, Ġwik, iped...  \n",
       "10  [Ġya, ogan, Ġwik, ipedia, Ġya, ogan, Ġé, ģ, ¥,...  \n",
       "11  [Ġstar, link, Ġwik, ipedia, Ġstar, link, Ġstar...  \n",
       "12  [las, rocket, Ġfamily, Ġwik, ipedia, las, Ġfam...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df65c0f4-f289-42c6-a0e1-dcfb76cff4ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train model on tokenized text\n",
    "- Set:\n",
    "    - Input data: Either \"tokens\" or \"tokens_less_sw\" (TOKENS_TYPE)\n",
    "    - Vector Size: length of word embeddings\n",
    "    - Window Size: span of sorrounding words to train model\n",
    "    - Min Count: minimum number of occurances of word to be be viable\n",
    "    - Ouput model file name: (model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdd18fea-f5f9-462f-bb46-cba3d6955806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir -p build\n",
      "BUILDDIR path: /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/vocab_count -min-count 3 -verbose 2 < training_data.txt > vocab.txt\n",
      "BUILDING VOCABULARY\n",
      "Processed 0 tokens.\u001b[0GProcessed 72779 tokens.\n",
      "Counted 7751 unique words.\n",
      "Truncating vocabulary at min count 3.\n",
      "Using vocabulary of size 3410.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/cooccur -memory 4.0 -vocab-file vocab.txt -verbose 2 -window-size 3 < training_data.txt > cooccurrence.bin\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 3\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"vocab.txt\"...loaded 3410 words.\n",
      "Building lookup table...table contains 11628101 elements.\n",
      "Processing token: 0\u001b[0GProcessed 72779 tokens.\n",
      "Writing cooccurrences to disk.......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G0 lines.\u001b[39G100000 lines.\u001b[0GMerging cooccurrence files: processed 199198 lines.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/shuffle -memory 4.0 -verbose 2 < cooccurrence.bin > cooccurrence.shuf.bin\n",
      "Using random seed 1693813655\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 199198 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G199198 lines.\u001b[0GMerging temp files: processed 199198 lines.\n",
      "\n",
      "$ /Users/peterargo/Documents/projects/question_and_answer/models/word_embeddings/glove/build/glove -save-file vectors -threads 8 -input-file cooccurrence.shuf.bin -x-max 10 -iter 30 -vector-size 50 -binary 2 -vocab-file vocab.txt -verbose 2\n",
      "TRAINING MODEL\n",
      "Read 199198 lines.\n",
      "Initializing parameters...Using random seed 1693813655\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 3410\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "09/04/23 - 12:47.36AM, iter: 001, cost: 0.106122\n",
      "09/04/23 - 12:47.36AM, iter: 002, cost: 0.085468\n",
      "09/04/23 - 12:47.36AM, iter: 003, cost: 0.079532\n",
      "09/04/23 - 12:47.36AM, iter: 004, cost: 0.071120\n",
      "09/04/23 - 12:47.36AM, iter: 005, cost: 0.066502\n",
      "09/04/23 - 12:47.36AM, iter: 006, cost: 0.062162\n",
      "09/04/23 - 12:47.36AM, iter: 007, cost: 0.058245\n",
      "09/04/23 - 12:47.36AM, iter: 008, cost: 0.054548\n",
      "09/04/23 - 12:47.36AM, iter: 009, cost: 0.050748\n",
      "09/04/23 - 12:47.36AM, iter: 010, cost: 0.046913\n",
      "09/04/23 - 12:47.36AM, iter: 011, cost: 0.043250\n",
      "09/04/23 - 12:47.36AM, iter: 012, cost: 0.039924\n",
      "09/04/23 - 12:47.36AM, iter: 013, cost: 0.036945\n",
      "09/04/23 - 12:47.36AM, iter: 014, cost: 0.034343\n",
      "09/04/23 - 12:47.36AM, iter: 015, cost: 0.032013\n",
      "09/04/23 - 12:47.36AM, iter: 016, cost: 0.029932\n",
      "09/04/23 - 12:47.36AM, iter: 017, cost: 0.028049\n",
      "09/04/23 - 12:47.36AM, iter: 018, cost: 0.026357\n",
      "09/04/23 - 12:47.36AM, iter: 019, cost: 0.024820\n",
      "09/04/23 - 12:47.36AM, iter: 020, cost: 0.023434\n",
      "09/04/23 - 12:47.36AM, iter: 021, cost: 0.022168\n",
      "09/04/23 - 12:47.36AM, iter: 022, cost: 0.021012\n",
      "09/04/23 - 12:47.36AM, iter: 023, cost: 0.019962\n",
      "09/04/23 - 12:47.36AM, iter: 024, cost: 0.019006\n",
      "09/04/23 - 12:47.36AM, iter: 025, cost: 0.018117\n",
      "09/04/23 - 12:47.36AM, iter: 026, cost: 0.017310\n",
      "09/04/23 - 12:47.36AM, iter: 027, cost: 0.016570\n",
      "09/04/23 - 12:47.36AM, iter: 028, cost: 0.015883\n",
      "09/04/23 - 12:47.36AM, iter: 029, cost: 0.015246\n",
      "09/04/23 - 12:47.36AM, iter: 030, cost: 0.014648\n"
     ]
    }
   ],
   "source": [
    "# Train Word2Vec model\n",
    "embedding_model_type = hyperparams['embedding_model_type']\n",
    "if embedding_model_type == 'Word2Vec':\n",
    "    kwargs = {\n",
    "     'sentences':df[TOKENS_TYPE].to_list(),\n",
    "     'vector_size':VECTOR_SIZE,\n",
    "     'window':WINDOW,\n",
    "     'min_count':MIN_COUNT,\n",
    "     'sg':hyperparams[\"sg\"]\n",
    "    }\n",
    "    \n",
    "    # Train the Word2Vec model\n",
    "    model = Word2Vec(**kwargs)\n",
    "    \n",
    "    # Save the model\n",
    "    model.save(os.path.join(\"..\", \"models\", \"word_embeddings\", EMBEDDING_MODEL_FNAME))\n",
    "    \n",
    "elif embedding_model_type == 'glove':\n",
    "    # Specify the file path for the output text file\n",
    "    output_file = os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\", 'training_data.txt')\n",
    "\n",
    "    # Write the \"tokens\" column to a text file with each row on a separate line\n",
    "    if os.getcwd().endswith('glove'):\n",
    "        os.chdir(os.path.join(\"..\", \"..\", \"..\", \"notebooks\"))\n",
    "    df[TOKENS_TYPE].apply(lambda x: ' '.join(x)).to_csv(output_file, header=False, index=False, sep='\\n', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "    os.environ[\"VECTOR_SIZE\"] = str(VECTOR_SIZE)\n",
    "    os.environ[\"WINDOW_SIZE\"] = str(WINDOW)\n",
    "    os.environ[\"VOCAB_MIN_COUNT\"] = str(MIN_COUNT)\n",
    "    sys.path.append(os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\"))\n",
    "    \n",
    "    # Train the model\n",
    "    os.chdir(os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\"))\n",
    "    !./demo.sh\n",
    "    if os.getcwd().endswith('glove'):\n",
    "        os.chdir(os.path.join(\"..\", \"..\", \"..\", \"notebooks\"))\n",
    "    \n",
    "    # Path to your GloVe vectors file\n",
    "    vectors_file = os.path.join(\"..\", \"models\", \"word_embeddings\", \"glove\", \"vectors.txt\")\n",
    "\n",
    "    # Load the custom spaCy model with GloVe vectors\n",
    "    custom_nlp = load_custom_vectors(vectors_file)\n",
    "\n",
    "    # Save the custom spaCy model to a directory\n",
    "    custom_nlp.to_disk(os.path.join(\"..\", \"models\", \"word_embeddings\", EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "325ca7b9-10a1-4759-8c58-b5edd2019c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'roberta_space_based_pdfs_glove_model.bin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2eff82a-9997-49e8-ada3-e99515c4e964",
   "metadata": {},
   "source": [
    "#### Examine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7a83fa-eff0-4458-8903-ff0a3ff5cc6e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count token frequencies\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m token_frequencies \u001b[38;5;241m=\u001b[39m \u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Print the frequency of \"number\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrequency of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_frequencies[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/collections/__init__.py:552\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    549\u001b[0m \n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 552\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/collections/__init__.py:637\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[38;5;28msuper\u001b[39m(Counter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate(iterable) \u001b[38;5;66;03m# fast path when counter is empty\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 637\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count token frequencies\n",
    "token_frequencies = Counter(df['tokens'].to_list())\n",
    "\n",
    "# Print the frequency of \"number\"\n",
    "print(\"Frequency of 'number':\", token_frequencies[\"number\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c141a3dd-47ed-41c5-a6b2-543083323093",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Ġrevenue' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mĠ\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TOKENIZER \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m word\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Access the embedding of a word\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Find similar words based on embedding similarity\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Ġrevenue' not present\""
     ]
    }
   ],
   "source": [
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(os.path.join(\"..\", \"models\", \"word_embeddings\", EMBEDDING_MODEL_FNAME))\n",
    "\n",
    "word = \"revenue\"\n",
    "\n",
    "# Add the special preface character if the tokenizer for roberta was used\n",
    "word = f\"Ġ{word}\" if TOKENIZER == 'roberta' else word\n",
    "\n",
    "# Access the embedding of a word\n",
    "embedding = model.wv[word]\n",
    "print(embedding)\n",
    "# Find similar words based on embedding similarity\n",
    "similar_words = model.wv.most_similar(word)\n",
    "print(similar_words)\n",
    "\n",
    "# You can also perform vector arithmetic operations\n",
    "# result = model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c82ce892-41cf-41e2-9ab8-b84043f24fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocabulary: 3410\n",
      "Is 'number' in vocabulary? True\n"
     ]
    }
   ],
   "source": [
    "vocabulary = model.wv.index_to_key\n",
    "print(\"Number of words in vocabulary:\", len(vocabulary))\n",
    "print(\"Is 'number' in vocabulary?\", 'number' in vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "441fb34d-7f82-42bc-bf05-750c8996dcf0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '[PAD]' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[PAD]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '[PAD]' not present\""
     ]
    }
   ],
   "source": [
    "token = '[PAD]'\n",
    "\n",
    "print(model.wv['[PAD]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d291b722-ea15-4915-a40b-6eb6b1c3c1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
