{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d7b836-2be2-4d93-8e56-646b1965cb02",
   "metadata": {},
   "source": [
    "### Notebook: Write query and get top N similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90027071-a4ef-4f1f-94b5-8b1435ada488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterargo/anaconda3/envs/question_answer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the current working directory (notebooks directory)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project directory\n",
    "project_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Assuming your project structure is as described before\n",
    "src_path = os.path.abspath(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from question_answer_site.question_answer.mongodb import MongoDb\n",
    "from question_answer_site.question_answer.utils import remove_non_word_chars, clean_text, tokens_to_embeddings, post_process_output, correct_spelling\n",
    "from question_answer_site.question_answer.config import TOKENIZER, EMBEDDING_MODEL_FNAME, EMBEDDING_MODEL_TYPE, TOKENS_EMBEDDINGS, DOCUMENT_EMBEDDING, \\\n",
    "    DOCUMENT_TOKENS, TOP_N, TRANSFORMER_MODEL_NAME, METHOD, MAX_QUERY_LENGTH, username, password, cluster_url, \\\n",
    "    database_name\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "from urllib.parse import quote_plus\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8300c24-3249-4f55-ac95-df4a0aa5ad1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What happened to signals intelligence during world war 1?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "375ba3c1-0bc1-4ec8-8eba-8493924d3d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_user_1 password33566 cluster0 question_answer\n"
     ]
    }
   ],
   "source": [
    "print(username, password, cluster_url, database_name)\n",
    "escaped_username = quote_plus(username)\n",
    "escaped_password = quote_plus(password)\n",
    "\n",
    "# use MongoDb class to connect to database instance and get the documents\n",
    "mongo_db = MongoDb(escaped_username, escaped_password, cluster_url, database_name, \"parsed_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea362d42-09ed-4b70-9c9f-a70468d0c709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n"
     ]
    }
   ],
   "source": [
    "if mongo_db.connect():\n",
    "    print(mongo_db.count_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "322a8f7a-7582-4135-bf91-aacacf5b6e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space=True)\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "tokenizer = tokenizers[TOKENIZER]\n",
    "\n",
    "# Load your trained Word2Vec model\n",
    "if EMBEDDING_MODEL_TYPE == 'Word2Vec':\n",
    "    embedding_model = Word2Vec.load(\n",
    "        os.path.join(os.getcwd(), \"question_answer\", \"embedding_models\", EMBEDDING_MODEL_FNAME))\n",
    "elif EMBEDDING_MODEL_TYPE.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    embedding_model = spacy.load(os.path.join(\"..\",\"src\",\"question_answer_site\", \"question_answer\", \"embedding_models\",\n",
    "                                    EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))\n",
    "\n",
    "# Specify Candidate token embeddings option\n",
    "if TOKENS_EMBEDDINGS == \"query\":\n",
    "    TOKENS = \"tokenized_query\"\n",
    "    EMBEDDINGS = \"query_embedding\"\n",
    "elif TOKENS_EMBEDDINGS == \"query_search\":\n",
    "    TOKENS = \"tokenized_query_search\"\n",
    "    EMBEDDINGS = \"query_embedding_search\"\n",
    "else:\n",
    "    TOKENS = \"tokenized_query_search_less_sw\"\n",
    "    EMBEDDINGS = \"query_embedding_search_less_sw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6636b8f8-03fc-43a4-8847-3dfb9898eae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spell_check(user_query):\n",
    "    tokenized_query = tokenizer.tokenize(user_query)\n",
    "\n",
    "    # Group tokens into words\n",
    "    words = []\n",
    "    current_word = \"\"\n",
    "    for token in tokenized_query:\n",
    "        if token.startswith(\"Ġ\"):  # Indicates the start of a new word\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "            current_word = token[1:] if token[1:] not in ['(', '[', '{', '/', '\\\\'] else ''\n",
    "        else:\n",
    "            current_word += token if token not in [')', ']', '}', '/', '\\\\', '?', \".\", \"!\"] else ''\n",
    "            if token in ['/', '\\\\']:\n",
    "                words.append(current_word)\n",
    "                current_word = ''\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "\n",
    "    # Identify misspelled words not in the embeddings model\n",
    "    misspelled_words = []\n",
    "    for word in words:\n",
    "        # Split punctuation and hyphens from the word\n",
    "        base_word = \"\".join(char for char in word if char.isalnum() or char in [\"'\", \"-\"])\n",
    "        if any(list(map(lambda x: not any(x),\n",
    "                        tokens_to_embeddings(tokenizer.tokenize(base_word), embedding_model, RANDOM=False)))):\n",
    "            # Add the original word to the misspelled_words list\n",
    "            misspelled_words.append(word)\n",
    "    # Correct the spelling of misspelled words\n",
    "    corrected_words = {word: correct_spelling(word) for word in misspelled_words}\n",
    "\n",
    "    # Replace misspelled words in the original query\n",
    "    corrected_query = user_query\n",
    "    for original, corrected in corrected_words.items():\n",
    "        corrected_query = corrected_query.replace(original, corrected)\n",
    "\n",
    "    return corrected_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98de6a3-1442-4c27-af8a-2bec729cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_query):\n",
    "    user_query = user_query.lower()\n",
    "\n",
    "    # clean query for BERT input\n",
    "    user_query = clean_text(user_query)\n",
    "    print(\"Uncorrected query: \", user_query)\n",
    "    user_query = spell_check(user_query)\n",
    "    print(\"Corrected query: \", user_query)\n",
    "\n",
    "    # clean query for candidate search\n",
    "    user_query_for_search = remove_non_word_chars(user_query)\n",
    "\n",
    "    # Tokenize the query for BERT input\n",
    "    tokenized_query = tokenizer.tokenize(user_query)\n",
    "\n",
    "    # Tokenize the query for candidate search\n",
    "    tokenized_query_for_search = tokenizer.tokenize(user_query_for_search)\n",
    "\n",
    "    # Remove the stop words for the tokenized query for search\n",
    "    nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    nltk_stop_words.extend([\"Ġ\" + word for word in nltk_stop_words])  # Add the roberta modified tokens\n",
    "    tokenized_query_for_search_less_sw = [token for token in tokenized_query_for_search if\n",
    "                                          token not in nltk_stop_words]\n",
    "\n",
    "    # Pad or truncate the query to a fixed length of 20 tokens (BERT input)\n",
    "\n",
    "    if len(tokenized_query) > MAX_QUERY_LENGTH:\n",
    "        tokenized_query = tokenized_query[:MAX_QUERY_LENGTH]\n",
    "    else:\n",
    "        padding_length = MAX_QUERY_LENGTH - len(tokenized_query)\n",
    "        tokenized_query = tokenized_query + [tokenizer.pad_token] * padding_length\n",
    "\n",
    "    # Convert the tokenized query to input IDs and attention mask\n",
    "    input_ids_query = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
    "    attention_mask_query = [1] * len(input_ids_query)\n",
    "\n",
    "    # Convert to tensors\n",
    "    input_ids_query = torch.tensor(input_ids_query).unsqueeze(0)  # Add batch dimension\n",
    "    attention_mask_query = torch.tensor(attention_mask_query).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Get the query embeddings for the candidate document search\n",
    "    query_embeddings = tokens_to_embeddings(tokenized_query, embedding_model, RANDOM=False)\n",
    "    query_embeddings_search = tokens_to_embeddings(tokenized_query_for_search, embedding_model, RANDOM=False)\n",
    "    query_embeddings_less_sw = tokens_to_embeddings(tokenized_query_for_search_less_sw, embedding_model, RANDOM=False)\n",
    "\n",
    "    query_data = {\n",
    "        \"query\": user_query,\n",
    "        \"input_ids_query\": input_ids_query.tolist(),\n",
    "        \"attention_mask_query\": attention_mask_query.tolist(),\n",
    "        \"query_search\": user_query_for_search,\n",
    "        \"tokenized_query\": tokenized_query,\n",
    "        \"tokenized_query_search\": tokenized_query_for_search,\n",
    "        \"tokenized_query_search_less_sw\": tokenized_query_for_search_less_sw,\n",
    "        \"query_embedding\": query_embeddings, #.tolist(),  # Just used for the candidate search\n",
    "        \"query_embedding_search\": query_embeddings_search, #.tolist(),  # Just used for the candidate search, cleaned\n",
    "        \"query_embedding_search_less_sw\": query_embeddings_less_sw # .tolist()\n",
    "        # Just used for the candidate search, cleaned more\n",
    "    }\n",
    "    # return json.dumps(query_data['query'], indent=2)\n",
    "    return query_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e1f434-0b64-40a2-b72a-8f7dbf9e69aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrected query:  what happened to signals intelligence during world war 1?\n",
      "Corrected query:  what happened to signals intelligence during world war 1?\n",
      "['Ġwhat', 'Ġhappened', 'Ġto', 'Ġsignals', 'Ġintelligence', 'Ġduring', 'Ġworld', 'Ġwar', 'Ġ1', '?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "query_data = process_query(user_query=query)\n",
    "print(query_data[ \"tokenized_query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c421d341-f501-48e0-8f7d-964c15fbe23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_documents_from_mongo():\n",
    "    if mongo_db.connect():\n",
    "        documents = mongo_db.get_documents(query={}, inclusion={\"tokens\": 1, \"tokens_less_sw\": 1, \"counter\": 1,\n",
    "                                                                \"Document\": 1, \"_id\": 0})\n",
    "        documents = list(documents)\n",
    "        # documents = [document for document in mongo_db.iterate_documents()]\n",
    "        print(f\"Total documents: {mongo_db.count_documents()}\")\n",
    "        mongo_db.disconnect()\n",
    "        return documents\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6645ef9-f29f-45eb-97b7-cc57ef946d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_doc_sim_scores(document, query_data):\n",
    "    \"\"\"\n",
    "\n",
    "    :param document: (dict) Mongo queried document from parsed_documents\n",
    "    :param query_data: Processed query\n",
    "    :return: (float, dict) Similarity score and original document data\n",
    "    \"\"\"\n",
    "    query_embedding = np.array(query_data[EMBEDDINGS])\n",
    "    query_tokens = np.array(query_data[TOKENS])\n",
    "\n",
    "    # remove the paddings from the query\n",
    "    query_embedding = np.array([emb for emb, token in zip(query_embedding, query_tokens) if token != '[PAD]'])\n",
    "\n",
    "    # List to store cosine similarity scores and corresponding document filenames\n",
    "    # chunk_embeddings = np.array(document[DOCUMENT_EMBEDDING])\n",
    "    chunk_tokens = np.array(document[DOCUMENT_TOKENS])\n",
    "    chunk_embeddings = tokens_to_embeddings(document[DOCUMENT_TOKENS], embedding_model)\n",
    "\n",
    "    # remove the paddings and unknown tokens from the query\n",
    "    chunk_embeddings = np.array(\n",
    "        [emb for emb, token in zip(chunk_embeddings, chunk_tokens) if token not in ['[PAD]', '[UNK]']])\n",
    "\n",
    "    # Calculate cosine similarity between query_embedding and chunk_embeddings METHOD = 'MEAN_MAX'\n",
    "    if METHOD == 'MEAN_MAX':\n",
    "        similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "        similarity = np.mean(np.max(similarity, axis=1))\n",
    "    elif METHOD == 'MEAN_MEAN':\n",
    "        similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "        similarity = np.mean(similarity)\n",
    "    elif METHOD == 'COMBINE_MEAN':  # 'COMBINE_MEAN'\n",
    "        similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                       np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "        similarity = np.mean(similarity)  # Get the single value out of the array\n",
    "    else:\n",
    "        mean_max_similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "        mean_max_similarity = np.mean(np.max(mean_max_similarity, axis=1))\n",
    "        combine_mean_similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                       np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "        combine_mean_similarity = np.mean(combine_mean_similarity) \n",
    "        similarity = .5*mean_max_similarity + .5*combine_mean_similarity\n",
    "\n",
    "    return (similarity, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0943ee3-8a10-4b75-b4c3-06968bc43aa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_candidate_docs(query_data):\n",
    "    \"\"\"\n",
    "    Get similarity score between query embeddings and all document embeddings, sort by score and return top N\n",
    "    :param query_data: (dict) Processed query\n",
    "    :return: [(float, dict)] sorted list of tuples continaing similarity score and data from Mongo\n",
    "    \"\"\"\n",
    "    documents = get_documents_from_mongo()\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit each document for processing concurrently\n",
    "        futures = [executor.submit(get_doc_sim_scores, doc, query_data) for doc in documents]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        sim_scores = [future.result() for future in futures]\n",
    "    sim_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return sim_scores[:TOP_N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ef5aeae-de5b-405e-8711-0824e213ff18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 502\n",
      "Time taken to find top 20 documents: 6.634253978729248 seconds\n"
     ]
    }
   ],
   "source": [
    "# METHOD = 'COMBINE_MEAN'\n",
    "# METHOD = 'MEAN_MAX'\n",
    "METHOD = 'both'\n",
    "\n",
    "# Get the candidate documents, top_n_documents: (similarity_score, document dictionary)\n",
    "start_time = time.time()\n",
    "\n",
    "top_n_documents = get_candidate_docs(query_data)\n",
    "top_n_documents.sort(key=lambda x: x[1]['counter'])\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to find top {TOP_N} documents: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea405201-abb6-48e3-b19c-17468e5503b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signals intelligence - Wikipedia.pdf \t\t\t482 \t 0.7348566965447785\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t494 \t 0.6805135708096614\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t478 \t 0.6712810532022575\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t491 \t 0.63252902506634\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t493 \t 0.6080013898031059\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t498 \t 0.6005332291422623\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t492 \t 0.5663472163544991\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t488 \t 0.5641943121699271\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t486 \t 0.5230971415599588\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t495 \t 0.5179160102216983\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t489 \t 0.5158054953403957\n",
      "Communications satellite - Wikipedia.pdf\t\t226 \t 0.510401767130889\n",
      "Communications satellite - Wikipedia.pdf\t\t222 \t 0.5073603642888664\n",
      "Communications satellite - Wikipedia.pdf\t\t221 \t 0.505918751209149\n",
      "Reconnaissance satellite - Wikipedia.pdf\t\t10 \t 0.5033843736526733\n",
      "Communications satellite - Wikipedia.pdf\t\t227 \t 0.49953250051195286\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t487 \t 0.4975293625032031\n",
      "Yaogan - Wikipedia.pdf\t\t\t\t\t380 \t 0.48995022705364377\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t483 \t 0.4861989611529903\n",
      "Signals intelligence - Wikipedia.pdf \t\t\t496 \t 0.4703175824140914\n"
     ]
    }
   ],
   "source": [
    "for sim, doc in top_n_documents:\n",
    "    if len(doc['Document']) < 23:\n",
    "        print(f\"{doc['Document']}\\t\\t\\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 30:\n",
    "        print(f\"{doc['Document']} \\t\\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 36:\n",
    "        print(f\"{doc['Document']} \\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 42:\n",
    "        print(f\"{doc['Document']}\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 46:\n",
    "        print(f\"{doc['Document']}\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    else:\n",
    "        print(f\"{doc['Document']}\\t{doc['counter']} \\t {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "72138caf-5dd2-433a-98a7-4f1b2abc60f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Counter</th>\n",
       "      <th>Simularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>480</td>\n",
       "      <td>0.803169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>496</td>\n",
       "      <td>0.803169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>495</td>\n",
       "      <td>0.781953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>479</td>\n",
       "      <td>0.767552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>486</td>\n",
       "      <td>0.767552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>489</td>\n",
       "      <td>0.767552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>491</td>\n",
       "      <td>0.767552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>492</td>\n",
       "      <td>0.767552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>497</td>\n",
       "      <td>0.765760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Signals intelligence - Wikipedia.pdf</td>\n",
       "      <td>498</td>\n",
       "      <td>0.765760</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Document  Counter  Simularity\n",
       "0  Signals intelligence - Wikipedia.pdf      480    0.803169\n",
       "1  Signals intelligence - Wikipedia.pdf      496    0.803169\n",
       "2  Signals intelligence - Wikipedia.pdf      495    0.781953\n",
       "3  Signals intelligence - Wikipedia.pdf      479    0.767552\n",
       "4  Signals intelligence - Wikipedia.pdf      486    0.767552\n",
       "5  Signals intelligence - Wikipedia.pdf      489    0.767552\n",
       "6  Signals intelligence - Wikipedia.pdf      491    0.767552\n",
       "7  Signals intelligence - Wikipedia.pdf      492    0.767552\n",
       "8  Signals intelligence - Wikipedia.pdf      497    0.765760\n",
       "9  Signals intelligence - Wikipedia.pdf      498    0.765760"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "disp_dict = {\n",
    "    \"Document\":[],\n",
    "    \"Counter\":[],  \n",
    "    \"Simularity\":[], \n",
    "}\n",
    "for sim, doc in top_n_documents:\n",
    "    disp_dict[\"Document\"].append(doc['Document'])\n",
    "    disp_dict[\"Counter\"].append(doc['counter'])\n",
    "    disp_dict[\"Simularity\"].append(sim)\n",
    "    \n",
    "disp_df = pd.DataFrame(disp_dict)\n",
    "# disp_df[disp_df['Counter']==39]\n",
    "# disp_df[disp_df['Document']==\"Starlink - Wikipedia.pdf\"]\n",
    "disp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c754ab1-d976-46cf-9f8b-f3f5c6d1f4c7",
   "metadata": {},
   "source": [
    "#### Get specific chunk from Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c8451fb-3aa1-47bb-90cc-559c923e71a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_view(TYPE, query_info, chunk, counter):\n",
    "    print(f\"Inspecting {TYPE} METHOD...\")\n",
    "    chunk_embeddings = chunk['token_embeddings_less_sw']\n",
    "    chunk_tokens = chunk['tokens_less_sw']\n",
    "    query_embedding = query_info[\"query_embedding_search_less_sw\"]\n",
    "    query_tokens = query_info[\"tokenized_query_search_less_sw\"]\n",
    "    \n",
    "    if TYPE == \"MEAN_MAX\":\n",
    "        print(\"Finding the most simlar words in the chunk for each query word...\\n\")\n",
    "\n",
    "        sim = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "\n",
    "        print(\"Position\\tQuery\\t\\t\\tChunk\\t\\tsim_score\")\n",
    "        for i, s, qt in zip(np.argmax(sim, axis=1), np.max(sim, axis=1), query_tokens):\n",
    "            print(i)\n",
    "            if len(chunk_tokens[i]) < 7 and len(qt) < 8:\n",
    "                print(f\"     {i}) \\t{qt}\\t\\t-->\\t{chunk_tokens[i]} \\t\\t{s}\")\n",
    "            elif len(chunk_tokens[i]) < 6:\n",
    "                print(f\"     {i}) \\t{qt}\\t-->\\t{chunk_tokens[i]} \\t\\t{s}\")\n",
    "            elif len(chunk_tokens[i]) >= 7 and len(qt) < 8:\n",
    "                print(f\"     {i}) \\t{qt}\\t\\t-->\\t{chunk_tokens[i]} \\t{s}\")\n",
    "            else:\n",
    "                print(f\"     {i}) \\t{qt}\\t-->\\t{chunk_tokens[i]} \\t{s}\")\n",
    "\n",
    "        print(f\"\\nnp.mean(np.max(sim, axis=1))\\tsimilarity score between query and {counter} is {np.mean(np.max(sim, axis=1))}\")\n",
    "        print(f\"\\nnp.mean(sim)\\t\\t\\tsimilarity score between query and {counter} is {np.mean(sim)}\")\n",
    "    \n",
    "    elif TYPE == \"COMBINE_MEAN\":\n",
    "        similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                       np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "        similarity = np.mean(similarity) # Get the single value out of the array\n",
    "        \n",
    "        print(f\"\\nThe average query embedding and average {counter} embedding is {similarity}\")\n",
    "    \n",
    "    elif TYPE == \"MEAN_MEAN\":\n",
    "        sim = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "        print(f\"\\nnp.mean(sim) similarity score between query and {filename} is {np.mean(sim)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"TYPE {TYPE} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84c4d908-e211-42c8-85dc-6f4cdb372acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġhappened', 'Ġsignals', 'Ġintelligence', 'Ġworld', 'Ġwar', 'Ġ1']\n"
     ]
    }
   ],
   "source": [
    "query_data[\"query\"]\n",
    "print(query_data['tokenized_query_search_less_sw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d8a058ac-84bc-4a98-af56-b717363cde52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'input_ids_query', 'attention_mask_query', 'query_search', 'tokenized_query', 'tokenized_query_search', 'tokenized_query_search_less_sw', 'query_embedding', 'query_embedding_search', 'query_embedding_search_less_sw'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5dbd0e86-b2dc-491d-a11c-8d6e51cd880b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mongo_db.connect():\n",
    "    # cursor = mongo_db.get_collection().find({\"Document\": \"Starlink - Wikipedia.pdf\"})\n",
    "    cursor = mongo_db.get_collection().find({\"counter\": 478})\n",
    "    \n",
    "    mongo_data = list(cursor)\n",
    "    mongo_db.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22882546-a8dd-444e-907d-252aa5ef31a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "dict_keys(['_id', 'tokens', 'tokens_less_sw', 'token_embeddings_less_sw', 'Document', 'sha_256', 'counter'])\n",
      "['Ġsignals', 'Ġintelligence', 'Ġwik', 'ipedia', 'Ġr', 'af', 'Ġmen', 'Ġhill', 'Ġlarge', 'Ġsite', 'Ġunited', 'Ġkingdom', 'Ġpart', 'Ġe', 'che', 'lon', 'Ġu', 'k', 'usa', 'Ġagreement', 'Ġg', 'erman', 'Ġmessage', 'Ġintercepted', 'Ġb', 'rit', 'ish', 'Ġworld', 'Ġwar', 'Ġsignaling', 'Ġg', 'erman', \"'s\", 'Ġunconditional', 'Ġsurrender', 'Ġsignals', 'Ġintelligence', 'red', 'irect', 'ed', 'Ġsig', 'int', 'Ġsignals', 'Ġintelligence', 'ig', 'int', 'Ġinterception', 'Ġsignals', 'Ġwhether', 'Ġcommunications', 'Ġpeople', 'communications', 'int', 'Ġelectronic', 'Ġsignals', 'Ġdirectly', 'Ġused', 'Ġcommunication', 'elect', 'ronic', 'Ġel', 'int', ').[', 'Ġsignals', 'Ġintelligence', 'Ġsubset', 'Ġintelligence', 'Ġcollection', 'Ġmanagement', 'Ġclassified', 'Ġsensitive', 'Ġinformation', 'Ġusually', 'Ġencrypted', 'Ġsignals', 'Ġintelligence', 'Ġturn', 'Ġinvolves', 'Ġuse', 'Ġcrypt', 'analysis', 'Ġdecipher', 'Ġmessages', 'Ġtraffic', 'Ġanalysis', 'âĢĶ', 'Ġstudy', 'Ġsignaling', 'Ġquantity', 'âĢĶ', 'Ġalso', 'Ġused', 'Ġintegrate', 'Ġinformation', 'Ġelectronic', 'Ġinterceptions', 'Ġappeared', 'Ġearly', 'Ġbo', 'er', 'Ġwar', 'âĢĵ', '02', 'Ġb', 'rit', 'ish', 'Ġroyal', 'Ġnavy', 'Ġinstalled', 'Ġwireless', 'Ġsets', 'Ġproduced', 'Ġmar', 'coni', 'Ġboard', 'Ġships', 'Ġlate', 'Ġb', 'rit', 'ish', 'Ġarmy', 'Ġused', 'Ġlimited', 'Ġwireless', 'Ġsignalling', 'Ġbo', 'ers', 'Ġcaptured', 'Ġwireless', 'Ġsets', 'Ġused', 'Ġmake', 'Ġvital', 'Ġtransmissions', 'Ġsince', 'Ġb', 'rit', 'ish', 'Ġpeople', 'Ġtransmitting', 'Ġtime', 'Ġspecial', 'Ġinterpretation', 'Ġsignals', 'Ġintercepted', 'Ġb', 'rit', 'ish', 'Ġnecessary', 'Ġbirth', 'Ġsignals', 'Ġintelligence', 'Ġmodern', 'Ġsense', 'Ġdates', 'Ġr', 'us', 'j', 'apan', 'ese', 'Ġwar', 'âĢĵ', '05', 'Ġr', 'ussian', 'Ġfleet', 'Ġprepared', 'Ġconflict', 'Ġj', 'apan', 'Ġb', 'rit', 'ish', 'Ġship', 'iana', 'Ġstationed', 'Ġcanal', 'Ġintercepted', 'Ġr', 'ussian', 'Ġnaval', 'Ġwireless', 'Ġsignals', 'Ġsent', 'Ġmobilization', 'Ġfleet', 'Ġfirst', 'Ġtime', 'Ġhistory', 'Ġcourse', 'Ġfirst', 'Ġworld', 'Ġwar', 'Ġnew', 'Ġmethod', 'Ġsignals', 'Ġintelligence', 'Ġreached', 'Ġmaturity', 'Ġfailure', 'Ġproperly', 'Ġprotect', 'Ġcommunications', 'Ġfatally', 'Ġcompromised', 'Ġr', 'ussian', 'Ġarmy', 'Ġadvance', 'Ġearly', 'Ġworld', 'Ġwar', 'Ġled', 'Ġdisastrous', 'Ġdefeat', 'Ġgerm', 'ans', 'Ġl', 'ud', 'endor', 'ff', 'Ġhind', 'enburg', 'Ġbattle', 'ann', 'enberg', 'Ġfrench', 'Ġintercept', 'Ġpersonnel', 'Ġcaptured', 'Ġmessage', 'Ġwritten', 'Ġnew', 'Ġcipher', 'Ġcrypt', 'analy', 'zed', 'Ġge', 'org', 'es', 'Ġpain', 'vin', 'Ġgave', 'Ġallies', 'Ġadvance', 'Ġwarning', 'Ġg', 'erman']\n"
     ]
    }
   ],
   "source": [
    "print(len(mongo_data))\n",
    "print(mongo_data[0].keys())\n",
    "print(mongo_data[0]['tokens_less_sw'])\n",
    "# mongo_data[0]['token_embeddings_less_sw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e94a889-28b1-42c4-a1fd-6aecfa4e7802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting MEAN_MAX METHOD...\n",
      "Finding the most simlar words in the chunk for each query word...\n",
      "\n",
      "Position\tQuery\t\t\tChunk\t\tsim_score\n",
      "205\n",
      "     205) \tĠhappened\t-->\tmber \t\t0.8190155052386583\n",
      "142\n",
      "     142) \tĠsignals\t-->\tĠsignals \t0.9999999999999999\n",
      "143\n",
      "     143) \tĠintelligence\t-->\tĠintelligence \t0.9999999999999997\n",
      "153\n",
      "     153) \tĠworld\t\t-->\tĠworld \t\t0.9999999999999997\n",
      "43\n",
      "     43) \tĠwar\t\t-->\tĠwar \t\t0.9999999999999999\n",
      "0\n",
      "     0) \tĠ1\t\t-->\tĠsmall \t\t0.0\n",
      "\n",
      "np.mean(np.max(sim, axis=1))\tsimilarity score between query and 480 is 0.8031692508731095\n",
      "\n",
      "np.mean(sim)\t\t\tsimilarity score between query and 480 is 0.017971111829255957\n"
     ]
    }
   ],
   "source": [
    "print_view(METHOD, query_data, mongo_data[0], mongo_data[0]['counter'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a5eaea-035f-4a60-a0b7-1e60cc3a3c0a",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39ad2aa4-a303-404e-87fa-a9e926ec5999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_embeddings = mongo_data[0]['token_embeddings_less_sw']\n",
    "chunk_tokens = mongo_data[0]['tokens_less_sw']\n",
    "query_embedding = query_data[\"query_embedding_search_less_sw\"]\n",
    "query_tokens = query_data[\"tokenized_query_search_less_sw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c57f96a8-3f91-4171-b397-f7e07a8c4d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġstar'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0fc06f51-7fb3-49f6-9093-91dbfe59aa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġstar'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data[\"tokenized_query_search_less_sw\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dda51cc1-713a-473b-a57b-ed8df0e78f83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1263670027256012, -0.18757300078868866, 0.3148829936981201, 0.6943079829216003, -1.6521389484405518, 0.7149810194969177, 1.2589759826660156, -0.9046199917793274, -1.0553289651870728, 0.6717360019683838, 0.5311689972877502, 0.3487280011177063, 0.3001149892807007, 0.967756986618042, -0.8603249788284302, -0.4125550091266632, -0.7426400184631348, 0.726872980594635, 0.4211600124835968, -1.3967779874801636, -0.3427030146121979, 0.16761000454425812, -0.2244739979505539, 0.9043030142784119, 1.0735490322113037, 0.09345600008964539, -0.20636099576950073, 0.8705009818077087, 0.3816690146923065, 0.6370450258255005, 0.009087000042200089, -0.2014629989862442, -0.7317540049552917, -0.8246780037879944, 1.1714550256729126, -0.5089390277862549, 0.24671700596809387, 0.4250909984111786, 0.38922399282455444, -0.5507810115814209, 0.5331500172615051, -0.3759540021419525, 0.6035339832305908, -0.08156999945640564, 0.05593999847769737, 0.06643900275230408, 0.18744899332523346, 0.2751159965991974, 0.0687360018491745, 0.4534359872341156]\n"
     ]
    }
   ],
   "source": [
    "print(chunk_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "69d9f1e7-711f-4420-a6be-a5a4fe8263b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1263670027256012, -0.18757300078868866, 0.3148829936981201, 0.6943079829216003, -1.6521389484405518, 0.7149810194969177, 1.2589759826660156, -0.9046199917793274, -1.0553289651870728, 0.6717360019683838, 0.5311689972877502, 0.3487280011177063, 0.3001149892807007, 0.967756986618042, -0.8603249788284302, -0.4125550091266632, -0.7426400184631348, 0.726872980594635, 0.4211600124835968, -1.3967779874801636, -0.3427030146121979, 0.16761000454425812, -0.2244739979505539, 0.9043030142784119, 1.0735490322113037, 0.09345600008964539, -0.20636099576950073, 0.8705009818077087, 0.3816690146923065, 0.6370450258255005, 0.009087000042200089, -0.2014629989862442, -0.7317540049552917, -0.8246780037879944, 1.1714550256729126, -0.5089390277862549, 0.24671700596809387, 0.4250909984111786, 0.38922399282455444, -0.5507810115814209, 0.5331500172615051, -0.3759540021419525, 0.6035339832305908, -0.08156999945640564, 0.05593999847769737, 0.06643900275230408, 0.18744899332523346, 0.2751159965991974, 0.0687360018491745, 0.4534359872341156]\n"
     ]
    }
   ],
   "source": [
    "print(query_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb0b0888-1561-4ba4-8f61-eab59e9c0654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [ 0.036363   -0.393042   -0.14290901 -0.46448901  0.761226    0.76178199\n",
      " -0.35284099  1.00311196  1.06885898 -0.42710701 -0.724011   -0.37086099\n",
      "  0.25220501  1.56391394 -0.005681   -1.03142202 -1.27680898  0.82345903\n",
      " -1.13170898 -0.506347   -0.380328   -0.63763601 -0.649562   -0.353872\n",
      " -0.016324    0.44753599  0.79029101 -0.99025601  0.35526401  0.51425499\n",
      "  0.206158    0.746557   -0.257566   -0.95098799  0.57719302  0.76185697\n",
      " -0.21814799  1.15517604 -0.113426   -0.96824402 -0.25894901  0.52897602\n",
      "  0.166455   -0.47958601 -0.148857    0.020318   -0.91852498 -0.195447\n",
      "  0.191122    1.20281506]\n",
      "B: [ 0.027065    0.117158   -0.149758    0.095494    0.78823799  0.044426\n",
      "  0.21710899 -1.09294701  0.17413101 -0.54461998 -1.32539797  0.66004902\n",
      "  0.88568503  0.35447499 -0.79060501  0.30542299  0.43905699 -1.33964705\n",
      "  0.074849   -0.38930199 -1.14452004  0.169121   -0.13893899  0.053343\n",
      " -0.076741   -1.16213596 -0.890517   -0.112144   -0.54387599  0.45776999\n",
      " -0.91843998 -0.038122    0.054959    0.59223998 -0.049424    0.29127401\n",
      "  0.96805203  0.225623   -0.327277    1.67805195  0.25254899  0.362665\n",
      " -0.009165   -0.58305901 -0.67017698  0.141753    0.202885    1.53154504\n",
      " -0.88028699 -0.52501601]\n",
      "Cosine Similarity: -0.18411191267012425\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    "# define two lists or array\n",
    "A = np.array(chunk_embeddings[0])\n",
    "B = np.array(query_embedding[0])\n",
    " \n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    " \n",
    "# compute cosine similarity\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17fdb7-91c8-4bef-b0ad-306c16917b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "239054cc-ed7d-4331-b71c-41d07bc87551",
   "metadata": {},
   "source": [
    "#### Get the Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dcce19-43df-4d34-801c-5e8bf87ef82f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
