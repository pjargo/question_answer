{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d7b836-2be2-4d93-8e56-646b1965cb02",
   "metadata": {},
   "source": [
    "### Notebook: Write query and get top N similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90027071-a4ef-4f1f-94b5-8b1435ada488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterargo/anaconda3/envs/question_answer/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the current working directory (notebooks directory)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project directory\n",
    "project_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Assuming your project structure is as described before\n",
    "src_path = os.path.abspath(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from question_answer_site.question_answer.mongodb import MongoDb\n",
    "from question_answer_site.question_answer.utils import remove_non_word_chars, clean_text, tokens_to_embeddings, post_process_output, correct_spelling\n",
    "from question_answer_site.question_answer.config import TOKENIZER, EMBEDDING_MODEL_FNAME, EMBEDDING_MODEL_TYPE, TOKENS_EMBEDDINGS, DOCUMENT_EMBEDDING, \\\n",
    "    DOCUMENT_TOKENS, TOP_N, TRANSFORMER_MODEL_NAME, METHOD, MAX_QUERY_LENGTH, username, password, cluster_url, \\\n",
    "    database_name\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "from urllib.parse import quote_plus\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "375ba3c1-0bc1-4ec8-8eba-8493924d3d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_user_1 password33566 cluster0 question_answer\n"
     ]
    }
   ],
   "source": [
    "query = \"what does the starlink program do?\"\n",
    "print(username, password, cluster_url, database_name)\n",
    "escaped_username = quote_plus(username)\n",
    "escaped_password = quote_plus(password)\n",
    "\n",
    "# use MongoDb class to connect to database instance and get the documents\n",
    "mongo_db = MongoDb(escaped_username, escaped_password, cluster_url, database_name, \"parsed_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea362d42-09ed-4b70-9c9f-a70468d0c709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "536\n"
     ]
    }
   ],
   "source": [
    "if mongo_db.connect():\n",
    "    print(mongo_db.count_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "322a8f7a-7582-4135-bf91-aacacf5b6e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space=True)\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "tokenizer = tokenizers[TOKENIZER]\n",
    "\n",
    "# Load your trained Word2Vec model\n",
    "if EMBEDDING_MODEL_TYPE == 'Word2Vec':\n",
    "    model = Word2Vec.load(\n",
    "        os.path.join(os.getcwd(), \"question_answer\", \"embedding_models\", EMBEDDING_MODEL_FNAME))\n",
    "elif EMBEDDING_MODEL_TYPE.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    model = spacy.load(os.path.join(\"..\",\"src\",\"question_answer_site\", \"question_answer\", \"embedding_models\",\n",
    "                                    EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))\n",
    "\n",
    "# Specify Candidate token embeddings option\n",
    "if TOKENS_EMBEDDINGS == \"query\":\n",
    "    TOKENS = \"tokenized_query\"\n",
    "    EMBEDDINGS = \"query_embedding\"\n",
    "elif TOKENS_EMBEDDINGS == \"query_search\":\n",
    "    TOKENS = \"tokenized_query_search\"\n",
    "    EMBEDDINGS = \"query_embedding_search\"\n",
    "else:\n",
    "    TOKENS = \"tokenized_query_search_less_sw\"\n",
    "    EMBEDDINGS = \"query_embedding_search_less_sw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6636b8f8-03fc-43a4-8847-3dfb9898eae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spell_check(user_query):\n",
    "    tokenized_query = tokenizer.tokenize(user_query)\n",
    "\n",
    "    # Group tokens into words\n",
    "    words = []\n",
    "    current_word = \"\"\n",
    "    for token in tokenized_query:\n",
    "        if token.startswith(\"Ġ\"):  # Indicates the start of a new word\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "            current_word = token[1:] if token[1:] not in ['(', '[', '{', '/', '\\\\'] else ''\n",
    "        else:\n",
    "            current_word += token if token not in [')', ']', '}', '/', '\\\\', '?', \".\", \"!\"] else ''\n",
    "            if token in ['/', '\\\\']:\n",
    "                words.append(current_word)\n",
    "                current_word = ''\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "\n",
    "    # Identify misspelled words not in the embeddings model\n",
    "    misspelled_words = []\n",
    "    for word in words:\n",
    "        # Split punctuation and hyphens from the word\n",
    "        base_word = \"\".join(char for char in word if char.isalnum() or char in [\"'\", \"-\"])\n",
    "        if any(list(map(lambda x: not any(x),\n",
    "                        tokens_to_embeddings(tokenizer.tokenize(base_word), model, RANDOM=False)))):\n",
    "            # Add the original word to the misspelled_words list\n",
    "            misspelled_words.append(word)\n",
    "    # Correct the spelling of misspelled words\n",
    "    corrected_words = {word: correct_spelling(word) for word in misspelled_words}\n",
    "\n",
    "    # Replace misspelled words in the original query\n",
    "    corrected_query = user_query\n",
    "    for original, corrected in corrected_words.items():\n",
    "        corrected_query = corrected_query.replace(original, corrected)\n",
    "\n",
    "    return corrected_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98de6a3-1442-4c27-af8a-2bec729cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_query):\n",
    "    user_query = user_query.lower()\n",
    "\n",
    "    # clean query for BERT input\n",
    "    user_query = clean_text(user_query)\n",
    "    print(\"Uncorrected query: \", user_query)\n",
    "    user_query = spell_check(user_query)\n",
    "    print(\"Corrected query: \", user_query)\n",
    "\n",
    "    # clean query for candidate search\n",
    "    user_query_for_search = remove_non_word_chars(user_query)\n",
    "\n",
    "    # Tokenize the query for BERT input\n",
    "    tokenized_query = tokenizer.tokenize(user_query)\n",
    "\n",
    "    # Tokenize the query for candidate search\n",
    "    tokenized_query_for_search = tokenizer.tokenize(user_query_for_search)\n",
    "\n",
    "    # Remove the stop words for the tokenized query for search\n",
    "    nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    nltk_stop_words.extend([\"Ġ\" + word for word in nltk_stop_words])  # Add the roberta modified tokens\n",
    "    tokenized_query_for_search_less_sw = [token for token in tokenized_query_for_search if\n",
    "                                          token not in nltk_stop_words]\n",
    "\n",
    "    # Pad or truncate the query to a fixed length of 20 tokens (BERT input)\n",
    "\n",
    "    if len(tokenized_query) > MAX_QUERY_LENGTH:\n",
    "        tokenized_query = tokenized_query[:MAX_QUERY_LENGTH]\n",
    "    else:\n",
    "        padding_length = MAX_QUERY_LENGTH - len(tokenized_query)\n",
    "        tokenized_query = tokenized_query + [tokenizer.pad_token] * padding_length\n",
    "\n",
    "    # Convert the tokenized query to input IDs and attention mask\n",
    "    input_ids_query = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
    "    attention_mask_query = [1] * len(input_ids_query)\n",
    "\n",
    "    # Convert to tensors\n",
    "    input_ids_query = torch.tensor(input_ids_query).unsqueeze(0)  # Add batch dimension\n",
    "    attention_mask_query = torch.tensor(attention_mask_query).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Get the query embeddings for the candidate document search\n",
    "    query_embeddings = tokens_to_embeddings(tokenized_query, model, RANDOM=False)\n",
    "    query_embeddings_search = tokens_to_embeddings(tokenized_query_for_search, model, RANDOM=False)\n",
    "    query_embeddings_less_sw = tokens_to_embeddings(tokenized_query_for_search_less_sw, model, RANDOM=False)\n",
    "\n",
    "    query_data = {\n",
    "        \"query\": user_query,\n",
    "        \"input_ids_query\": input_ids_query.tolist(),\n",
    "        \"attention_mask_query\": attention_mask_query.tolist(),\n",
    "        \"query_search\": user_query_for_search,\n",
    "        \"tokenized_query\": tokenized_query,\n",
    "        \"tokenized_query_search\": tokenized_query_for_search,\n",
    "        \"tokenized_query_search_less_sw\": tokenized_query_for_search_less_sw,\n",
    "        \"query_embedding\": query_embeddings, #.tolist(),  # Just used for the candidate search\n",
    "        \"query_embedding_search\": query_embeddings_search, #.tolist(),  # Just used for the candidate search, cleaned\n",
    "        \"query_embedding_search_less_sw\": query_embeddings_less_sw # .tolist()\n",
    "        # Just used for the candidate search, cleaned more\n",
    "    }\n",
    "    # return json.dumps(query_data['query'], indent=2)\n",
    "    return query_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e1f434-0b64-40a2-b72a-8f7dbf9e69aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrected query:  what does the starlink program do?\n",
      "Corrected query:  what does the starlink program do?\n",
      "['Ġwhat', 'Ġdoes', 'Ġthe', 'Ġstar', 'link', 'Ġprogram', 'Ġdo', '?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "query_data = process_query(user_query=query)\n",
    "print(query_data[ \"tokenized_query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c421d341-f501-48e0-8f7d-964c15fbe23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_documents_from_mongo():\n",
    "    if mongo_db.connect():\n",
    "        documents = [document for document in mongo_db.iterate_documents()]\n",
    "        print(f\"Total documents: {mongo_db.count_documents()}\")\n",
    "        mongo_db.disconnect()\n",
    "        return documents\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b355dd1-b2af-4bf0-9b65-579571e1a1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# METHOD = 'COMBINE_MEAN'\n",
    "METHOD = 'MEAN_MAX'\n",
    "def get_topn_docs(documents_list, query_data):\n",
    "    query_embedding = np.array(query_data[EMBEDDINGS])\n",
    "    query_tokens = np.array(query_data[TOKENS])\n",
    "\n",
    "    # remove the paddings from the query\n",
    "    query_embedding = np.array([emb for emb, token in zip(query_embedding, query_tokens) if token != '[PAD]'])\n",
    "\n",
    "    # List to store cosine similarity scores and corresponding document filenames\n",
    "    similarity_scores = []\n",
    "\n",
    "    for doc in documents_list:\n",
    "        chunk_embeddings = np.array(doc[DOCUMENT_EMBEDDING])\n",
    "        chunk_tokens = np.array(doc[DOCUMENT_TOKENS])\n",
    "\n",
    "        # remove the paddings and unknown tokens from the query\n",
    "        chunk_embeddings = np.array(\n",
    "            [emb for emb, token in zip(chunk_embeddings, chunk_tokens) if token not in ['[PAD]', '[UNK]']])\n",
    "\n",
    "        # Calculate cosine similarity between query_embedding and chunk_embeddings METHOD = 'MEAN_MAX'\n",
    "        if METHOD == 'MEAN_MAX':\n",
    "            similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "            similarity = np.mean(np.max(similarity, axis=1))\n",
    "\n",
    "        elif METHOD == 'MEAN_MEAN':\n",
    "            similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "            similarity = np.mean(similarity)\n",
    "\n",
    "        # if METHOD == 'COMBINE_MEAN':\n",
    "        else:\n",
    "            similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                           np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "            similarity = np.mean(similarity)  # Get the single value out of the array\n",
    "\n",
    "        # Store similarity score and filename\n",
    "        similarity_scores.append((similarity, doc))\n",
    "\n",
    "    # Sort the similarity_scores in descending order based on the similarity score\n",
    "    if similarity_scores:\n",
    "        similarity_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        # for confidence, parsed_doc_chunk_dict in similarity_scores[:TOP_N]:\n",
    "        #     print(parsed_doc_chunk_dict['counter'])\n",
    "        #     print(self.tokenizer.convert_tokens_to_string(parsed_doc_chunk_dict['tokens']))\n",
    "        #     print(parsed_doc_chunk_dict['Document'])\n",
    "        #     print(confidence)\n",
    "        #     print()\n",
    "        return similarity_scores[:TOP_N]\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3392b2a7-b063-42cf-93ad-4b710458af52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_candidate_docs(query_data):\n",
    "    documents = get_documents_from_mongo()\n",
    "    top_n_documents = get_topn_docs(documents_list=documents,\n",
    "                                         query_data=query_data)\n",
    "\n",
    "    return top_n_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ef5aeae-de5b-405e-8711-0824e213ff18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 536\n",
      "Time taken to find top 10 documents: 7.174426794052124 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the candidate documents, top_n_documents: (similarity_score, document dictionary)\n",
    "start_time = time.time()\n",
    "\n",
    "top_n_documents = get_candidate_docs(query_data)\n",
    "top_n_documents.sort(key=lambda x: x[1]['counter'])\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to find top {TOP_N} documents: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea405201-abb6-48e3-b19c-17468e5503b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Extremely High Frequency - Wikipedia.pdf\t40 \t 0.43302890508182323\n",
      "Rocket Lab Electron - Wikipedia.pdf \t\t\t92 \t 0.41054024062193734\n",
      "Kepler-11 - Wikipedia.pdf \t\t\t\t213 \t 0.41192279830281087\n",
      "Space Based Space Surveillance - Wikipedia.pdf\t\t296 \t 0.4049510918797441\n",
      "James Webb Space Telescope - Wikipedia.pdf\t\t327 \t 0.4071029292797257\n",
      "James Webb Space Telescope - Wikipedia.pdf\t\t353 \t 0.429566456857389\n",
      "James Webb Space Telescope - Wikipedia.pdf\t\t357 \t 0.43302890508182323\n",
      "James Webb Space Telescope - Wikipedia.pdf\t\t360 \t 0.4049510918797441\n",
      "James Webb Space Telescope - Wikipedia.pdf\t\t362 \t 0.4071029292797257\n",
      "Kosmos 2251 - Wikipedia.pdf \t\t\t\t530 \t 0.4305808634441024\n"
     ]
    }
   ],
   "source": [
    "for sim, doc in top_n_documents:\n",
    "    if len(doc['Document']) < 23:\n",
    "        print(f\"{doc['Document']}\\t\\t\\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 30:\n",
    "        print(f\"{doc['Document']} \\t\\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 36:\n",
    "        print(f\"{doc['Document']} \\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 42:\n",
    "        print(f\"{doc['Document']}\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 46:\n",
    "        print(f\"{doc['Document']}\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    else:\n",
    "        print(f\"{doc['Document']}\\t{doc['counter']} \\t {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "72138caf-5dd2-433a-98a7-4f1b2abc60f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Counter</th>\n",
       "      <th>Simularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reconnaissance satellite - Wikipedia.pdf</td>\n",
       "      <td>11</td>\n",
       "      <td>0.645220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wideband Global SATCOM - Wikipedia.pdf</td>\n",
       "      <td>14</td>\n",
       "      <td>0.675101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advanced Extremely High Frequency - Wikipedia.pdf</td>\n",
       "      <td>35</td>\n",
       "      <td>0.702840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Advanced Extremely High Frequency - Wikipedia.pdf</td>\n",
       "      <td>37</td>\n",
       "      <td>0.664623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Advanced Extremely High Frequency - Wikipedia.pdf</td>\n",
       "      <td>38</td>\n",
       "      <td>0.646915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Falcon 9 - Wikipedia.pdf</td>\n",
       "      <td>58</td>\n",
       "      <td>0.678308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Boeing X-37 - Wikipedia.pdf</td>\n",
       "      <td>165</td>\n",
       "      <td>0.636067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Boeing X-37 - Wikipedia.pdf</td>\n",
       "      <td>173</td>\n",
       "      <td>0.636681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Communications satellite - Wikipedia.pdf</td>\n",
       "      <td>223</td>\n",
       "      <td>0.654481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Falcon Heavy - Wikipedia.pdf</td>\n",
       "      <td>260</td>\n",
       "      <td>0.655848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Counter  Simularity\n",
       "0           Reconnaissance satellite - Wikipedia.pdf       11    0.645220\n",
       "1             Wideband Global SATCOM - Wikipedia.pdf       14    0.675101\n",
       "2  Advanced Extremely High Frequency - Wikipedia.pdf       35    0.702840\n",
       "3  Advanced Extremely High Frequency - Wikipedia.pdf       37    0.664623\n",
       "4  Advanced Extremely High Frequency - Wikipedia.pdf       38    0.646915\n",
       "5                           Falcon 9 - Wikipedia.pdf       58    0.678308\n",
       "6                        Boeing X-37 - Wikipedia.pdf      165    0.636067\n",
       "7                        Boeing X-37 - Wikipedia.pdf      173    0.636681\n",
       "8           Communications satellite - Wikipedia.pdf      223    0.654481\n",
       "9                       Falcon Heavy - Wikipedia.pdf      260    0.655848"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "disp_dict = {\n",
    "    \"Document\":[],\n",
    "    \"Counter\":[],  \n",
    "    \"Simularity\":[], \n",
    "}\n",
    "for sim, doc in top_n_documents:\n",
    "    disp_dict[\"Document\"].append(doc['Document'])\n",
    "    disp_dict[\"Counter\"].append(doc['counter'])\n",
    "    disp_dict[\"Simularity\"].append(sim)\n",
    "    \n",
    "disp_df = pd.DataFrame(disp_dict)\n",
    "# disp_df[disp_df['Counter']==39]\n",
    "# disp_df[disp_df['Document']==\"Starlink - Wikipedia.pdf\"]\n",
    "disp_df\n",
    "\n",
    "# for sim, doc in top_n_documents:\n",
    "#     if len(doc['Document']) < 23:\n",
    "#         print(f\"{doc['Document']}\\t\\t\\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "#     elif len(doc['Document']) <= 30:\n",
    "#         print(f\"{doc['Document']} \\t\\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "#     elif len(doc['Document']) <= 36:\n",
    "#         print(f\"{doc['Document']} \\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "#     elif len(doc['Document']) <= 42:\n",
    "#         print(f\"{doc['Document']}\\t\\t{doc['counter']} \\t {sim}\")\n",
    "#     else:\n",
    "#         print(f\"{doc['Document']}\\t\\t{doc['counter']} \\t {sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c754ab1-d976-46cf-9f8b-f3f5c6d1f4c7",
   "metadata": {},
   "source": [
    "#### Get specific chunk from Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0c8451fb-3aa1-47bb-90cc-559c923e71a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_view(TYPE, query_info, chunk, counter):\n",
    "    print(f\"Inspecting {TYPE} METHOD...\")\n",
    "    chunk_embeddings = chunk['token_embeddings_less_sw']\n",
    "    chunk_tokens = chunk['tokens_less_sw']\n",
    "    query_embedding = query_info[\"query_embedding_search_less_sw\"]\n",
    "    query_tokens = query_info[\"tokenized_query_search_less_sw\"]\n",
    "    \n",
    "    if TYPE == \"MEAN_MAX\":\n",
    "        print(\"Finding the most simlar words in the chunk for each query word...\\n\")\n",
    "\n",
    "        sim = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "\n",
    "        print(\"Position\\tQuery\\t\\t\\tChunk\\t\\tsim_score\")\n",
    "        for i, s, qt in zip(np.argmax(sim, axis=1), np.max(sim, axis=1), query_tokens):\n",
    "            print(i)\n",
    "            if len(chunk_tokens[i]) < 7 and len(qt) < 8:\n",
    "                print(f\"     {i}) \\t{qt}\\t\\t-->\\t{chunk_tokens[i]} \\t\\t{s}\")\n",
    "            elif len(chunk_tokens[i]) < 6:\n",
    "                print(f\"     {i}) \\t{qt}\\t-->\\t{chunk_tokens[i]} \\t\\t{s}\")\n",
    "            elif len(chunk_tokens[i]) >= 7 and len(qt) < 8:\n",
    "                print(f\"     {i}) \\t{qt}\\t\\t-->\\t{chunk_tokens[i]} \\t{s}\")\n",
    "            else:\n",
    "                print(f\"     {i}) \\t{qt}\\t-->\\t{chunk_tokens[i]} \\t{s}\")\n",
    "\n",
    "        print(f\"\\nnp.mean(np.max(sim, axis=1))\\tsimilarity score between query and {counter} is {np.mean(np.max(sim, axis=1))}\")\n",
    "        print(f\"\\nnp.mean(sim)\\t\\t\\tsimilarity score between query and {counter} is {np.mean(sim)}\")\n",
    "    \n",
    "    elif TYPE == \"COMBINE_MEAN\":\n",
    "        similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                       np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "        similarity = np.mean(similarity) # Get the single value out of the array\n",
    "        \n",
    "        print(f\"\\nThe average query embedding and average {counter} embedding is {similarity}\")\n",
    "    \n",
    "    elif TYPE == \"MEAN_MEAN\":\n",
    "        sim = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "        print(f\"\\nnp.mean(sim) similarity score between query and {filename} is {np.mean(sim)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"TYPE {TYPE} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "84c4d908-e211-42c8-85dc-6f4cdb372acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġdesign', 'Ġx', '37', 'Ġorbital', 'Ġtest', 'Ġvehicle']\n"
     ]
    }
   ],
   "source": [
    "query_data[\"query\"]\n",
    "print(query_data['tokenized_query_search_less_sw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d8a058ac-84bc-4a98-af56-b717363cde52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'input_ids_query', 'attention_mask_query', 'query_search', 'tokenized_query', 'tokenized_query_search', 'tokenized_query_search_less_sw', 'query_embedding', 'query_embedding_search', 'query_embedding_search_less_sw'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5dbd0e86-b2dc-491d-a11c-8d6e51cd880b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mongo_db.connect():\n",
    "    # cursor = mongo_db.get_collection().find({\"Document\": \"Starlink - Wikipedia.pdf\"})\n",
    "    cursor = mongo_db.get_collection().find({\"counter\": 169})\n",
    "    \n",
    "    mongo_data = list(cursor)\n",
    "    mongo_db.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "22882546-a8dd-444e-907d-252aa5ef31a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "dict_keys(['_id', 'tokens', 'tokens_less_sw', 'token_embeddings_less_sw', 'Document', 'sha_256', 'counter'])\n",
      "['scale', 'Ġderivative', 'Ġbo', 'e', 'ing', 'Ġx', 'Ġmeasuring', 'Ġfeet', 'Ġlength', 'Ġfeatures', 'Ġtwo', 'Ġangled', 'Ġtail', 'Ġfins', 'Ġlaunches', 'Ġatop', 'las', 'Ġspace', 'x', 'Ġfal', 'con', 'Ġrocket', 'Ġspace', 'plane', 'Ġdesigned', 'Ġoperate', 'Ġspeed', 'Ġrange', 'Ġmach', 'entry', 'Ġtechnologies', 'Ġdemonstrated', 'Ġinclude', 'Ġimproved', 'Ġthermal', 'Ġprotection', 'Ġsystem', 'Ġenhanced', 'Ġav', 'ionics', 'Ġautonomous', 'Ġguidance', 'Ġsystem', 'Ġadvanced', 'Ġair', 'frame', 'Ġspace', 'plane', \"'s\", 'Ġthermal', 'Ġprotection', 'Ġsystem', 'Ġbuilt', 'Ġupon', 'Ġprevious', 'Ġgenerations', 'Ġatmospheric', 'entry', 'Ġspacecraft', 'Ġincorporating', 'Ġsil', 'ica', 'Ġceramic', 'Ġtiles', 'Ġav', 'ionics', 'Ġsuite', 'Ġused', 'Ġbo', 'e', 'ing', 'Ġdevelop', 'Ġc', 'st', 'Ġcrew', 'ed', 'Ġspacecraft', 'Ġdevelopment', 'aid', 'Ġdesign', 'Ġdevelopment', 'Ġn', 'asa', \"'s\", 'Ġorbital', 'Ġspace', 'Ġplane', 'Ġdesigned', 'Ġprovide', 'Ġcrew', 'Ġrescue', 'Ġcrew', 'Ġtransport', 'Ġcapability', 'Ġinternational', 'Ġspace', 'Ġstation', '\",', 'Ġaccording', 'Ġn', 'asa', 'Ġfact', 'Ġsheet', 'Ġn', 'asa', 'Ġpowered', 'Ġone', 'ero', 'jet', 'Ġengine', 'Ġusing', 'Ġst', 'orable', 'Ġpropell', 'ants', 'Ġproviding', 'Ġthrust', 'Ġpounds', 'force', 'Ġhuman', 'rated', 'Ġengine', 'Ġused', 'Ġdual', 'power', 'Ġastronaut', 'Ġtraining', 'Ġvehicle', 'Ġgiven', 'Ġnew', 'Ġflight', 'Ġcertification', 'Ġuse', 'Ġhydrogen', 'Ġpropell', 'ants', 'Ġreportedly', 'Ġchanged', 'Ġhyper', 'g', 'olic', 'Ġpropulsion', 'Ġsystem', 'Ġlands', 'Ġautomatically', 'Ġupon', 'Ġreturning', 'Ġorbit', 'Ġthird', 'Ġreusable', 'Ġspacecraft', 'Ġcapability', 'v', 'iet', 'Ġbur', 'Ġshuttle', 'Ġu', 'Ġspace', 'Ġshuttle', 'Ġautomatic', 'Ġlanding', 'Ġcapability', 'Ġmid', 'Ġnever', 'Ġtested', 'Ġsmallest', 'Ġlight', 'est', 'Ġorbital', 'Ġspace', 'plane', 'Ġflown', 'Ġdate', 'Ġlaunch', 'Ġmass', 'Ġaround', '000', 'Ġpounds', '000', 'Ġapproximately', 'Ġone', 'Ġquarter', 'Ġsize', 'Ġspace', 'Ġshuttle', 'Ġorbit', 'er', 'Ġprocessing', 'Ġdesign', 'Ġbo', 'e', 'ing', 'Ġwik', 'ipedia', 'Ġapr', 'il', 'Ġspace', 'Ġfoundation', 'Ġawarded', 'Ġteam', 'Ġspace', 'Ġachievement', 'Ġaward', 'Ġsignificantly', 'Ġadvancing', 'Ġstate', 'Ġart', 'Ġreusable', 'Ġspacecraft', 'orbit', 'Ġoperations', 'Ġdesign', 'Ġdevelopment', 'Ġtest']\n"
     ]
    }
   ],
   "source": [
    "print(len(mongo_data))\n",
    "print(mongo_data[0].keys())\n",
    "print(mongo_data[0]['tokens_less_sw'])\n",
    "# mongo_data[0]['token_embeddings_less_sw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6e94a889-28b1-42c4-a1fd-6aecfa4e7802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting MEAN_MAX METHOD...\n",
      "Finding the most simlar words in the chunk for each query word...\n",
      "\n",
      "Position\tQuery\t\t\tChunk\t\tsim_score\n",
      "79\n",
      "     79) \tĠdesign\t\t-->\tĠdesign \t1.0000000000000002\n",
      "5\n",
      "     5) \tĠx\t\t-->\tĠx \t\t1.0\n",
      "0\n",
      "     0) \t37\t\t-->\tscale \t\t0.0\n",
      "84\n",
      "     84) \tĠorbital\t-->\tĠorbital \t0.9999999999999999\n",
      "213\n",
      "     213) \tĠtest\t\t-->\tĠtest \t\t1.0\n",
      "127\n",
      "     127) \tĠvehicle\t-->\tĠvehicle \t1.0000000000000004\n",
      "\n",
      "np.mean(np.max(sim, axis=1))\tsimilarity score between query and 169 is 0.8333333333333334\n",
      "\n",
      "np.mean(sim)\t\t\tsimilarity score between query and 169 is 0.051522842245752376\n"
     ]
    }
   ],
   "source": [
    "print_view(METHOD, query_data, mongo_data[0], mongo_data[0]['counter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5ec74-fd04-4da6-af6d-b19fddb7540e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5a5eaea-035f-4a60-a0b7-1e60cc3a3c0a",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "39ad2aa4-a303-404e-87fa-a9e926ec5999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_embeddings = mongo_data[0]['token_embeddings_less_sw']\n",
    "chunk_tokens = mongo_data[0]['tokens_less_sw']\n",
    "query_embedding = query_data[\"query_embedding_search_less_sw\"]\n",
    "query_tokens = query_data[\"tokenized_query_search_less_sw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c57f96a8-3f91-4171-b397-f7e07a8c4d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġstar'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fc06f51-7fb3-49f6-9093-91dbfe59aa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġstar'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data[\"tokenized_query_search_less_sw\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dda51cc1-713a-473b-a57b-ed8df0e78f83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03636299818754196, -0.3930419981479645, -0.1429090052843094, -0.46448901295661926, 0.7612259984016418, 0.7617819905281067, -0.35284098982810974, 1.0031119585037231, 1.0688589811325073, -0.4271070063114166, -0.7240110039710999, -0.3708609938621521, 0.2522050142288208, 1.5639139413833618, -0.005681000184267759, -1.0314220190048218, -1.2768089771270752, 0.8234590291976929, -1.1317089796066284, -0.5063470005989075, -0.3803279995918274, -0.6376360058784485, -0.6495620012283325, -0.35387200117111206, -0.01632400043308735, 0.44753599166870117, 0.7902910113334656, -0.9902560114860535, 0.35526400804519653, 0.5142549872398376, 0.20615799725055695, 0.7465569972991943, -0.25756600499153137, -0.9509879946708679, 0.577193021774292, 0.7618569731712341, -0.21814799308776855, 1.155176043510437, -0.11342599987983704, -0.9682440161705017, -0.25894901156425476, 0.5289760231971741, 0.1664550006389618, -0.47958600521087646, -0.1488569974899292, 0.02031799964606762, -0.918524980545044, -0.19544699788093567, 0.19112199544906616, 1.202815055847168]\n"
     ]
    }
   ],
   "source": [
    "print(chunk_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "69d9f1e7-711f-4420-a6be-a5a4fe8263b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02706499956548214, 0.11715800315141678, -0.1497579962015152, 0.09549400210380554, 0.788237988948822, 0.044426001608371735, 0.21710899472236633, -1.092947006225586, 0.17413100600242615, -0.5446199774742126, -1.3253979682922363, 0.6600490212440491, 0.8856850266456604, 0.3544749915599823, -0.7906050086021423, 0.30542299151420593, 0.43905699253082275, -1.3396470546722412, 0.07484900206327438, -0.38930198550224304, -1.1445200443267822, 0.16912099719047546, -0.1389389932155609, 0.05334300175309181, -0.07674100250005722, -1.1621359586715698, -0.890516996383667, -0.11214400082826614, -0.5438759922981262, 0.4577699899673462, -0.9184399843215942, -0.03812199831008911, 0.054958999156951904, 0.5922399759292603, -0.04942400008440018, 0.2912740111351013, 0.9680520296096802, 0.2256229966878891, -0.3272770047187805, 1.6780519485473633, 0.2525489926338196, 0.36266499757766724, -0.009165000170469284, -0.5830590128898621, -0.6701769828796387, 0.14175300300121307, 0.2028850018978119, 1.5315450429916382, -0.8802869915962219, -0.5250160098075867]\n"
     ]
    }
   ],
   "source": [
    "print(query_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb0b0888-1561-4ba4-8f61-eab59e9c0654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [ 0.036363   -0.393042   -0.14290901 -0.46448901  0.761226    0.76178199\n",
      " -0.35284099  1.00311196  1.06885898 -0.42710701 -0.724011   -0.37086099\n",
      "  0.25220501  1.56391394 -0.005681   -1.03142202 -1.27680898  0.82345903\n",
      " -1.13170898 -0.506347   -0.380328   -0.63763601 -0.649562   -0.353872\n",
      " -0.016324    0.44753599  0.79029101 -0.99025601  0.35526401  0.51425499\n",
      "  0.206158    0.746557   -0.257566   -0.95098799  0.57719302  0.76185697\n",
      " -0.21814799  1.15517604 -0.113426   -0.96824402 -0.25894901  0.52897602\n",
      "  0.166455   -0.47958601 -0.148857    0.020318   -0.91852498 -0.195447\n",
      "  0.191122    1.20281506]\n",
      "B: [ 0.027065    0.117158   -0.149758    0.095494    0.78823799  0.044426\n",
      "  0.21710899 -1.09294701  0.17413101 -0.54461998 -1.32539797  0.66004902\n",
      "  0.88568503  0.35447499 -0.79060501  0.30542299  0.43905699 -1.33964705\n",
      "  0.074849   -0.38930199 -1.14452004  0.169121   -0.13893899  0.053343\n",
      " -0.076741   -1.16213596 -0.890517   -0.112144   -0.54387599  0.45776999\n",
      " -0.91843998 -0.038122    0.054959    0.59223998 -0.049424    0.29127401\n",
      "  0.96805203  0.225623   -0.327277    1.67805195  0.25254899  0.362665\n",
      " -0.009165   -0.58305901 -0.67017698  0.141753    0.202885    1.53154504\n",
      " -0.88028699 -0.52501601]\n",
      "Cosine Similarity: -0.18411191267012425\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    "# define two lists or array\n",
    "A = np.array(chunk_embeddings[0])\n",
    "B = np.array(query_embedding[0])\n",
    " \n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    " \n",
    "# compute cosine similarity\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f689f7b8-c25a-4290-b3fc-cd10ffb7e71a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
