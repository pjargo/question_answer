{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26705118-b459-4fb3-a59d-cf16ebda2cba",
   "metadata": {},
   "source": [
    "#### Notebook: Write data to the Mongodb collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bb0ce09b-04e8-4cf6-943f-a30eb8868fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "# Get the current working directory (notebooks directory)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project directory\n",
    "project_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Assuming your project structure is as described before\n",
    "src_path = os.path.abspath(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "sys.path.append(src_path)\n",
    "from question_answer_site.question_answer.parse_document import parse_document, update_collection\n",
    "from question_answer_site.question_answer.mongodb import MongoDb\n",
    "from question_answer_site.question_answer.config import TOKENIZER, EMBEDDING_MODEL_FNAME, EMBEDDING_MODEL_TYPE, TOKENS_EMBEDDINGS, DOCUMENT_EMBEDDING, \\\n",
    "    DOCUMENT_TOKENS, TOP_N, TRANSFORMER_MODEL_NAME, METHOD, MAX_QUERY_LENGTH, username, password, cluster_url, INPUT_FOLDER, \\\n",
    "    database_name, special_characters, CHUNK_SIZE, CHUNK_OVERLAP\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import spacy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "400d6994-e275-4f10-a21b-7e26315a6369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d102555-fb50-4c80-8607-57e02e93147b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "if EMBEDDING_MODEL_TYPE == 'Word2Vec':\n",
    "    embedding_model = Word2Vec.load(\n",
    "        os.path.join(os.getcwd(), \"question_answer\", \"embedding_models\", EMBEDDING_MODEL_FNAME))\n",
    "elif EMBEDDING_MODEL_TYPE.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    embedding_model = spacy.load(os.path.join(\"..\",\"src\",\"question_answer_site\", \"question_answer\", \"embedding_models\",\n",
    "                                         EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "36ee2c39-73dd-4841-a370-e41d575d0ac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "document_path = os.path.join(\"..\", \"data\", \"space_based_txts\", \"Starlink Explained- What You Need to Know About Elon Musk's Satellite Internet Service.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d86ae211-ba46-466d-b720-fea5599646a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/space_based_txts/Starlink Explained- What You Need to Know About Elon Musk's Satellite Internet Service.txt Starlink Explained- What You Need to Know About Elon Musk's Satellite Internet Service.txt\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "Chunking the tokenized text...\n",
      "\n",
      "printing the shape of chunked dataframe\n",
      "(7, 13)\n"
     ]
    }
   ],
   "source": [
    "data = parse_document(document_path, embedding_model, tokenizer, CHUNK_SIZE, CHUNK_OVERLAP, special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a98f2bc-0a27-4d3c-9962-181248bc9514",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the 'parsed_documents' collection\n",
      "502 documents in 'parsed_documents' before adding\n",
      "509 documents in 'parsed_documents' after adding\n"
     ]
    }
   ],
   "source": [
    "update_collection(\"parsed_documents\", copy.deepcopy(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e894cd5-76fc-45fa-a06f-c282e34e5fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the 'extracted_text' collection\n",
      "27 documents in 'extracted_text' before adding\n",
      "28 documents in 'extracted_text' after adding\n"
     ]
    }
   ],
   "source": [
    "# Should be +1 for adding one document\n",
    "update_collection(\"extracted_text\", copy.deepcopy(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec504ddc-5bad-4bf1-a811-f592668f91d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
