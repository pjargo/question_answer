{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66fec00a-0bb1-41cb-8e46-6743a12e66ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53f8de28-104c-4bfd-a3af-ec3793f3bc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_view(TYPE, query_embedding, chunk_embeddings):\n",
    "    print(f\"Inspecting {TYPE} METHOD...\")\n",
    "    \n",
    "    if TYPE == \"MEAN_MAX\":\n",
    "        print(\"Finding the most simlar words in the chunk for each query word...\\n\")\n",
    "\n",
    "        sim = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "\n",
    "        print(\"Position\\tQuery\\t\\t\\tChunk\\t\\tsim_score\")\n",
    "        for i, s, qt in zip(np.argmax(sim, axis=1), np.max(sim, axis=1), query_tokens):\n",
    "            if len(chunk_tokens[i]) < 7 and len(qt) < 8:\n",
    "                print(f\"     {i}) \\t{qt}\\t\\t-->\\t{chunk_tokens[i]} \\t\\t{s}\")\n",
    "            elif len(chunk_tokens[i]) < 6:\n",
    "                print(f\"     {i}) \\t{qt}\\t-->\\t{chunk_tokens[i]} \\t\\t{s}\")\n",
    "            elif len(chunk_tokens[i]) >= 7 and len(qt) < 8:\n",
    "                print(f\"     {i}) \\t{qt}\\t\\t-->\\t{chunk_tokens[i]} \\t{s}\")\n",
    "            else:\n",
    "                print(f\"     {i}) \\t{qt}\\t-->\\t{chunk_tokens[i]} \\t{s}\")\n",
    "\n",
    "        print(f\"\\nnp.mean(np.max(sim, axis=1))\\tsimilarity score between query and {filename} is {np.mean(np.max(sim, axis=1))}\")\n",
    "        print(f\"\\nnp.mean(sim)\\t\\t\\tsimilarity score between query and {filename} is {np.mean(sim)}\")\n",
    "    \n",
    "    elif TYPE == \"COMBINE_MEAN\":\n",
    "        similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                       np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "        similarity = np.mean(similarity) # Get the single value out of the array\n",
    "        \n",
    "        print(f\"\\nThe average query embedding and average {filename} embedding is {similarity}\")\n",
    "    \n",
    "    elif TYPE == \"MEAN_MEAN\":\n",
    "        sim = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "        print(f\"\\nnp.mean(sim) similarity score between query and {filename} is {np.mean(sim)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"TYPE {TYPE} not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb096e-9b67-46a7-83a4-de3a31699ff9",
   "metadata": {},
   "source": [
    "#### Get Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e0d34e-1e91-4a9e-9abb-34c9d8649c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TOKENIZER\": \"roberta\",\n",
      "    \"input_folder\": \"space_based_pdfs\",\n",
      "    \"embedding_model_type\": \"glove\",\n",
      "    \"embedding_model_fname\": \"roberta_space_based_pdfs_glove_model.bin\",\n",
      "    \"vector_size\": 50,\n",
      "    \"window\": 3,\n",
      "    \"min_count\": 3,\n",
      "    \"sg\": 0,\n",
      "    \"TOKENS_TPYE\": \"tokens_less_sw\",\n",
      "    \"chunk_size\": 350,\n",
      "    \"chunk_overlap\": 0,\n",
      "    \"max_query_length\": 20,\n",
      "    \"top_N\": 20,\n",
      "    \"TOKENS_EMBEDDINGS\": \"query_search_less_sw\",\n",
      "    \"DOCUMENT_EMBEDDING\": \"token_embeddings_less_sw\",\n",
      "    \"DOCUMENT_TOKENS\": \"tokens_less_sw\",\n",
      "    \"METHOD\": \"MEAN_MAX\",\n",
      "    \"transformer_model_name\": \"deepset/roberta-base-squad2\",\n",
      "    \"context_size\": 350\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")) as json_file:\n",
    "    hyperparams = json.load(json_file)\n",
    "    print(json.dumps(hyperparams, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce7b13-c5ba-42f2-a2f0-1c3b2c20ae6f",
   "metadata": {},
   "source": [
    "#### Find the candidate documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502ade2d-c118-4528-980a-ad11465995fc",
   "metadata": {},
   "source": [
    "##### Get the data from the query JSON file\n",
    "- Either specify the filename or the code will get the most recently written file\n",
    "- specify tokens and embeddings \n",
    "    - \"tokenized_query\" | \"tokenized_query_search\" | \"tokenized_query_search_less_sw\"\n",
    "    - \"query_embedding\" | \"query_embedding_search\" | \"query_embedding_search_less_sw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5390ab51-ccff-40d5-93d8-1b9c9418881a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the tokens and embeddings for the query\n",
    "TOKENS_EMBEDDINGS = hyperparams['TOKENS_EMBEDDINGS']\n",
    "\n",
    "# Specify Candidate token embeddings option\n",
    "DOCUMENT_EMBEDDING = hyperparams['DOCUMENT_EMBEDDING']\n",
    "DOCUMENT_TOKENS = hyperparams['DOCUMENT_TOKENS']\n",
    "\n",
    "if TOKENS_EMBEDDINGS == \"query\":\n",
    "    TOKENS = \"tokenized_query\"\n",
    "    EMBEDDINGS = \"query_embedding\"\n",
    "    \n",
    "elif TOKENS_EMBEDDINGS == \"query_search\":\n",
    "    TOKENS = \"tokenized_query_search\"\n",
    "    EMBEDDINGS = \"query_embedding_search\"\n",
    "    \n",
    "elif TOKENS_EMBEDDINGS == \"query_search_less_sw\":\n",
    "    TOKENS = \"tokenized_query_search_less_sw\"\n",
    "    EMBEDDINGS = \"query_embedding_search_less_sw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cafff9-d650-4683-9907-9b0f8a544c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the directory path containing query JSON files\n",
    "query_dir = os.path.join(\"..\", 'query')\n",
    "query_fname = ''\n",
    "\n",
    "# Get the latest query JSON file if the file name is not specified\n",
    "if query_fname == '': \n",
    "    # Get a list of all files in the directory\n",
    "    query_files = os.listdir(query_dir)\n",
    "\n",
    "    # Filter out directories and get only files\n",
    "    query_files = [file for file in query_files if os.path.isfile(os.path.join(query_dir, file))]\n",
    "\n",
    "    # Sort the files by modification time (latest first)\n",
    "    query_files.sort(key=lambda x: os.path.getmtime(os.path.join(query_dir, x)), reverse=True)\n",
    "\n",
    "    # Get the latest file\n",
    "    query_fname = os.path.join(query_dir, query_files[0])\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open(query_fname, 'r') as json_file:\n",
    "    query_data = json.load(json_file)\n",
    "\n",
    "# Now 'data' contains the content of the JSON file as a Python dictionary or list\n",
    "query_embedding = np.array(query_data[EMBEDDINGS])\n",
    "query_tokens = np.array(query_data[TOKENS])\n",
    "\n",
    "# remove the paddings from the query\n",
    "query_embedding = np.array([emb for emb, token in zip(query_embedding, query_tokens) if token != '[PAD]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c981dab-c4e8-4060-ae74-e63b978bd39f",
   "metadata": {},
   "source": [
    "##### Create the ouput directory for the candidate doc to be saved to for this query\n",
    "- If exist, delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b01712-43b1-498d-a04d-ab2e8c778c57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../candidate_docs/8d3958cd0d203f82009a93699d6fc8503747100bfa9b1e083ab0aaa64ec056d7' already exists.\n",
      "Contents of directory '../candidate_docs/8d3958cd0d203f82009a93699d6fc8503747100bfa9b1e083ab0aaa64ec056d7' deleted.\n"
     ]
    }
   ],
   "source": [
    "# Instatiate output path\n",
    "query_file_basename = os.path.basename(query_fname)\n",
    "query_file_basename = os.path.splitext(query_file_basename)[0]\n",
    "candidate_docs_fpath = os.path.join(\"..\", 'candidate_docs', query_file_basename)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(candidate_docs_fpath):\n",
    "    # Create the directory\n",
    "    os.makedirs(candidate_docs_fpath)\n",
    "    print(f\"Directory '{candidate_docs_fpath}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{candidate_docs_fpath}' already exists.\")\n",
    "    # Remove all files and subdirectories within the directory\n",
    "    for item in os.listdir(candidate_docs_fpath):\n",
    "        item_path = os.path.join(candidate_docs_fpath, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            os.remove(item_path)\n",
    "        elif os.path.isdir(item_path):\n",
    "            shutil.rmtree(item_path)\n",
    "    print(f\"Contents of directory '{candidate_docs_fpath}' deleted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215121be-199f-4dbe-a1ef-69138a3fdf84",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Get the candidate documents \n",
    "- Specify number of candidates to get (top_N)\n",
    "- Specify directory containing the documents to inspect (parsed_docs_dir)\n",
    "- Specify similarity score method\n",
    "    - 'MEAN_MEAN': Get the similarity score for each word in the query for a cadidate document by taking the 'average' value of all cosine similarity scores between the word embedding and every word in the candidate docuemnt.  The overall similarity score between the query and each candidate document is the average of the 'average' similarity scores for each word in the query. \n",
    "    - 'MEAN_MAX': Get simlarity score for each word in the query for a candidate document by taking the 'maximum' value of all cosine similarity scores between the word embedding and every word in the candidate docuemnt.  The overall similarity score between the query and each candidate document is the average of the 'maximum' similarity scores for each word in the query.\n",
    "    - 'COMBINE_MEAN': The overall simlarity between a query and a candidate document is the cosime simiilarity beteen the mean of the word embedding of the combined query and the mean word embedding of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ccca7fb-1d15-4abd-b433-164eafed1bb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_N = hyperparams['top_N']\n",
    "\n",
    "# Specify the directory path containing JSON files\n",
    "parsed_docs_dir = os.path.join(\"..\", 'data', f\"{hyperparams['input_folder']}_{hyperparams['TOKENIZER']}_parsed\")\n",
    "\n",
    "METHOD = hyperparams['METHOD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54040e49-379b-4360-8285-88238b4a8c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score for...\n",
      "   method: MEAN_MAX\n",
      "   query embeddings: query_search_less_sw\n",
      "   document tokens/embeddings: tokens_less_sw/token_embeddings_less_sw...\n",
      "\n",
      "Filename: 311.json, Similarity Score: 0.7759228230655965\n",
      "Filename: 340.json, Similarity Score: 0.7373656309564064\n",
      "Filename: 320.json, Similarity Score: 0.7331769783743358\n",
      "Filename: 305.json, Similarity Score: 0.7161916888454517\n",
      "Filename: 274.json, Similarity Score: 0.7161754807269979\n",
      "Filename: 327.json, Similarity Score: 0.7099717189892485\n",
      "Filename: 1.json, Similarity Score: 0.708495858357235\n",
      "Filename: 319.json, Similarity Score: 0.706822070586072\n",
      "Filename: 98.json, Similarity Score: 0.7058784357002936\n",
      "Filename: 338.json, Similarity Score: 0.694804970721026\n",
      "Filename: 105.json, Similarity Score: 0.69288636763282\n",
      "Filename: 23.json, Similarity Score: 0.688114411278079\n",
      "Filename: 335.json, Similarity Score: 0.6833741808795726\n",
      "Filename: 307.json, Similarity Score: 0.6832816147324434\n",
      "Filename: 197.json, Similarity Score: 0.6831008058513993\n",
      "Filename: 118.json, Similarity Score: 0.6819811967349068\n",
      "Filename: 423.json, Similarity Score: 0.6734824763717031\n",
      "Filename: 414.json, Similarity Score: 0.6734824763717031\n",
      "Filename: 329.json, Similarity Score: 0.6729504785834501\n",
      "Filename: 273.json, Similarity Score: 0.6716356086098213\n"
     ]
    }
   ],
   "source": [
    "# List to store cosine similarity scores and corresponding document filenames\n",
    "similarity_scores = []\n",
    "\n",
    "# Iterate through JSON files in the directory\n",
    "print(f\"Similarity score for...\\n   method: {METHOD}\\n   query embeddings: {TOKENS_EMBEDDINGS}\\n   document tokens/embeddings: {DOCUMENT_TOKENS}/{DOCUMENT_EMBEDDING}...\\n\")\n",
    "for filename in os.listdir(parsed_docs_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(parsed_docs_dir, filename)\n",
    "\n",
    "        with open(file_path, 'r') as json_file:\n",
    "            data = json.load(json_file)\n",
    "            chunk_embeddings = np.array(data[DOCUMENT_EMBEDDING])\n",
    "            chunk_tokens = np.array(data[DOCUMENT_TOKENS])\n",
    "            \n",
    "            # remove the paddingsand unknown tokens from the query\n",
    "            chunk_embeddings = np.array([emb for emb, token in zip(chunk_embeddings, chunk_tokens) if token not in ['[PAD]', '[UNK]']])\n",
    "            \n",
    "            # Calculate cosine similarity between query_embedding and chunk_embeddings METHOD = 'MEAN_MAX'\n",
    "            if METHOD == 'MEAN_MAX':\n",
    "                similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "                similarity = np.mean(np.max(similarity, axis=1))\n",
    "                \n",
    "            if METHOD == 'MEAN_MEAN':\n",
    "                similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "                similarity = np.mean(similarity)\n",
    "            \n",
    "            if METHOD == 'COMBINE_MEAN':\n",
    "                similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                               np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "                similarity = np.mean(similarity) # Get the single value out of the array\n",
    "                \n",
    "            # Store similarity score and filename\n",
    "            similarity_scores.append((similarity, filename))\n",
    "\n",
    "# Sort the similarity_scores in descending order based on the similarity score\n",
    "similarity_scores.sort(reverse=True)\n",
    "\n",
    "# Get the top 10 candidate documents\n",
    "top_N_candidates = similarity_scores[:top_N]\n",
    "\n",
    "# Print the top 10 candidate documents and their similarity scores and copy the files to the candidate docs filepath\n",
    "for similarity, filename in top_N_candidates:\n",
    "    print(f\"Filename: {filename}, Similarity Score: {similarity}\")\n",
    "    shutil.copy(os.path.join(parsed_docs_dir, filename), candidate_docs_fpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806cfd3-8e46-42a2-b138-88bee8307a41",
   "metadata": {},
   "source": [
    "##### Inspect the similarity scores for individual documents\n",
    "- Compare similarity scores of top_N documnets with expected answer\n",
    "- Specify JSON file that was the expected most similar (filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b36301-c281-4e2e-9def-b106b7b85b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get data for all methods\n",
    "filename = \"340.json\"\n",
    "file_path = os.path.join(parsed_docs_dir, filename)\n",
    "with open(file_path, 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    chunk_embeddings = np.array(data['token_embeddings'])\n",
    "    chunk_tokens = np.array(data['tokens'])\n",
    "    \n",
    "    # remove the paddings from the query\n",
    "    chunk_embeddings = np.array([emb for emb, token in zip(chunk_embeddings, chunk_tokens) if token not in ['[PAD]', '[UNK]']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d2baa87-4836-4a68-8a61-e7192e26a488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting MEAN_MAX METHOD...\n",
      "Finding the most simlar words in the chunk for each query word...\n",
      "\n",
      "Position\tQuery\t\t\tChunk\t\tsim_score\n",
      "     3) \tĠmuch\t\t-->\tĠadverse \t0.4112510188623063\n",
      "     12) \tĠstar\t\t-->\tĠstar \t\t0.9999999999999994\n",
      "     13) \tlink\t\t-->\tlink \t\t0.9999999999999997\n",
      "     6) \tĠsatellite\t-->\tĠsatellite \t1.0\n",
      "     307) \tĠprogram\t-->\tĠprogram \t1.0\n",
      "     302) \tĠcost\t\t-->\tĠyears \t\t0.5511542718394515\n",
      "     3) \tĠmuch\t\t-->\tĠadverse \t0.4112510188623063\n",
      "     0) \tĠrevenue\t-->\tĠdeal \t\t0.0\n",
      "     296) \tĠexpected\t-->\tĠexpected \t0.9999999999999999\n",
      "     307) \tĠprogram\t-->\tĠprogram \t1.0\n",
      "\n",
      "np.mean(np.max(sim, axis=1))\tsimilarity score between query and 340.json is 0.7373656309564064\n",
      "\n",
      "np.mean(sim)\t\t\tsimilarity score between query and 340.json is 0.013114345302941359\n"
     ]
    }
   ],
   "source": [
    "print_view(METHOD, query_embedding, chunk_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8027f-1171-45b0-85be-70bb54f3e2a1",
   "metadata": {},
   "source": [
    "#### Below code if for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b467f1bc-40c3-41fe-9142-a8b8328fced1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key '##var' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mword2vec_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Access the embedding of a word\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchunk_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:403\u001b[0m, in \u001b[0;36mKeyedVectors.__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get vector representation of `key_or_keys`.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key_or_keys, _KEY_TYPES):\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_or_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m vstack([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_vector(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m key_or_keys])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:446\u001b[0m, in \u001b[0;36mKeyedVectors.get_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vector\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    423\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the key's vector, as a 1D numpy array.\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \n\u001b[1;32m    425\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    444\u001b[0m \n\u001b[1;32m    445\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m norm:\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfill_norms()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/gensim/models/keyedvectors.py:420\u001b[0m, in \u001b[0;36mKeyedVectors.get_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not present\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key '##var' not present\""
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# Load the trained Word2Vec model\n",
    "model = Word2Vec.load(\"word2vec_model.bin\")\n",
    "\n",
    "# Access the embedding of a word\n",
    "embedding = model.wv[chunk_tokens[2]]\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10024922-a33a-4cf5-b3bf-3dc7d46f14c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.87479382671848 4.88086295121961\n",
      "[0.99875655]\n"
     ]
    }
   ],
   "source": [
    "# Prooving that element 2 of cosine_similarity output corresponds to cosine_similarity of token 1 in query and token 2 in chunk\n",
    "num = 0\n",
    "den_a = 0\n",
    "den_b = 0 \n",
    "for a, b in zip(query_embedding[0].reshape(-1, 1), chunk_embeddings[71].reshape(-1, 1)):\n",
    "    num += a*b\n",
    "    den_a += a**2\n",
    "    den_b += b**2\n",
    "    \n",
    "den = np.sqrt(den_a) * np.sqrt(den_b)\n",
    "print(num[0], den[0])\n",
    "print(num[0]/den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26718837-f74e-4d54-9189-631a3680d78b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00979525] [0.02950395]\n",
      "[0.1801499] [0.23714575]\n",
      "[-0.11282597] [-0.14750037]\n",
      "[-0.16389379] [-0.20449817]\n",
      "[0.12019866] [0.16305223]\n",
      "[-0.19132757] [-0.24321704]\n",
      "[0.03743042] [0.04248741]\n",
      "[0.36185887] [0.45494699]\n",
      "[-0.18961333] [-0.2351187]\n",
      "[-0.08775483] [-0.11479293]\n",
      "[-0.03145975] [-0.03520138]\n",
      "[-0.43530247] [-0.50552255]\n",
      "[-0.28460452] [-0.34083328]\n",
      "[-0.01264222] [-0.02899849]\n",
      "[0.11428754] [0.14555983]\n",
      "[-0.02115079] [-0.01415997]\n",
      "[-0.10840299] [-0.12329137]\n",
      "[-0.17891493] [-0.20056075]\n",
      "[0.11752865] [0.12509882]\n",
      "[-0.79247826] [-0.95711064]\n",
      "[0.0254809] [0.02256134]\n",
      "[0.17190607] [0.19424245]\n",
      "[0.37160259] [0.44927961]\n",
      "[0.00258394] [0.00096512]\n",
      "[-0.26391593] [-0.31885713]\n",
      "[-0.13601364] [-0.15838972]\n",
      "[-0.06832439] [-0.08720339]\n",
      "[-0.30661389] [-0.36412066]\n",
      "[0.06496375] [0.08291019]\n",
      "[-0.00426452] [0.00857907]\n",
      "[0.2630567] [0.31602088]\n",
      "[0.06148953] [0.06348988]\n",
      "[0.30864471] [0.36563432]\n",
      "[-0.08815695] [-0.10643951]\n",
      "[-0.03167176] [-0.05894051]\n",
      "[0.18723206] [0.21207635]\n",
      "[0.01943905] [0.01611895]\n",
      "[0.00176542] [0.01378242]\n",
      "[0.06011398] [0.09025013]\n",
      "[-0.11576083] [-0.15505823]\n",
      "[0.214077] [0.25421658]\n",
      "[-0.32721213] [-0.38321361]\n",
      "[0.17942585] [0.24016681]\n",
      "[0.02022693] [0.01126133]\n",
      "[0.13222012] [0.18320598]\n",
      "[-0.0077295] [-0.0002453]\n",
      "[-0.10052134] [-0.10489325]\n",
      "[-0.25017101] [-0.28836116]\n",
      "[0.10541049] [0.12931791]\n",
      "[0.22803409] [0.27146852]\n",
      "[-0.06985743] [-0.11103578]\n",
      "[0.12506942] [0.16145298]\n",
      "[0.18061037] [0.23374085]\n",
      "[0.05854814] [0.05694872]\n",
      "[-0.07007992] [-0.06431906]\n",
      "[0.21390846] [0.27588102]\n",
      "[0.26435599] [0.31215665]\n",
      "[-0.05428379] [-0.04492967]\n",
      "[0.09215041] [0.11584856]\n",
      "[-0.01455719] [-0.01549899]\n",
      "[0.15124777] [0.18813503]\n",
      "[0.11826792] [0.14421654]\n",
      "[-0.26218906] [-0.28495178]\n",
      "[0.16718416] [0.19887303]\n",
      "[-0.09972232] [-0.10459019]\n",
      "[0.10910607] [0.14095631]\n",
      "[0.10802886] [0.14892595]\n",
      "[0.39006442] [0.47784364]\n",
      "[-0.3066175] [-0.36720681]\n",
      "[0.37935326] [0.44213501]\n",
      "[-0.37435785] [-0.45390645]\n",
      "[-0.26133454] [-0.31810844]\n",
      "[0.06530114] [0.07836393]\n",
      "[0.20023556] [0.25185126]\n",
      "[0.24153526] [0.29954749]\n",
      "[-0.0667763] [-0.08509386]\n",
      "[-0.08314129] [-0.10486429]\n",
      "[0.05980352] [0.08972413]\n",
      "[-0.05648984] [-0.06379373]\n",
      "[0.16252342] [0.19165456]\n",
      "[-0.02183185] [-0.02753208]\n",
      "[-0.03928515] [-0.06500075]\n",
      "[-0.44979697] [-0.56154472]\n",
      "[0.26927412] [0.31909162]\n",
      "[0.07215489] [0.10740604]\n",
      "[0.0439691] [0.04938825]\n",
      "[0.09785072] [0.12086819]\n",
      "[0.33124194] [0.406389]\n",
      "[0.23865572] [0.28591985]\n",
      "[-0.07179324] [-0.09659936]\n",
      "[0.15202269] [0.18313849]\n",
      "[-0.06470573] [-0.06761394]\n",
      "[-0.18670893] [-0.23490962]\n",
      "[-0.08760365] [-0.09285161]\n",
      "[0.14860144] [0.19121511]\n",
      "[0.25000155] [0.31772155]\n",
      "[0.10073803] [0.11557852]\n",
      "[-0.30593249] [-0.38052502]\n",
      "[-0.02374373] [-0.02673626]\n",
      "[0.05383135] [0.04900356]\n"
     ]
    }
   ],
   "source": [
    "for a, b in zip(query_embedding[0].reshape(-1, 1), chunk_embeddings[71].reshape(-1, 1)):\n",
    "    print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83f574fb-94e3-4b60-9906-7dfe1d178527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d7c3f3e-ed3c-4808-b831-247015272d25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8f873-ea3e-4afa-914b-0118d9569c40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
