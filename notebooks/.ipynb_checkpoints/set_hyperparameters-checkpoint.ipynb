{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20180d59-bcc7-40a1-930b-f232caee62af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb29cf-dde6-4181-8791-89aaa088b03d",
   "metadata": {},
   "source": [
    "#### Set hyperparameters for the data processing\n",
    "1. Set embedding layer\n",
    "    - TOKENIZER: Type of tokenizer used \n",
    "    - input_folder: input data to create the embedding layer\n",
    "    - embedding_model_type: Wordembedding algorithm\n",
    "    - embedding_model_fname: filname of output embedding layer model\n",
    "    - vector_size: length of the word embeddings\n",
    "    - window: Size of wondow to creat \n",
    "    - min_count: minimum frequency of word occuring in training data to exist in word embedding\n",
    "    - sg\n",
    "    - TOKENS_TPYE: Tokens to use for building the embedding layer (with or without stopwords)\n",
    "2. PDF PARSER\n",
    "    - TOKENIZER: Tokenizer used to split the text\n",
    "    - input_folder: input data to parse\n",
    "    - embedding_model_fname: filname of input embedding layer model\n",
    "    - chunk_size: Size of candidate documents\n",
    "    - chunk_overlap: by how much the chunks overlap\n",
    "3. Build Query\n",
    "    - TOKENIZER: Tokenizer used to split the query\n",
    "    - embedding_model_fname: filename of input embedding layer to get embeddings of tokenized query\n",
    "    - max_query_length: Size to pad of truncate query\n",
    "4. Get Candidate Documents\n",
    "    - top_N: Number of candidate documents to save\n",
    "    - TOKENS_EMBEDDINGS: Which tokens/embeddings of the query to use\n",
    "    - DOCUMENT_EMBEDDING: Which embeddings of the chunks to use\n",
    "    - DOCUMENT_TOKENS: Which tokens of the chunks to use (same as DOCUMENT_EMBEDDING)\n",
    "    - METHOD: method of cosine similarity\n",
    "5. Question and Answers\n",
    "    - transformer_model_name: What model used to fine the answer\n",
    "    - TOKENIZER: Tokenizer used to create the model inputs and the decode the message\n",
    "    - context_size: Size of the context vector to fit the candidate documents into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86150bf0-739d-4f1c-8cdd-6e769b4316cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the global vars\n",
    "TOKENIZER = 'roberta' # 'bert' or 'roberta'\n",
    "input_folder = \"space_based_pdfs\" # 'space_based_pdfs' or 'math_based_pdfs'\n",
    "embedding_model_type = 'glove' # Word2Vec, Fastrack, GLove\n",
    "embedding_model_fname = f\"{TOKENIZER}_{input_folder}_{embedding_model_type}_model.bin\"\n",
    "vector_size = 50\n",
    "window = 3\n",
    "min_count = 3\n",
    "sg = 0\n",
    "TOKENS_TPYE = \"tokens_less_sw\" # \"tokens_less_sw\", \"tokens\"\n",
    "chunk_size = 350\n",
    "chunk_overlap = 0\n",
    "max_query_length = 20\n",
    "top_N = 20\n",
    "TOKENS_EMBEDDINGS = \"query_search_less_sw\" # \"query_search_less_sw\", \"query_search\", \"query\"\n",
    "DOCUMENT_EMBEDDING = \"token_embeddings_less_sw\" # \"token_embeddings_less_sw\", \"token_embeddings\"\n",
    "DOCUMENT_TOKENS = \"tokens_less_sw\" # \"tokens_less_sw\", \"tokens\"\n",
    "METHOD = \"COMBINE_MEAN\" # 'MEAN_MAX', 'MEAN_MEAN', 'COMBINE_MEAN'\n",
    "transformer_model_name = \"deepset/roberta-base-squad2\" # \"deepset/roberta-base-squad2\", \"bert-base-uncased\"\n",
    "context_size = 350 \n",
    "\n",
    "hyperparameters = {\n",
    "    'TOKENIZER': TOKENIZER,\n",
    "    'input_folder': input_folder,\n",
    "    'embedding_model_type': embedding_model_type,\n",
    "    'embedding_model_fname': embedding_model_fname,\n",
    "    'vector_size': vector_size,\n",
    "    'window': window,\n",
    "    'min_count': min_count,\n",
    "    'sg': sg,\n",
    "    'TOKENS_TPYE': TOKENS_TPYE,\n",
    "    'chunk_size': chunk_size,\n",
    "    'chunk_overlap': chunk_overlap,\n",
    "    'max_query_length': max_query_length,\n",
    "    'top_N': top_N,\n",
    "    'TOKENS_EMBEDDINGS': TOKENS_EMBEDDINGS,\n",
    "    'DOCUMENT_EMBEDDING': DOCUMENT_EMBEDDING,\n",
    "    'DOCUMENT_TOKENS': DOCUMENT_TOKENS,\n",
    "    'METHOD': METHOD,\n",
    "    'transformer_model_name': transformer_model_name,\n",
    "    'context_size': context_size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5315cf6-be11-4447-b980-9af7c062567e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file '../vars/hyperparameters1.json' updated with new data.\n"
     ]
    }
   ],
   "source": [
    "# File path for the JSON file\n",
    "json_file_path = os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")\n",
    "\n",
    "# Check if the JSON file exists\n",
    "if not os.path.exists(json_file_path):\n",
    "    # If the file doesn't exist, create and write to it\n",
    "    with open(json_file_path, \"w\") as json_file:\n",
    "        json.dump(hyperparameters, json_file, indent=4)\n",
    "    print(f\"JSON file '{json_file_path}' created and data written.\")\n",
    "else:\n",
    "    # If the file exists, update its contents\n",
    "    with open(json_file_path, \"w\") as json_file:\n",
    "        json.dump(hyperparameters, json_file, indent=4)\n",
    "    print(f\"JSON file '{json_file_path}' updated with new data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d993a83c-cae0-44de-ae8e-2939e5bdd85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5a74d-94b7-4e87-9ac4-b1ec4227d059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
