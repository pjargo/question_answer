{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d7b836-2be2-4d93-8e56-646b1965cb02",
   "metadata": {},
   "source": [
    "### Notebook: Write query and get top N similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90027071-a4ef-4f1f-94b5-8b1435ada488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# Get the current working directory (notebooks directory)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project directory\n",
    "project_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Assuming your project structure is as described before\n",
    "src_path = os.path.abspath(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from question_answer_site.question_answer.mongodb import MongoDb\n",
    "from question_answer_site.question_answer.utils import remove_non_word_chars, clean_text, tokens_to_embeddings, post_process_output, correct_spelling\n",
    "from question_answer_site.question_answer.config import TOKENIZER, EMBEDDING_MODEL_FNAME, EMBEDDING_MODEL_TYPE, TOKENS_EMBEDDINGS, DOCUMENT_EMBEDDING, \\\n",
    "    DOCUMENT_TOKENS, TOP_N, TRANSFORMER_MODEL_NAME, METHOD, MAX_QUERY_LENGTH, username, password, cluster_url, \\\n",
    "    database_name\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "from urllib.parse import quote_plus\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "375ba3c1-0bc1-4ec8-8eba-8493924d3d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_user_1 password33566 cluster0 question_answer\n"
     ]
    }
   ],
   "source": [
    "query = \"what does the starlink program do?\"\n",
    "print(username, password, cluster_url, database_name)\n",
    "escaped_username = quote_plus(username)\n",
    "escaped_password = quote_plus(password)\n",
    "\n",
    "# use MongoDb class to connect to database instance and get the documents\n",
    "mongo_db = MongoDb(escaped_username, escaped_password, cluster_url, database_name, \"parsed_documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ea362d42-09ed-4b70-9c9f-a70468d0c709",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "531\n"
     ]
    }
   ],
   "source": [
    "if mongo_db.connect():\n",
    "    print(mongo_db.count_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "322a8f7a-7582-4135-bf91-aacacf5b6e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space=True)\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "tokenizer = tokenizers[TOKENIZER]\n",
    "\n",
    "# Load your trained Word2Vec model\n",
    "if EMBEDDING_MODEL_TYPE == 'Word2Vec':\n",
    "    model = Word2Vec.load(\n",
    "        os.path.join(os.getcwd(), \"question_answer\", \"embedding_models\", EMBEDDING_MODEL_FNAME))\n",
    "elif EMBEDDING_MODEL_TYPE.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    model = spacy.load(os.path.join(\"..\",\"src\",\"question_answer_site\", \"question_answer\", \"embedding_models\",\n",
    "                                    EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))\n",
    "\n",
    "# Specify Candidate token embeddings option\n",
    "if TOKENS_EMBEDDINGS == \"query\":\n",
    "    TOKENS = \"tokenized_query\"\n",
    "    EMBEDDINGS = \"query_embedding\"\n",
    "elif TOKENS_EMBEDDINGS == \"query_search\":\n",
    "    TOKENS = \"tokenized_query_search\"\n",
    "    EMBEDDINGS = \"query_embedding_search\"\n",
    "else:\n",
    "    TOKENS = \"tokenized_query_search_less_sw\"\n",
    "    EMBEDDINGS = \"query_embedding_search_less_sw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6636b8f8-03fc-43a4-8847-3dfb9898eae0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def spell_check(user_query):\n",
    "    tokenized_query = tokenizer.tokenize(user_query)\n",
    "\n",
    "    # Group tokens into words\n",
    "    words = []\n",
    "    current_word = \"\"\n",
    "    for token in tokenized_query:\n",
    "        if token.startswith(\"Ġ\"):  # Indicates the start of a new word\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "            current_word = token[1:] if token[1:] not in ['(', '[', '{', '/', '\\\\'] else ''\n",
    "        else:\n",
    "            current_word += token if token not in [')', ']', '}', '/', '\\\\', '?', \".\", \"!\"] else ''\n",
    "            if token in ['/', '\\\\']:\n",
    "                words.append(current_word)\n",
    "                current_word = ''\n",
    "    if current_word:\n",
    "        words.append(current_word)\n",
    "\n",
    "    # Identify misspelled words not in the embeddings model\n",
    "    misspelled_words = []\n",
    "    for word in words:\n",
    "        # Split punctuation and hyphens from the word\n",
    "        base_word = \"\".join(char for char in word if char.isalnum() or char in [\"'\", \"-\"])\n",
    "        if any(list(map(lambda x: not any(x),\n",
    "                        tokens_to_embeddings(tokenizer.tokenize(base_word), model, RANDOM=False)))):\n",
    "            # Add the original word to the misspelled_words list\n",
    "            misspelled_words.append(word)\n",
    "    # Correct the spelling of misspelled words\n",
    "    corrected_words = {word: correct_spelling(word) for word in misspelled_words}\n",
    "\n",
    "    # Replace misspelled words in the original query\n",
    "    corrected_query = user_query\n",
    "    for original, corrected in corrected_words.items():\n",
    "        corrected_query = corrected_query.replace(original, corrected)\n",
    "\n",
    "    return corrected_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b98de6a3-1442-4c27-af8a-2bec729cc956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_query):\n",
    "    user_query = user_query.lower()\n",
    "\n",
    "    # clean query for BERT input\n",
    "    user_query = clean_text(user_query)\n",
    "    print(\"Uncorrected query: \", user_query)\n",
    "    user_query = spell_check(user_query)\n",
    "    print(\"Corrected query: \", user_query)\n",
    "\n",
    "    # clean query for candidate search\n",
    "    user_query_for_search = remove_non_word_chars(user_query)\n",
    "\n",
    "    # Tokenize the query for BERT input\n",
    "    tokenized_query = tokenizer.tokenize(user_query)\n",
    "\n",
    "    # Tokenize the query for candidate search\n",
    "    tokenized_query_for_search = tokenizer.tokenize(user_query_for_search)\n",
    "\n",
    "    # Remove the stop words for the tokenized query for search\n",
    "    nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    nltk_stop_words.extend([\"Ġ\" + word for word in nltk_stop_words])  # Add the roberta modified tokens\n",
    "    tokenized_query_for_search_less_sw = [token for token in tokenized_query_for_search if\n",
    "                                          token not in nltk_stop_words]\n",
    "\n",
    "    # Pad or truncate the query to a fixed length of 20 tokens (BERT input)\n",
    "\n",
    "    if len(tokenized_query) > MAX_QUERY_LENGTH:\n",
    "        tokenized_query = tokenized_query[:MAX_QUERY_LENGTH]\n",
    "    else:\n",
    "        padding_length = MAX_QUERY_LENGTH - len(tokenized_query)\n",
    "        tokenized_query = tokenized_query + [tokenizer.pad_token] * padding_length\n",
    "\n",
    "    # Convert the tokenized query to input IDs and attention mask\n",
    "    input_ids_query = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
    "    attention_mask_query = [1] * len(input_ids_query)\n",
    "\n",
    "    # Convert to tensors\n",
    "    input_ids_query = torch.tensor(input_ids_query).unsqueeze(0)  # Add batch dimension\n",
    "    attention_mask_query = torch.tensor(attention_mask_query).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Get the query embeddings for the candidate document search\n",
    "    query_embeddings = tokens_to_embeddings(tokenized_query, model, RANDOM=False)\n",
    "    query_embeddings_search = tokens_to_embeddings(tokenized_query_for_search, model, RANDOM=False)\n",
    "    query_embeddings_less_sw = tokens_to_embeddings(tokenized_query_for_search_less_sw, model, RANDOM=False)\n",
    "\n",
    "    query_data = {\n",
    "        \"query\": user_query,\n",
    "        \"input_ids_query\": input_ids_query.tolist(),\n",
    "        \"attention_mask_query\": attention_mask_query.tolist(),\n",
    "        \"query_search\": user_query_for_search,\n",
    "        \"tokenized_query\": tokenized_query,\n",
    "        \"tokenized_query_search\": tokenized_query_for_search,\n",
    "        \"tokenized_query_search_less_sw\": tokenized_query_for_search_less_sw,\n",
    "        \"query_embedding\": query_embeddings, #.tolist(),  # Just used for the candidate search\n",
    "        \"query_embedding_search\": query_embeddings_search, #.tolist(),  # Just used for the candidate search, cleaned\n",
    "        \"query_embedding_search_less_sw\": query_embeddings_less_sw # .tolist()\n",
    "        # Just used for the candidate search, cleaned more\n",
    "    }\n",
    "    # return json.dumps(query_data['query'], indent=2)\n",
    "    return query_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "64e1f434-0b64-40a2-b72a-8f7dbf9e69aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncorrected query:  what does the starlink program do?\n",
      "Corrected query:  what does the starlink program do?\n",
      "['Ġwhat', 'Ġdoes', 'Ġthe', 'Ġstar', 'link', 'Ġprogram', 'Ġdo', '?', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "query_data = process_query(user_query=query)\n",
    "print(query_data[ \"tokenized_query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c421d341-f501-48e0-8f7d-964c15fbe23a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_documents_from_mongo():\n",
    "    if mongo_db.connect():\n",
    "        documents = [document for document in mongo_db.iterate_documents()]\n",
    "        print(f\"Total documents: {mongo_db.count_documents()}\")\n",
    "        mongo_db.disconnect()\n",
    "        return documents\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0b355dd1-b2af-4bf0-9b65-579571e1a1b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# METHOD = 'COMBINE_MEAN'\n",
    "METHOD = 'MEAN_MAX'\n",
    "def get_topn_docs(documents_list, query_data):\n",
    "    query_embedding = np.array(query_data[EMBEDDINGS])\n",
    "    query_tokens = np.array(query_data[TOKENS])\n",
    "\n",
    "    # remove the paddings from the query\n",
    "    query_embedding = np.array([emb for emb, token in zip(query_embedding, query_tokens) if token != '[PAD]'])\n",
    "\n",
    "    # List to store cosine similarity scores and corresponding document filenames\n",
    "    similarity_scores = []\n",
    "\n",
    "    for doc in documents_list:\n",
    "        chunk_embeddings = np.array(doc[DOCUMENT_EMBEDDING])\n",
    "        chunk_tokens = np.array(doc[DOCUMENT_TOKENS])\n",
    "\n",
    "        # remove the paddings and unknown tokens from the query\n",
    "        chunk_embeddings = np.array(\n",
    "            [emb for emb, token in zip(chunk_embeddings, chunk_tokens) if token not in ['[PAD]', '[UNK]']])\n",
    "\n",
    "        # Calculate cosine similarity between query_embedding and chunk_embeddings METHOD = 'MEAN_MAX'\n",
    "        if METHOD == 'MEAN_MAX':\n",
    "            similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "            similarity = np.mean(np.max(similarity, axis=1))\n",
    "\n",
    "        elif METHOD == 'MEAN_MEAN':\n",
    "            similarity = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "            similarity = np.mean(similarity)\n",
    "\n",
    "        # if METHOD == 'COMBINE_MEAN':\n",
    "        else:\n",
    "            similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                           np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "            similarity = np.mean(similarity)  # Get the single value out of the array\n",
    "\n",
    "        # Store similarity score and filename\n",
    "        similarity_scores.append((similarity, doc))\n",
    "\n",
    "    # Sort the similarity_scores in descending order based on the similarity score\n",
    "    if similarity_scores:\n",
    "        similarity_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        # for confidence, parsed_doc_chunk_dict in similarity_scores[:TOP_N]:\n",
    "        #     print(parsed_doc_chunk_dict['counter'])\n",
    "        #     print(self.tokenizer.convert_tokens_to_string(parsed_doc_chunk_dict['tokens']))\n",
    "        #     print(parsed_doc_chunk_dict['Document'])\n",
    "        #     print(confidence)\n",
    "        #     print()\n",
    "        return similarity_scores[:TOP_N]\n",
    "    return similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3392b2a7-b063-42cf-93ad-4b710458af52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_candidate_docs(query_data):\n",
    "    documents = get_documents_from_mongo()\n",
    "    top_n_documents = get_topn_docs(documents_list=documents,\n",
    "                                         query_data=query_data)\n",
    "\n",
    "    return top_n_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2ef5aeae-de5b-405e-8711-0824e213ff18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 531\n",
      "Time taken to find top 10 documents: 8.157408952713013 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get the candidate documents, top_n_documents: (similarity_score, document dictionary)\n",
    "start_time = time.time()\n",
    "\n",
    "top_n_documents = get_candidate_docs(query_data)\n",
    "top_n_documents.sort(key=lambda x: x[1]['counter'])\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Time taken to find top {TOP_N} documents: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ea405201-abb6-48e3-b19c-17468e5503b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas V - Wikipedia.pdf \t\t\t\t129 \t 0.9793264932782672\n",
      "Atlas V - Wikipedia.pdf \t\t\t\t134 \t 0.9793264932782672\n",
      "Atlas V - Wikipedia.pdf \t\t\t\t144 \t 0.9793264932782672\n",
      "Atlas V - Wikipedia.pdf \t\t\t\t148 \t 0.9793264932782672\n",
      "Starlink - Wikipedia.pdf \t\t\t\t394 \t 1.0000000000000002\n",
      "Starlink - Wikipedia.pdf \t\t\t\t395 \t 1.0000000000000002\n",
      "Starlink - Wikipedia.pdf \t\t\t\t404 \t 1.0000000000000002\n",
      "Starlink - Wikipedia.pdf \t\t\t\t405 \t 1.0000000000000002\n",
      "Starlink - Wikipedia.pdf \t\t\t\t406 \t 1.0000000000000002\n",
      "Starlink - Wikipedia.pdf \t\t\t\t421 \t 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "for sim, doc in top_n_documents:\n",
    "    if len(doc['Document']) < 23:\n",
    "        print(f\"{doc['Document']}\\t\\t\\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 30:\n",
    "        print(f\"{doc['Document']} \\t\\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 36:\n",
    "        print(f\"{doc['Document']} \\t\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 42:\n",
    "        print(f\"{doc['Document']}\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    elif len(doc['Document']) <= 46:\n",
    "        print(f\"{doc['Document']}\\t\\t{doc['counter']} \\t {sim}\")\n",
    "    else:\n",
    "        print(f\"{doc['Document']}\\t{doc['counter']} \\t {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "72138caf-5dd2-433a-98a7-4f1b2abc60f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Counter</th>\n",
       "      <th>Simularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Atlas V - Wikipedia.pdf</td>\n",
       "      <td>129</td>\n",
       "      <td>0.979275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Atlas V - Wikipedia.pdf</td>\n",
       "      <td>134</td>\n",
       "      <td>0.979275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Atlas V - Wikipedia.pdf</td>\n",
       "      <td>144</td>\n",
       "      <td>0.979275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>James Webb Space Telescope - Wikipedia.pdf</td>\n",
       "      <td>361</td>\n",
       "      <td>0.979275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Starlink - Wikipedia.pdf</td>\n",
       "      <td>394</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Starlink - Wikipedia.pdf</td>\n",
       "      <td>395</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Starlink - Wikipedia.pdf</td>\n",
       "      <td>404</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Starlink - Wikipedia.pdf</td>\n",
       "      <td>405</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Starlink - Wikipedia.pdf</td>\n",
       "      <td>406</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Starlink - Wikipedia.pdf</td>\n",
       "      <td>421</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Document  Counter  Simularity\n",
       "0                     Atlas V - Wikipedia.pdf      129    0.979275\n",
       "1                     Atlas V - Wikipedia.pdf      134    0.979275\n",
       "2                     Atlas V - Wikipedia.pdf      144    0.979275\n",
       "3  James Webb Space Telescope - Wikipedia.pdf      361    0.979275\n",
       "4                    Starlink - Wikipedia.pdf      394    1.000000\n",
       "5                    Starlink - Wikipedia.pdf      395    1.000000\n",
       "6                    Starlink - Wikipedia.pdf      404    1.000000\n",
       "7                    Starlink - Wikipedia.pdf      405    1.000000\n",
       "8                    Starlink - Wikipedia.pdf      406    1.000000\n",
       "9                    Starlink - Wikipedia.pdf      421    1.000000"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "disp_dict = {\n",
    "    \"Document\":[],\n",
    "    \"Counter\":[],  \n",
    "    \"Simularity\":[], \n",
    "}\n",
    "for sim, doc in top_n_documents:\n",
    "    disp_dict[\"Document\"].append(doc['Document'])\n",
    "    disp_dict[\"Counter\"].append(doc['counter'])\n",
    "    disp_dict[\"Simularity\"].append(sim)\n",
    "    \n",
    "disp_df = pd.DataFrame(disp_dict)\n",
    "# disp_df[disp_df['Counter']==39]\n",
    "# disp_df[disp_df['Document']==\"Starlink - Wikipedia.pdf\"]\n",
    "disp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c754ab1-d976-46cf-9f8b-f3f5c6d1f4c7",
   "metadata": {},
   "source": [
    "#### Get specific chunk from Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c8451fb-3aa1-47bb-90cc-559c923e71a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_view(TYPE, query_info, chunk, counter):\n",
    "    print(f\"Inspecting {TYPE} METHOD...\")\n",
    "    chunk_embeddings = chunk['token_embeddings_less_sw']\n",
    "    chunk_tokens = chunk['tokens_less_sw']\n",
    "    query_embedding = query_info[\"query_embedding_search_less_sw\"]\n",
    "    query_tokens = query_info[\"tokenized_query_search_less_sw\"]\n",
    "    \n",
    "    if TYPE == \"MEAN_MAX\":\n",
    "        print(\"Finding the most simlar words in the chunk for each query word...\\n\")\n",
    "\n",
    "        sim = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "\n",
    "        print(\"Position\\tQuery\\t\\t\\tChunk\\t\\tsim_score\")\n",
    "        for i, s, qt in zip(np.argmax(sim, axis=1), np.max(sim, axis=1), query_tokens):\n",
    "            print(i)\n",
    "            if len(chunk_tokens[i]) < 7 and len(qt) < 8:\n",
    "                print(f\"     {i}) \\t{qt}\\t\\t-->\\t{chunk_tokens[i]} \\t\\t{s}\")\n",
    "            elif len(chunk_tokens[i]) < 6:\n",
    "                print(f\"     {i}) \\t{qt}\\t-->\\t{chunk_tokens[i]} \\t\\t{s}\")\n",
    "            elif len(chunk_tokens[i]) >= 7 and len(qt) < 8:\n",
    "                print(f\"     {i}) \\t{qt}\\t\\t-->\\t{chunk_tokens[i]} \\t{s}\")\n",
    "            else:\n",
    "                print(f\"     {i}) \\t{qt}\\t-->\\t{chunk_tokens[i]} \\t{s}\")\n",
    "\n",
    "        print(f\"\\nnp.mean(np.max(sim, axis=1))\\tsimilarity score between query and {counter} is {np.mean(np.max(sim, axis=1))}\")\n",
    "        print(f\"\\nnp.mean(sim)\\t\\t\\tsimilarity score between query and {counter} is {np.mean(sim)}\")\n",
    "    \n",
    "    elif TYPE == \"COMBINE_MEAN\":\n",
    "        similarity = cosine_similarity(np.mean(query_embedding, axis=0).reshape(1, -1),\n",
    "                                       np.mean(chunk_embeddings, axis=0).reshape(1, -1))\n",
    "        similarity = np.mean(similarity) # Get the single value out of the array\n",
    "        \n",
    "        print(f\"\\nThe average query embedding and average {counter} embedding is {similarity}\")\n",
    "    \n",
    "    elif TYPE == \"MEAN_MEAN\":\n",
    "        sim = cosine_similarity(query_embedding, chunk_embeddings)\n",
    "        print(f\"\\nnp.mean(sim) similarity score between query and {filename} is {np.mean(sim)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"TYPE {TYPE} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "84c4d908-e211-42c8-85dc-6f4cdb372acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġstar', 'link', 'Ġprogram']\n"
     ]
    }
   ],
   "source": [
    "query_data[\"query\"]\n",
    "print(query_data['tokenized_query_search_less_sw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d8a058ac-84bc-4a98-af56-b717363cde52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['query', 'input_ids_query', 'attention_mask_query', 'query_search', 'tokenized_query', 'tokenized_query_search', 'tokenized_query_search_less_sw', 'query_embedding', 'query_embedding_search', 'query_embedding_search_less_sw'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5dbd0e86-b2dc-491d-a11c-8d6e51cd880b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if mongo_db.connect():\n",
    "    # cursor = mongo_db.get_collection().find({\"Document\": \"Starlink - Wikipedia.pdf\"})\n",
    "    cursor = mongo_db.get_collection().find({\"counter\": 393})\n",
    "    \n",
    "    mongo_data = list(cursor)\n",
    "    mongo_db.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "22882546-a8dd-444e-907d-252aa5ef31a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "dict_keys(['_id', 'tokens', 'tokens_less_sw', 'token_embeddings_less_sw', 'Document', 'sha_256', 'counter'])\n",
      "['Ġstar', 'link', 'Ġwik', 'ipedia', 'Ġstar', 'link', 'Ġstar', 'link', 'Ġsatellites', 'Ġstacked', 'Ġtogether', 'Ġdeployment', 'Ġmay', 'Ġmanufacturer', 'Ġspace', 'x', 'Ġcountry', 'Ġorigin', 'Ġunited', 'Ġstates', 'Ġoperator', 'Ġspace', 'x', 'Ġapplications', 'Ġinternet', 'Ġservice', 'Ġwebsite', 'Ġstar', 'link', 'st', 'arl', 'Ġink', 'Ġspe', 'ci', 'ï', '¬', 'ģ', 'c', 'ations', 'Ġspacecraft', 'Ġtype', 'Ġsmall', 'Ġsatellite', 'Ġlaunch', 'Ġmass', 'Ġmini', 'Ġequipment', 'Ġka', 'Ġe', 'band', 'Ġphased', 'Ġarray', 'Ġantennas', 'Ġlaser', 'Ġtrans', 'p', 'ond', 'ers', 'Ġunits', 'Ġhall', 'effect', 'Ġthr', 'usters', 'Ġregime', 'Ġlow', 'Ġearth', 'Ġorbit', 'Ġsun', 'syn', 'chron', 'ous', 'Ġorbit', 'Ġproduction', 'Ġstatus', 'Ġactive', 'Ġstar', 'link', 'Ġstar', 'link', 'Ġsatellite', 'Ġinternet', 'Ġconstellation', 'Ġoperated', 'Ġameric', 'Ġaerospace', 'Ġcompany', 'Ġspace', 'x', 'Ġproviding', 'Ġcoverage', 'Ġcountries', 'Ġalso', 'Ġaims', 'Ġglobal', 'Ġmobile', 'Ġphone', 'Ġservice', 'Ġspace', 'x', 'Ġstarted', 'Ġlaunching', 'Ġstar', 'link', 'Ġsatellites', 'Ġaug', 'ust', 'Ġstar', 'link', 'Ġconsists', '000', 'Ġmass', 'produced', 'Ġsmall', 'Ġsatellites', 'Ġlow', 'Ġearth', 'Ġorbit', 'Ġcommunicate', 'Ġdesignated', 'Ġground', 'Ġtrans', 'ce', 'ivers', 'Ġtotal', 'Ġnearly', '000', 'Ġsatellites', 'Ġplanned', 'Ġdeployed', 'Ġpossible', 'Ġlater', 'Ġextension', '000', 'Ġspace', 'x', 'Ġannounced', 'Ġreaching', 'Ġmillion', 'Ġsubscribers', 'Ġde', 'cember', 'Ġmillion', 'Ġsubscribers', 'Ġmay', 'Ġspace', 'x', 'Ġsatellite', 'Ġdevelopment', 'Ġfacility', 'Ġred', 'mond', 'Ġwashing', 'ton', 'Ġhouses', 'Ġstar', 'link', 'Ġresearch', 'Ġdevelopment', 'Ġmanufacturing', 'Ġorbit', 'Ġcontrol', 'Ġteams', 'Ġcost', 'Ġdecade', 'long', 'Ġproject', 'Ġdesign', 'Ġbuild', 'Ġdeploy', 'Ġconstellation', 'Ġestimated', 'Ġspace', 'x', 'Ġmay', 'Ġleast', 'Ġus', 'Ġbillion', 'equ', 'ivalent', 'Ġus', 'Ġbillion', ').[', 'Ġspace', 'x', 'Ġexpects', 'Ġbillion', 'Ġrevenue', 'Ġsatellite', 'Ġconstellation', 'Ġrevenues', 'Ġlaunch', 'Ġbusiness', 'Ġexpected', 'Ġreach', 'Ġbillion', 'Ġyear', 'Ġastronomers', 'Ġraised', 'Ġconcerns', 'Ġeffect', 'Ġconstellation', 'Ġground', 'based', 'Ġastronomy', 'Ġsatellites', 'Ġadd', 'Ġalready', 'Ġcongest', 'ed', 'Ġorbital', 'Ġspace', 'x', 'Ġattempted', 'Ġmitigate', 'Ġastron', 'ometric', 'Ġinterference', 'Ġconcerns', 'Ġmeasures', 'Ġreduce', 'Ġbrightness', 'Ġoperation']\n"
     ]
    }
   ],
   "source": [
    "print(len(mongo_data))\n",
    "print(mongo_data[0].keys())\n",
    "print(mongo_data[0]['tokens_less_sw'])\n",
    "# mongo_data[0]['token_embeddings_less_sw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e94a889-28b1-42c4-a1fd-6aecfa4e7802",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting MEAN_MAX METHOD...\n",
      "Finding the most simlar words in the chunk for each query word...\n",
      "\n",
      "Position\tQuery\t\t\tChunk\t\tsim_score\n",
      "0\n",
      "     0) \tĠstar\t\t-->\tĠstar \t\t1.0000000000000007\n",
      "1\n",
      "     1) \tlink\t\t-->\tlink \t\t1.0000000000000002\n",
      "147\n",
      "     147) \tĠprogram\t-->\tĠdevelopment \t0.4965865217811394\n",
      "\n",
      "np.mean(np.max(sim, axis=1))\tsimilarity score between query and 393 is 0.83219550726038\n",
      "\n",
      "np.mean(sim)\t\t\tsimilarity score between query and 393 is 0.15482695542833694\n"
     ]
    }
   ],
   "source": [
    "print_view(METHOD, query_data, mongo_data[0], mongo_data[0]['counter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd5ec74-fd04-4da6-af6d-b19fddb7540e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5a5eaea-035f-4a60-a0b7-1e60cc3a3c0a",
   "metadata": {},
   "source": [
    "#### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "39ad2aa4-a303-404e-87fa-a9e926ec5999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk_embeddings = mongo_data[0]['token_embeddings_less_sw']\n",
    "chunk_tokens = mongo_data[0]['tokens_less_sw']\n",
    "query_embedding = query_data[\"query_embedding_search_less_sw\"]\n",
    "query_tokens = query_data[\"tokenized_query_search_less_sw\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c57f96a8-3f91-4171-b397-f7e07a8c4d0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġstar'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0fc06f51-7fb3-49f6-9093-91dbfe59aa4d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġstar'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_data[\"tokenized_query_search_less_sw\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dda51cc1-713a-473b-a57b-ed8df0e78f83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1263670027256012, -0.18757300078868866, 0.3148829936981201, 0.6943079829216003, -1.6521389484405518, 0.7149810194969177, 1.2589759826660156, -0.9046199917793274, -1.0553289651870728, 0.6717360019683838, 0.5311689972877502, 0.3487280011177063, 0.3001149892807007, 0.967756986618042, -0.8603249788284302, -0.4125550091266632, -0.7426400184631348, 0.726872980594635, 0.4211600124835968, -1.3967779874801636, -0.3427030146121979, 0.16761000454425812, -0.2244739979505539, 0.9043030142784119, 1.0735490322113037, 0.09345600008964539, -0.20636099576950073, 0.8705009818077087, 0.3816690146923065, 0.6370450258255005, 0.009087000042200089, -0.2014629989862442, -0.7317540049552917, -0.8246780037879944, 1.1714550256729126, -0.5089390277862549, 0.24671700596809387, 0.4250909984111786, 0.38922399282455444, -0.5507810115814209, 0.5331500172615051, -0.3759540021419525, 0.6035339832305908, -0.08156999945640564, 0.05593999847769737, 0.06643900275230408, 0.18744899332523346, 0.2751159965991974, 0.0687360018491745, 0.4534359872341156]\n"
     ]
    }
   ],
   "source": [
    "print(chunk_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "69d9f1e7-711f-4420-a6be-a5a4fe8263b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1263670027256012, -0.18757300078868866, 0.3148829936981201, 0.6943079829216003, -1.6521389484405518, 0.7149810194969177, 1.2589759826660156, -0.9046199917793274, -1.0553289651870728, 0.6717360019683838, 0.5311689972877502, 0.3487280011177063, 0.3001149892807007, 0.967756986618042, -0.8603249788284302, -0.4125550091266632, -0.7426400184631348, 0.726872980594635, 0.4211600124835968, -1.3967779874801636, -0.3427030146121979, 0.16761000454425812, -0.2244739979505539, 0.9043030142784119, 1.0735490322113037, 0.09345600008964539, -0.20636099576950073, 0.8705009818077087, 0.3816690146923065, 0.6370450258255005, 0.009087000042200089, -0.2014629989862442, -0.7317540049552917, -0.8246780037879944, 1.1714550256729126, -0.5089390277862549, 0.24671700596809387, 0.4250909984111786, 0.38922399282455444, -0.5507810115814209, 0.5331500172615051, -0.3759540021419525, 0.6035339832305908, -0.08156999945640564, 0.05593999847769737, 0.06643900275230408, 0.18744899332523346, 0.2751159965991974, 0.0687360018491745, 0.4534359872341156]\n"
     ]
    }
   ],
   "source": [
    "print(query_embedding[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb0b0888-1561-4ba4-8f61-eab59e9c0654",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: [ 0.036363   -0.393042   -0.14290901 -0.46448901  0.761226    0.76178199\n",
      " -0.35284099  1.00311196  1.06885898 -0.42710701 -0.724011   -0.37086099\n",
      "  0.25220501  1.56391394 -0.005681   -1.03142202 -1.27680898  0.82345903\n",
      " -1.13170898 -0.506347   -0.380328   -0.63763601 -0.649562   -0.353872\n",
      " -0.016324    0.44753599  0.79029101 -0.99025601  0.35526401  0.51425499\n",
      "  0.206158    0.746557   -0.257566   -0.95098799  0.57719302  0.76185697\n",
      " -0.21814799  1.15517604 -0.113426   -0.96824402 -0.25894901  0.52897602\n",
      "  0.166455   -0.47958601 -0.148857    0.020318   -0.91852498 -0.195447\n",
      "  0.191122    1.20281506]\n",
      "B: [ 0.027065    0.117158   -0.149758    0.095494    0.78823799  0.044426\n",
      "  0.21710899 -1.09294701  0.17413101 -0.54461998 -1.32539797  0.66004902\n",
      "  0.88568503  0.35447499 -0.79060501  0.30542299  0.43905699 -1.33964705\n",
      "  0.074849   -0.38930199 -1.14452004  0.169121   -0.13893899  0.053343\n",
      " -0.076741   -1.16213596 -0.890517   -0.112144   -0.54387599  0.45776999\n",
      " -0.91843998 -0.038122    0.054959    0.59223998 -0.049424    0.29127401\n",
      "  0.96805203  0.225623   -0.327277    1.67805195  0.25254899  0.362665\n",
      " -0.009165   -0.58305901 -0.67017698  0.141753    0.202885    1.53154504\n",
      " -0.88028699 -0.52501601]\n",
      "Cosine Similarity: -0.18411191267012425\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    " \n",
    "# define two lists or array\n",
    "A = np.array(chunk_embeddings[0])\n",
    "B = np.array(query_embedding[0])\n",
    " \n",
    "print(\"A:\", A)\n",
    "print(\"B:\", B)\n",
    " \n",
    "# compute cosine similarity\n",
    "cosine = np.dot(A,B)/(norm(A)*norm(B))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f689f7b8-c25a-4290-b3fc-cd10ffb7e71a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m temp \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m3\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtemp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "temp = {\"a\":1, \"b\":2, \"c\":3}\n",
    "temp.pop([\"a\", \"b\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e9381722-1292-41c8-bfb0-f8d205c5f993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "l1 = [1,2]\n",
    "l2 = [4,5]\n",
    "for val in l1+l2:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17fdb7-91c8-4bef-b0ad-306c16917b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
