{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acb76ef2-6f9d-41bd-8897-0eb97571439f",
   "metadata": {},
   "source": [
    "#### Notebook: Write single document to Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "55043f62-5379-4b02-8fda-dab868463edd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "sys.path.append(os.path.join('..', 'src', 'question_answer_site', 'question_answer'))\n",
    "from utils import parse_pdf_to_chunks\n",
    "\n",
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from mongo import MongoDb\n",
    "from urllib.parse import quote_plus\n",
    "import json\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fbf832-3e34-43b1-96b3-e54868113a46",
   "metadata": {},
   "source": [
    "##### Set hyperparameters\n",
    "- Tokenizer, embedding model, chunk size and overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "18590695-af99-4bb7-a9ba-6f1a6dd3c671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TOKENIZER\": \"roberta\",\n",
      "    \"input_folder\": \"space_based_pdfs\",\n",
      "    \"embedding_model_type\": \"glove\",\n",
      "    \"embedding_model_fname\": \"roberta_space_based_pdfs_glove_model.bin\",\n",
      "    \"vector_size\": 50,\n",
      "    \"window\": 3,\n",
      "    \"min_count\": 3,\n",
      "    \"sg\": 0,\n",
      "    \"TOKENS_TPYE\": \"tokens_less_sw\",\n",
      "    \"chunk_size\": 450,\n",
      "    \"chunk_overlap\": 0,\n",
      "    \"max_query_length\": 20,\n",
      "    \"top_N\": 10,\n",
      "    \"TOKENS_EMBEDDINGS\": \"query_search_less_sw\",\n",
      "    \"DOCUMENT_EMBEDDING\": \"token_embeddings_less_sw\",\n",
      "    \"DOCUMENT_TOKENS\": \"tokens_less_sw\",\n",
      "    \"METHOD\": \"COMBINE_MEAN\",\n",
      "    \"transformer_model_name\": \"deepset/roberta-base-squad2\",\n",
      "    \"context_size\": 500\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")) as json_file:\n",
    "    hyperparams = json.load(json_file)\n",
    "    print(json.dumps(hyperparams, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ea19c098-833a-41fa-8bc0-ab78bf7da04f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ca5bd1b9-a034-4188-9ab2-e035b2a64b17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "embedding_model_fname = hyperparams[\"embedding_model_fname\"]\n",
    "\n",
    "embedding_model_type = hyperparams['embedding_model_type']\n",
    "if embedding_model_type == 'Word2Vec':\n",
    "    model = Word2Vec.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname))\n",
    "\n",
    "elif embedding_model_type.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    model = spacy.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname.split(\".bin\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8045bbbf-bfc8-42b2-8ec3-f77e5ef8de5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '://', 'https', '\"', '\"...', '/)', 'www', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '.[', ',[', '-,', '][', 'com', '),', ',\"', 'Ġ!', 'Ġ\"', 'Ġ#', 'Ġ$', 'Ġ%', 'Ġ&', \"Ġ'\", 'Ġ(', 'Ġ)', 'Ġ*', 'Ġ+', 'Ġ,', 'Ġ-', 'Ġ.', 'Ġ/', 'Ġ://', 'Ġhttps', 'Ġ\"', 'Ġ\"...', 'Ġ/)', 'Ġwww', 'Ġ:', 'Ġ;', 'Ġ<', 'Ġ=', 'Ġ>', 'Ġ?', 'Ġ@', 'Ġ[', 'Ġ\\\\', 'Ġ]', 'Ġ^', 'Ġ_', 'Ġ`', 'Ġ{', 'Ġ|', 'Ġ}', 'Ġ~', 'Ġ.[', 'Ġ,[', 'Ġ-,', 'Ġ][', 'Ġcom', 'Ġ),', 'Ġ,\"']\n"
     ]
    }
   ],
   "source": [
    "# Specify additional stopwords to remove from the chunk cleaned for the candidate document search\n",
    "special_characters = [\n",
    "    \"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", \"(\", \")\", \"*\", \"+\", \",\", \"-\", \".\", \"/\", \"://\", \"https\",'\"', '\"...', \"/)\",\"www\",\n",
    "    \":\", \";\", \"<\", \"=\", \">\", \"?\", \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", \"`\", \"{\", \"|\", \"}\", \"~\", \".[\", \",[\", \"-,\", \"][\", \"com\",\n",
    "    \"),\", ',\"', ').'\n",
    "]\n",
    "\n",
    "special_characters += list(map(lambda x: \"Ġ\" + x, special_characters))\n",
    "print(special_characters)\n",
    "\n",
    "# Add numbers to remove\n",
    "special_characters += list(map(lambda x: str(x), range(100000)))\n",
    "special_characters += list(map(lambda x: \"Ġ\" + str(x), range(100000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "baf480df-1b75-4bf4-9557-e61aa2542a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/one_pdf/Falcon Heavy - Wikipedia.pdf\n"
     ]
    }
   ],
   "source": [
    "input_folder = \"one_pdf\"\n",
    "single_file = os.path.join(\"..\", \"data\", input_folder, 'Falcon Heavy - Wikipedia.pdf')\n",
    "print(single_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8178e5-2f06-4358-973f-37df3a65fae9",
   "metadata": {},
   "source": [
    "##### Read data from file into list of dictionaries representing chunks of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5edadb54-e6d9-4f35-848a-aec1b339781d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/one_pdf/Falcon Heavy - Wikipedia.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "Chunking the tokenized text...\n",
      "\n",
      "printing the shape of chunked dataframe\n",
      "(47, 14)\n"
     ]
    }
   ],
   "source": [
    "parsed_data = parse_pdf_to_chunks(single_file, \n",
    "                                   embedding_layer_model=model, \n",
    "                                   tokenizer=tokenizer, \n",
    "                                   chunk_size=hyperparams['chunk_size'],\n",
    "                                   chunk_overlap=hyperparams['chunk_overlap'], \n",
    "                                   additional_stopwords=special_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43599a59-b2a3-4f95-8e6a-eebd6842f9ea",
   "metadata": {},
   "source": [
    "##### Connect to MongoDb, write data to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3473f7a7-396a-4df2-b1a3-226b1eba3d60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "username = \"new_user_1\"\n",
    "password = \"password33566\"\n",
    "# Escape the username and password\n",
    "escaped_username = quote_plus(username)\n",
    "escaped_password = quote_plus(password)\n",
    "\n",
    "cluster_url = \"cluster0\"\n",
    "database_name = \"question_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "27857100-3ece-49d8-8ca9-e43423c1d289",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322 documents in parsed_documents before adding\n",
      "Data inserted successfully!\n",
      "369 documents in parsed_documents after adding\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"parsed_documents\"\n",
    "\n",
    "# Create a MongoClient and connect to the server\n",
    "mongodb = MongoDb(escaped_username, escaped_password, cluster_url, database_name, collection_name)\n",
    "mongodb.connect()\n",
    "\n",
    "doc_cnt = mongodb.count_documents()\n",
    "print(f\"{doc_cnt} documents in {collection_name} before adding\")\n",
    "\n",
    "document_tracker = set()\n",
    "for data_obj in copy.deepcopy(parsed_data):\n",
    "    # 'extracted_text'\n",
    "    data_obj.pop('Original_Text')\n",
    "    data_obj.pop('Text')\n",
    "\n",
    "    # -\n",
    "    data_obj.pop('language')\n",
    "    data_obj.pop('language_probability')\n",
    "    data_obj.pop('Path')\n",
    "    data_obj.pop('token_embeddings')\n",
    "    data_obj.pop('chunk_text')\n",
    "    data_obj.pop('chunk_text_less_sw')\n",
    "\n",
    "    mongodb.insert_document(data_obj)\n",
    "\n",
    "print(\"Data inserted successfully!\")\n",
    "\n",
    "doc_cnt = mongodb.count_documents()\n",
    "print(f\"{doc_cnt} documents in {collection_name} after adding\")\n",
    "\n",
    "# Close the MongoDB client when done\n",
    "mongodb.disconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a1bc7317-9da6-4d47-b6bc-335070972637",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 documents in extracted_text before adding\n",
      "Falcon Heavy - Wikipedia.pdf\n",
      "Data inserted successfully!\n",
      "15 documents in extracted_text after adding\n"
     ]
    }
   ],
   "source": [
    "collection_name = \"extracted_text\"\n",
    "\n",
    "# Create a MongoClient and connect to the server\n",
    "mongodb = MongoDb(escaped_username, escaped_password, cluster_url, database_name, collection_name)\n",
    "mongodb.connect()\n",
    "\n",
    "doc_cnt = mongodb.count_documents()\n",
    "print(f\"{doc_cnt} documents in {collection_name} before adding\")\n",
    "\n",
    "document_tracker = set()\n",
    "for data_obj in copy.deepcopy(parsed_data):\n",
    "    # -\n",
    "    data_obj.pop('language')\n",
    "    data_obj.pop('language_probability')\n",
    "    data_obj.pop('Path')\n",
    "    data_obj.pop('token_embeddings')\n",
    "    data_obj.pop('chunk_text')\n",
    "    data_obj.pop('chunk_text_less_sw')\n",
    "\n",
    "    # 'parsed_documents'\n",
    "    data_obj.pop('counter')\n",
    "    data_obj.pop('token_embeddings_less_sw')\n",
    "    data_obj.pop('tokens_less_sw')\n",
    "    data_obj.pop('tokens')\n",
    "\n",
    "    # Insert the JSON data as a document into the collection\n",
    "    if data_obj['Document'] not in document_tracker:\n",
    "        document_tracker.add(data_obj['Document'])\n",
    "        print(data_obj['Document'])\n",
    "        mongodb.insert_document(data_obj)\n",
    "\n",
    "print(\"Data inserted successfully!\")\n",
    "\n",
    "doc_cnt = mongodb.count_documents()\n",
    "print(f\"{doc_cnt} documents in {collection_name} after adding\")\n",
    "\n",
    "# Close the MongoDB client when done\n",
    "mongodb.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d4ccf7-b148-49cc-9ad4-a323a57d47bc",
   "metadata": {},
   "source": [
    "##### Once database is updated, the embedding model needs to be updates and all chunk tokens need to be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb03896-26ce-48bc-a96f-59d79a0a4e73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
