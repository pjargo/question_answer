{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b1ff252-c3ab-413b-ae16-139912919800",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, RobertaTokenizer\n",
    "import torch\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "from utils import get_sha256, clean_text, remove_non_word_chars, clean_text, tokens_to_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9565e6-82dc-450f-ae12-ad1c60a8e4e9",
   "metadata": {},
   "source": [
    "#### Get Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48fd0ab8-f536-4c4f-8aa4-289c154e23c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TOKENIZER\": \"roberta\",\n",
      "    \"input_folder\": \"space_based_pdfs\",\n",
      "    \"embedding_model_type\": \"glove\",\n",
      "    \"embedding_model_fname\": \"roberta_space_based_pdfs_glove_model.bin\",\n",
      "    \"vector_size\": 50,\n",
      "    \"window\": 3,\n",
      "    \"min_count\": 3,\n",
      "    \"sg\": 0,\n",
      "    \"TOKENS_TPYE\": \"tokens_less_sw\",\n",
      "    \"chunk_size\": 350,\n",
      "    \"chunk_overlap\": 0,\n",
      "    \"max_query_length\": 20,\n",
      "    \"top_N\": 20,\n",
      "    \"TOKENS_EMBEDDINGS\": \"query_search_less_sw\",\n",
      "    \"DOCUMENT_EMBEDDING\": \"token_embeddings_less_sw\",\n",
      "    \"DOCUMENT_TOKENS\": \"tokens_less_sw\",\n",
      "    \"METHOD\": \"MEAN_MAX\",\n",
      "    \"transformer_model_name\": \"deepset/roberta-base-squad2\",\n",
      "    \"context_size\": 350\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")) as json_file:\n",
    "    hyperparams = json.load(json_file)\n",
    "    print(json.dumps(hyperparams, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af33402-81aa-41d6-b2e0-27975ac5ac9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Prompt user for query\n",
    "- Specify the tokenizer, consistant with Q&A model and parsed pdfs (TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deb1d2a1-2785-42d3-a8bd-55b31acdf370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "TOKENIZER = hyperparams[\"TOKENIZER\"]\n",
    "\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b6c7669-1ce2-4884-a11e-b6b89032cf5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your query:  How much does the starlink satellite program cost and how much revenue is expected from the program?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized query:\n",
      " ['Ġhow', 'Ġmuch', 'Ġdoes', 'Ġthe', 'Ġstar', 'link', 'Ġsatellite', 'Ġprogram', 'Ġcost', 'Ġand', 'Ġhow', 'Ġmuch', 'Ġrevenue', 'Ġis', 'Ġexpected', 'Ġfrom', 'Ġthe', 'Ġprogram', '?', '<pad>'] \n",
      "\n",
      "Tokenized query for seach:\n",
      " ['Ġhow', 'Ġmuch', 'Ġdoes', 'Ġthe', 'Ġstar', 'link', 'Ġsatellite', 'Ġprogram', 'Ġcost', 'Ġand', 'Ġhow', 'Ġmuch', 'Ġrevenue', 'Ġis', 'Ġexpected', 'Ġfrom', 'Ġthe', 'Ġprogram'] \n",
      "\n",
      "Tokenized query for seach less stop words:\n",
      " ['Ġmuch', 'Ġstar', 'link', 'Ġsatellite', 'Ġprogram', 'Ġcost', 'Ġmuch', 'Ġrevenue', 'Ġexpected', 'Ġprogram'] \n",
      "\n",
      "Input IDs query:\n",
      " tensor([[  141,   203,   473,     5,   999, 12139,  7595,   586,   701,     8,\n",
      "           141,   203,   903,    16,   421,    31,     5,   586,   116,     1]]) \n",
      "\n",
      "Attention mask query:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user for an input query\n",
    "user_query = input(\"Enter your query: \")\n",
    "user_query = user_query.lower()\n",
    "\n",
    "# clean query for BERT input\n",
    "user_query = clean_text(user_query)\n",
    "\n",
    "# clean query for candidate search\n",
    "user_query_for_search = remove_non_word_chars(user_query)\n",
    "\n",
    "# Tokenize the query for BERT input\n",
    "tokenized_query = tokenizer.tokenize(user_query)\n",
    "\n",
    "# Tokenize the query for candidate search\n",
    "tokenized_query_for_search = tokenizer.tokenize(user_query_for_search)\n",
    "\n",
    "# Remove the stop words for the tokenized query for search\n",
    "nltk_stop_words = nltk.corpus.stopwords.words('english')\n",
    "nltk_stop_words.extend([\"Ġ\" + word for word in nltk_stop_words])  # Add the roberta modified tokens\n",
    "tokenized_query_for_search_less_sw = [token for token in tokenized_query_for_search if token not in nltk_stop_words]\n",
    "\n",
    "# Pad or truncate the query to a fixed length of 20 tokens (BERT input)\n",
    "max_query_length = hyperparams[\"max_query_length\"]\n",
    "if len(tokenized_query) > max_query_length:\n",
    "    tokenized_query = tokenized_query[:max_query_length]\n",
    "else:\n",
    "    padding_length = max_query_length - len(tokenized_query)\n",
    "    tokenized_query = tokenized_query + [tokenizer.pad_token] * padding_length\n",
    "\n",
    "# Convert the tokenized query to input IDs and attention mask\n",
    "input_ids_query = tokenizer.convert_tokens_to_ids(tokenized_query)\n",
    "attention_mask_query = [1] * len(input_ids_query)\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_query = torch.tensor(input_ids_query).unsqueeze(0)  # Add batch dimension\n",
    "attention_mask_query = torch.tensor(attention_mask_query).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(\"Tokenized query:\\n\", tokenized_query, \"\\n\")\n",
    "print(\"Tokenized query for seach:\\n\", tokenized_query_for_search, \"\\n\")\n",
    "print(\"Tokenized query for seach less stop words:\\n\", tokenized_query_for_search_less_sw, \"\\n\")\n",
    "print(\"Input IDs query:\\n\", input_ids_query, \"\\n\")\n",
    "print(\"Attention mask query:\\n\", attention_mask_query, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce0bdf-7a2d-4cdc-90b0-be9513da32b1",
   "metadata": {},
   "source": [
    "##### Add the embeddings\n",
    "- Specify the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93a01a63-032d-4826-977d-e6c90da993f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "embedding_model_fname = hyperparams[\"embedding_model_fname\"]\n",
    "\n",
    "embedding_model_type = hyperparams['embedding_model_type']\n",
    "if embedding_model_type == 'Word2Vec':\n",
    "    model = Word2Vec.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname))\n",
    "\n",
    "elif embedding_model_type.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    model = spacy.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname.split(\".bin\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25a49f57-9661-4f92-9915-401897e35a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\t\tTokens Length\t\tEmbeddings Shape\n",
      "\t\t\t   Query embeddings:\t      20\t\t     (20, 50)\n",
      "\t\tQuery embeddings for search:\t      18\t\t     (18, 50)\n",
      " Query embeddings for search less stopwords:\t      10\t\t     (10, 50)\n"
     ]
    }
   ],
   "source": [
    "# Get the query embeddings for the candidate document search\n",
    "query_embeddings = tokens_to_embeddings(tokenized_query, model, RANDOM=False)\n",
    "query_embeddings_search = tokens_to_embeddings(tokenized_query_for_search, model, RANDOM=False)\n",
    "query_embeddings_less_sw = tokens_to_embeddings(tokenized_query_for_search_less_sw, model, RANDOM=False)\n",
    "\n",
    "print(\"\\t\\t\\t\\t\\t\\tTokens Length\\t\\tEmbeddings Shape\")\n",
    "print(f\"\\t\\t\\t   Query embeddings:\\t      {len(tokenized_query)}\\t\\t     {query_embeddings.shape}\")\n",
    "print(f\"\\t\\tQuery embeddings for search:\\t      {len(tokenized_query_for_search)}\\t\\t     {query_embeddings_search.shape}\")\n",
    "print(f\" Query embeddings for search less stopwords:\\t      {len(tokenized_query_for_search_less_sw)}\\t\\t     {query_embeddings_less_sw.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb34f19-37d2-4a76-a013-b1cbd88da2b5",
   "metadata": {},
   "source": [
    "##### Store the output the the query directory, filename is hash of query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29dd5893-3316-4690-86c8-23267f788b99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8d3958cd0d203f82009a93699d6fc8503747100bfa9b1e083ab0aaa64ec056d7\n",
      "../query/8d3958cd0d203f82009a93699d6fc8503747100bfa9b1e083ab0aaa64ec056d7.json\n"
     ]
    }
   ],
   "source": [
    "# store the query\n",
    "query_data = {\n",
    "    \"query\": user_query,\n",
    "    \"input_ids_query\":input_ids_query.tolist(),\n",
    "    \"attention_mask_query\": attention_mask_query.tolist(),\n",
    "    \"query_search\":user_query_for_search,\n",
    "    \"tokenized_query\":tokenized_query,\n",
    "    \"tokenized_query_search\":tokenized_query_for_search,\n",
    "    \"tokenized_query_search_less_sw\":tokenized_query_for_search_less_sw,\n",
    "    \"query_embedding\": query_embeddings.tolist(), # Just used for the candidate search\n",
    "    \"query_embedding_search\": query_embeddings_search.tolist(), # Just used for the candidate search, cleaned\n",
    "    \"query_embedding_search_less_sw\": query_embeddings_less_sw.tolist() # Just used for the candidate search, cleaned more\n",
    "}\n",
    "\n",
    "json_string = json.dumps(query_data['query'], indent=2)\n",
    "# print(json_string)\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = os.path.join(\"..\", 'query')\n",
    "\n",
    "# Check if the directory exists, if not create the directory\n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "\n",
    "# Generate a UUID\n",
    "# unique_id = uuid.uuid4()\n",
    "unique_id = get_sha256(json_string)\n",
    "print(unique_id)\n",
    "\n",
    "fname = os.path.join(directory_path, str(unique_id)+'.json')\n",
    "print(fname)\n",
    "\n",
    "with open(fname, 'w') as j_file:\n",
    "    json.dump(query_data, j_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c42e9-3116-4978-9226-83fdfef0566d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5278c0c7-ce6b-4566-9c3c-901797122c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res ={\n",
    "    \"1\":1,\n",
    "    \"2\":2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccbb03b9-1dae-4260-aeb3-b3467b12ccb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (626430293.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    res.0\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "res.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff1b486-79d2-4b5d-968e-e0f84960e4fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3302916474.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(res.0)\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "res = (1,2)\n",
    "print(res.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123981f1-1c76-42fd-a22e-eb46f156aa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
