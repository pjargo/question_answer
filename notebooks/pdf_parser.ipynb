{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165ac2ee-06dd-4fef-940e-a8c860e9f5f7",
   "metadata": {},
   "source": [
    "### Notebook: Processes directory of PDF's into Mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b45a15b0-2934-4da3-855e-fd95b3533f00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import os\n",
    "import sys\n",
    "# Get the current working directory (notebooks directory)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Go up one level to the project directory\n",
    "project_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Assuming your project structure is as described before\n",
    "src_path = os.path.abspath(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Add the 'src' directory to the Python path\n",
    "sys.path.append(src_path)\n",
    "from question_answer_site.question_answer.parse_document import parse_document, update_collection\n",
    "from question_answer_site.question_answer.mongodb import MongoDb\n",
    "from question_answer_site.question_answer.utils import remove_non_word_chars, clean_text, tokens_to_embeddings, post_process_output, correct_spelling\n",
    "from question_answer_site.question_answer.config import TOKENIZER, EMBEDDING_MODEL_FNAME, EMBEDDING_MODEL_TYPE, TOKENS_EMBEDDINGS, DOCUMENT_EMBEDDING, \\\n",
    "    DOCUMENT_TOKENS, TOP_N, TRANSFORMER_MODEL_NAME, METHOD, MAX_QUERY_LENGTH, username, password, cluster_url, INPUT_FOLDER, \\\n",
    "    database_name, special_characters, CHUNK_SIZE, CHUNK_OVERLAP\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, RobertaTokenizer, RobertaForQuestionAnswering\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from urllib.parse import quote_plus\n",
    "import spacy\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe55cd-e1a2-475e-ae6f-96008da9cacd",
   "metadata": {},
   "source": [
    "#### Parse pdfs in directory for text, tokenize, chunk and save to JSON files\n",
    "- Specify PDF directory (directory)\n",
    "- Specify Storage directory for the JSON files (storage_dir)\n",
    "- Specify word embedding model, consistent with the query for getting candidate documents (model_fname)\n",
    "- Specify chunk overlap: the number of tokens consecutive chunks overlap by (chunk_overlap)\n",
    "- Specify the tokenizer (TOKENIZER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4516b4-0cc5-4ee5-90fc-b1eae7e83363",
   "metadata": {},
   "source": [
    "##### Parse documents in test_pdfs dirctory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a6feaf-5281-45a2-a22f-52bfdd17c250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398d03b4-6975-4e9c-9523-5946d7b015e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "if EMBEDDING_MODEL_TYPE == 'Word2Vec':\n",
    "    model = Word2Vec.load(\n",
    "        os.path.join(os.getcwd(), \"question_answer\", \"embedding_models\", EMBEDDING_MODEL_FNAME))\n",
    "elif EMBEDDING_MODEL_TYPE.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    model = spacy.load(os.path.join(\"..\",\"src\",\"question_answer_site\", \"question_answer\", \"embedding_models\",\n",
    "                                         EMBEDDING_MODEL_FNAME.split(\".bin\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1f59bc-dc6d-4536-8c19-6018ab26927a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input data location: ../data/space_based_pdfs\n"
     ]
    }
   ],
   "source": [
    "directory = os.path.join(\"..\", \"data\", INPUT_FOLDER)\n",
    "print(f\"input data location: {directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7feb646b-9bca-41a1-a90b-c2bace557135",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contents of directory '../data/space_based_pdfs_roberta_parsed' deleted.\n"
     ]
    }
   ],
   "source": [
    "directory = os.path.join(\"..\", \"data\", INPUT_FOLDER)\n",
    "\n",
    "# Specify the directory path you want to check and create\n",
    "output_folder = f\"{INPUT_FOLDER}_{TOKENIZER}_parsed\"\n",
    "storage_dir = os.path.join(\"..\", \"data\", output_folder)\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(storage_dir):\n",
    "    # If the directory doesn't exist, create it\n",
    "    os.makedirs(storage_dir)\n",
    "    print(f\"Directory '{storage_dir}' created.\")\n",
    "else:\n",
    "    # If the directory exists, delete its contents\n",
    "    for filename in os.listdir(storage_dir):\n",
    "        file_path = os.path.join(storage_dir, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "    print(f\"Contents of directory '{storage_dir}' deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc71901a-5094-453a-bd98-7d302587ab3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path: ../data/space_based_pdfs/Galaxy 15 - Wikipedia.pdf,\n",
      "file name: Galaxy 15 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Reconnaissance satellite - Wikipedia.pdf,\n",
      "file name: Reconnaissance satellite - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Wideband Global SATCOM - Wikipedia.pdf,\n",
      "file name: Wideband Global SATCOM - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/.DS_Store,\n",
      "file name: .DS_Store\n",
      "File not a recognized format\n",
      "\tlanguage detection error!\n",
      "file path: ../data/space_based_pdfs/Swarm Technologies - Wikipedia.pdf,\n",
      "file name: Swarm Technologies - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Fengyun - Wikipedia.pdf,\n",
      "file name: Fengyun - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Advanced Extremely High Frequency - Wikipedia.pdf,\n",
      "file name: Advanced Extremely High Frequency - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Falcon 9 - Wikipedia.pdf,\n",
      "file name: Falcon 9 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Rocket Lab Electron - Wikipedia.pdf,\n",
      "file name: Rocket Lab Electron - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Cygnus NG-19 - Wikipedia.pdf,\n",
      "file name: Cygnus NG-19 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Falcon 9 Full Thrust - Wikipedia.pdf,\n",
      "file name: Falcon 9 Full Thrust - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Atlas V - Wikipedia.pdf,\n",
      "file name: Atlas V - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Boeing X-37 - Wikipedia.pdf,\n",
      "file name: Boeing X-37 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Inmarsat - Wikipedia.pdf,\n",
      "file name: Inmarsat - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Kepler-11 - Wikipedia.pdf,\n",
      "file name: Kepler-11 - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Technology demonstration - Wikipedia.pdf,\n",
      "file name: Technology demonstration - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Autonomous Nanosatellite Guardian for Evaluating Local Space - Wikipedia.pdf,\n",
      "file name: Autonomous Nanosatellite Guardian for Evaluating Local Space - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Communications satellite - Wikipedia.pdf,\n",
      "file name: Communications satellite - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Falcon Heavy - Wikipedia.pdf,\n",
      "file name: Falcon Heavy - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Delta IV Heavy - Wikipedia.pdf,\n",
      "file name: Delta IV Heavy - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Space Based Space Surveillance - Wikipedia.pdf,\n",
      "file name: Space Based Space Surveillance - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/James Webb Space Telescope - Wikipedia.pdf,\n",
      "file name: James Webb Space Telescope - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Space-Based Infrared System - Wikipedia.pdf,\n",
      "file name: Space-Based Infrared System - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Yaogan - Wikipedia.pdf,\n",
      "file name: Yaogan - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Starlink - Wikipedia.pdf,\n",
      "file name: Starlink - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Atlas (rocket family) - Wikipedia.pdf,\n",
      "file name: Atlas (rocket family) - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/Signals intelligence - Wikipedia.pdf,\n",
      "file name: Signals intelligence - Wikipedia.pdf\n",
      "file path: ../data/space_based_pdfs/GPS Block IIF - Wikipedia.pdf,\n",
      "file name: GPS Block IIF - Wikipedia.pdf\n",
      "processing text...\n",
      "making lower-case...\n",
      "Removing non-text elements (extra whitespaces)...\n",
      "Removing unnecessary whitespace and special characters...\n",
      "Removing line breaks...\n",
      "Removing gibberish...\n",
      "Removing unicode...\n",
      "remove single letters or super large words (so big they don't make sense)...\n",
      "done cleaning.\n",
      "\n",
      "tokenize the processed text...\n",
      "Chunking the tokenized text...\n",
      "\n",
      "printing the shape of chunked dataframe\n",
      "(502, 13)\n"
     ]
    }
   ],
   "source": [
    "parsed_data = parse_document(directory=directory,\n",
    "                            embedding_layer_model=model,\n",
    "                            tokenizer=tokenizer,\n",
    "                            chunk_size=CHUNK_SIZE,\n",
    "                            chunk_overlap=CHUNK_OVERLAP,\n",
    "                            additional_stopwords=special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ad2959c-a69a-427f-b9d9-c4c553ba54bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chunk_text', 'chunk_text_less_sw', 'tokens', 'tokens_less_sw', 'token_embeddings', 'token_embeddings_less_sw', 'Document', 'Path', 'Text', 'Original_Text', 'sha_256', 'language', 'language_probability', 'counter'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b144c21b-b0c2-434a-b179-02dddfb6e4e6",
   "metadata": {},
   "source": [
    "#### Write to Mongodb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5c1de-b954-42d6-bb67-fc8312458c0e",
   "metadata": {},
   "source": [
    "##### Extracted Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fba743-edce-44b6-aa9d-1032cd6c0dad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the 'extracted_text' collection\n",
      "0 documents in 'extracted_text' before adding\n",
      "27 documents in 'extracted_text' after adding\n"
     ]
    }
   ],
   "source": [
    "update_collection(\"extracted_text\", copy.deepcopy(parsed_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f69ec-9949-46eb-a377-3c896282a1c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Parsed Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18da3063-8ac0-4cf9-be75-db57fd7ce319",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating the 'parsed_documents' collection\n",
      "0 documents in 'parsed_documents' before adding\n",
      "502 documents in 'parsed_documents' after adding\n"
     ]
    }
   ],
   "source": [
    "update_collection(\"parsed_documents\", copy.deepcopy(parsed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed477ebb-0b72-4f4c-bc30-88f473c536ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
