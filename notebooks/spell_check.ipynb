{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600e90b2-3df9-445e-aeb1-fb3be4c268c8",
   "metadata": {},
   "source": [
    "#### Notebook: correct spelling issues in query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6732591e-b79b-426e-8c05-aadb94ef4cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join('..', 'src'))\n",
    "from utils import clean_text, remove_non_word_chars, tokens_to_embeddings\n",
    "\n",
    "from transformers import BertTokenizer, RobertaTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdf025-c316-4077-9bd6-375e46a36d53",
   "metadata": {},
   "source": [
    "##### Load embedding model, only spell check words not observed in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32a4ea28-8965-4931-969b-38f83835ca25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TOKENIZER\": \"roberta\",\n",
      "    \"input_folder\": \"space_based_pdfs\",\n",
      "    \"embedding_model_type\": \"glove\",\n",
      "    \"embedding_model_fname\": \"roberta_space_based_pdfs_glove_model.bin\",\n",
      "    \"vector_size\": 50,\n",
      "    \"window\": 3,\n",
      "    \"min_count\": 3,\n",
      "    \"sg\": 0,\n",
      "    \"TOKENS_TPYE\": \"tokens_less_sw\",\n",
      "    \"chunk_size\": 450,\n",
      "    \"chunk_overlap\": 0,\n",
      "    \"max_query_length\": 20,\n",
      "    \"top_N\": 10,\n",
      "    \"TOKENS_EMBEDDINGS\": \"query_search_less_sw\",\n",
      "    \"DOCUMENT_EMBEDDING\": \"token_embeddings_less_sw\",\n",
      "    \"DOCUMENT_TOKENS\": \"tokens_less_sw\",\n",
      "    \"METHOD\": \"COMBINE_MEAN\",\n",
      "    \"transformer_model_name\": \"deepset/roberta-base-squad2\",\n",
      "    \"context_size\": 500\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(\"..\", \"vars\", \"hyperparameters1.json\")) as json_file:\n",
    "    hyperparams = json.load(json_file)\n",
    "    print(json.dumps(hyperparams, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e635fceb-6c82-4981-b791-966b0a35ef8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your trained Word2Vec model\n",
    "embedding_model_fname = hyperparams[\"embedding_model_fname\"]\n",
    "\n",
    "embedding_model_type = hyperparams['embedding_model_type']\n",
    "if embedding_model_type == 'Word2Vec':\n",
    "    model = Word2Vec.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname))\n",
    "\n",
    "elif embedding_model_type.lower() == 'glove':\n",
    "    # Load the custom spaCy model\n",
    "    model = spacy.load(os.path.join(\"..\", \"models\", \"word_embeddings\", embedding_model_fname.split(\".bin\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "556b67d5-00cd-4214-84b0-9d311a3203fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the Tokenizer for your specific BERT model variant\n",
    "TOKENIZER = hyperparams[\"TOKENIZER\"]\n",
    "\n",
    "bert_base_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained(\"deepset/roberta-base-squad2\", add_prefix_space = True)\n",
    "\n",
    "tokenizers = {'bert': bert_base_tokenizer, 'roberta': roberta_tokenizer}\n",
    "\n",
    "tokenizer = tokenizers[TOKENIZER]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc04ceaf-fdcb-475e-af90-100b2e408621",
   "metadata": {},
   "source": [
    "##### Check query for unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff7ff28e-83b6-4de8-b9a6-594a91de286f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġwhat', 'f', \"'s\", 'Ġstar', 'link', 'Ġ(', 'the', 'Ġbig', '/', 'larg', 'Ġfull', '-', 'scale', 'Ġone', ')', 'Ġpr', 'agram', 'Ġdo', '?']\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Whatf's starlink (the big/larg full-scale one) pragram do?\"\n",
    "\n",
    "user_query = user_query.lower()\n",
    "\n",
    "# clean query for BERT input\n",
    "user_query = clean_text(user_query)\n",
    "\n",
    "# clean query for candidate search\n",
    "# user_query_for_search = remove_non_word_chars(user_query)\n",
    "\n",
    "# Tokenize the query for BERT input\n",
    "tokenized_query = tokenizer.tokenize(user_query)\n",
    "print(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e72c2e3-a310-4ca2-a33f-274d056b8435",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get embeddings\n",
    "query_embeddings = tokens_to_embeddings(tokenized_query, model, RANDOM=False)\n",
    "list(map(lambda x: not any(x), query_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ab09c878-dfa9-4905-84b8-551e21d8b271",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n"
     ]
    }
   ],
   "source": [
    "current_word = 'token'\n",
    "token = ']'\n",
    "current_word += token if token not in [')', ']', '}'] else ''\n",
    "print(current_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "115e8092-e323-4bfc-b64f-347ac336fece",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġwhat', 'f', \"'s\", 'Ġstar', 'link', 'Ġ(', 'the', 'Ġbig', '/', 'larg', 'Ġfull', '-', 'scale', 'Ġone', ')', 'Ġpr', 'agram', 'Ġdo', '?']\n",
      "words:  [\"whatf's\", 'starlink', 'the', 'big', 'larg', 'full-scale', 'one', 'pragram', 'do']\n",
      "[\"whatf's\", 'the', 'larg', 'full-scale', 'pragram', 'do']\n",
      "whatf's what's\n",
      "the the\n",
      "larg large\n",
      "full-scale full-scale\n",
      "pragram program\n",
      "do do\n",
      "what's starlink (the big/large full-scale one) program do?\n"
     ]
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(word):\n",
    "    # Your spelling correction logic\n",
    "    corrected_word = spell.correction(word)\n",
    "    return corrected_word if corrected_word else word  # Replace this with your actual correction logic\n",
    "\n",
    "tokenized_query = tokenizer.tokenize(user_query)\n",
    "print(tokenized_query)\n",
    "query_embeddings = tokens_to_embeddings(tokenized_query, model, RANDOM=False)\n",
    "\n",
    "# Group tokens into words\n",
    "words = []\n",
    "current_word = \"\"\n",
    "for token in tokenized_query:\n",
    "    if token.startswith(\"Ġ\"):  # Indicates the start of a new word\n",
    "        if current_word:\n",
    "            words.append(current_word)\n",
    "        current_word = token[1:] if token[1:] not in ['(', '[', '{', '/', '\\\\'] else ''\n",
    "    else:\n",
    "        current_word += token if token not in [')', ']', '}', '/', '\\\\', '?', \".\", \"!\"] else ''\n",
    "        if token in ['/', '\\\\']:\n",
    "            words.append(current_word)\n",
    "            current_word = ''\n",
    "\n",
    "if current_word:\n",
    "    words.append(current_word)\n",
    "print(\"words: \", words)\n",
    "\n",
    "# Identify misspelled words not in the embeddings model\n",
    "misspelled_words = []\n",
    "for word in words:\n",
    "    # Split punctuation and hyphens from the word\n",
    "    base_word = \"\".join(char for char in word if char.isalnum() or char in [\"'\", \"-\"])\n",
    "    if any(list(map(lambda x: not any(x), tokens_to_embeddings(tokenizer.tokenize(base_word), model, RANDOM=False)))):\n",
    "        # Add the original word to the misspelled_words list\n",
    "        misspelled_words.append(word)\n",
    "print(misspelled_words)\n",
    "# Correct the spelling of misspelled words\n",
    "corrected_words = {word: correct_spelling(word) for word in misspelled_words}\n",
    "\n",
    "# Replace misspelled words in the original query\n",
    "corrected_query = user_query\n",
    "for original, corrected in corrected_words.items():\n",
    "    print(original, corrected)\n",
    "    corrected_query = corrected_query.replace(original, corrected)\n",
    "\n",
    "print(corrected_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d49fb702-58aa-46a1-a817-047bd9eacb9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(query):\n",
    "    words = query.split()\n",
    "    corrected_words = [spell.correction(word) for word in words]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "\n",
    "query_embeddings = tokens_to_embeddings(tokenized_query, model, RANDOM=False)misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7a27878-a090-4f20-b6dc-a8975a9ceec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "correct_spelling(\"fulls-scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d53335b-b177-4b85-821c-62ed49e6a011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not any([0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a76cc-5230-42e1-9be6-9cad2a894b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(query, word_list, threshold=80):\n",
    "    similar_words = [word for word in word_list if fuzz.ratio(query, word) > threshold]\n",
    "    return similar_words\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
